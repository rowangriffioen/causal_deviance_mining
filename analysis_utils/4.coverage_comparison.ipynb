{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61197c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rule datasets\n",
    "all_rules_crm_path = Path(\"5_analysis/random/combined_sorted_all.csv\")\n",
    "all_rules_dt_path = Path(\"5_analysis/dt/rules_dt.csv\")\n",
    "all_rules_ripperk_path = Path(\"5_analysis/ripperk/rules_ripperk.csv\")\n",
    "\n",
    "all_rules_crm = pd.read_csv(all_rules_crm_path)\n",
    "all_rules_dt = pd.read_csv(all_rules_dt_path)\n",
    "all_rules_ripperk = pd.read_csv(all_rules_ripperk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d8715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct encodings per labeling\n",
    "unique_counts = (\n",
    "    all_rules_crm\n",
    "    .groupby(\"Labeling\")[\"Feature Encoding\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"unique_encodings\")\n",
    ")\n",
    "\n",
    "display(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483470fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labeling values and align column names across dataframes\n",
    "\n",
    "def _normalize_labeling_column(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Normalize the labeling/Labeling column to one of:\n",
    "    {'declare', 'sequential', 'payload'} based on substrings.\n",
    "    Operates in-place.\n",
    "    \"\"\"\n",
    "    for col in (\"labeling\", \"Labeling\"):\n",
    "        if col in df.columns:\n",
    "            lower = df[col].astype(str).str.lower()\n",
    "            df[col] = np.select(\n",
    "                [\n",
    "                    lower.str.contains(\"decl\", na=False),\n",
    "                    lower.str.contains(\"mr_tr\", na=False),\n",
    "                    lower.str.contains(\"payload\", na=False),\n",
    "                ],\n",
    "                [\"declare\", \"sequential\", \"payload\"],\n",
    "                default=df[col],\n",
    "            )\n",
    "\n",
    "def _canonicalize_and_subset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize common column variants and return only the key columns\n",
    "    if present: ['Dataset', 'Labeling', 'Feature Encoding', 'Rule'].\n",
    "    \"\"\"\n",
    "    rename_map = {}\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if cl == \"dataset\":\n",
    "            rename_map[c] = \"Dataset\"\n",
    "        elif cl == \"labeling\":\n",
    "            rename_map[c] = \"Labeling\"\n",
    "        elif cl in {\"feature encoding\", \"encoding\"}:\n",
    "            rename_map[c] = \"Feature Encoding\"\n",
    "        elif cl == \"rule\":\n",
    "            rename_map[c] = \"Rule\"\n",
    "    df2 = df.rename(columns=rename_map)\n",
    "\n",
    "    keep = [c for c in [\"Dataset\", \"Labeling\", \"Feature Encoding\", \"Rule\"] if c in df2.columns]\n",
    "    return df2[keep].copy() if keep else df2.copy()\n",
    "\n",
    "# Apply normalization and column alignment to all three dataframes\n",
    "for name in (\"all_rules_crm\", \"all_rules_dt\", \"all_rules_ripperk\"):\n",
    "    df = globals()[name]\n",
    "    _normalize_labeling_column(df)\n",
    "    df = _canonicalize_and_subset(df)\n",
    "    globals()[name] = df\n",
    "\n",
    "# Quick check\n",
    "display(all_rules_crm)\n",
    "display(all_rules_dt)\n",
    "display(all_rules_ripperk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ede48",
   "metadata": {},
   "source": [
    "## Splitting CRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Parse LHS and RHS from all_rules_crm[\"Rule\"]\n",
    "\n",
    "def extract_lhs_exact(rule_str: str) -> str:\n",
    "    \"\"\"Return the substring before '-->' (keeps quotes/brackets as-is).\"\"\"\n",
    "    m = re.search(r\"^(.*?)(?=\\s*-->)\", str(rule_str))\n",
    "    return m.group(1) if m else str(rule_str)\n",
    "\n",
    "def parse_rhs_label(rule_str: str):\n",
    "    \"\"\"Map RHS to 1 for 'Label', 0 for '!Label'; None if not present.\"\"\"\n",
    "    m = re.search(r\"-->\\s*(Label|!Label)\", str(rule_str))\n",
    "    if not m:\n",
    "        return None\n",
    "    return 1 if m.group(1) == \"Label\" else 0\n",
    "\n",
    "crm_df = all_rules_crm.copy()\n",
    "crm_df[\"LHS_features\"] = crm_df[\"Rule\"].apply(extract_lhs_exact)\n",
    "crm_df[\"RHS_label\"]    = crm_df[\"Rule\"].apply(parse_rhs_label)\n",
    "\n",
    "# 2) Split LHS into up to 3 features\n",
    "\n",
    "def _find_outer_brackets_span(text: str):\n",
    "    \"\"\"Indices of the outermost [...] in text; returns (start, end).\"\"\"\n",
    "    s = str(text)\n",
    "    start = s.find(\"[\")\n",
    "    if start < 0:\n",
    "        return None, None\n",
    "\n",
    "    depth = 0\n",
    "    in_s = in_d = esc = False\n",
    "    end = None\n",
    "    for i, ch in enumerate(s[start:], start):\n",
    "        if esc:\n",
    "            esc = False\n",
    "            continue\n",
    "        if ch == \"\\\\\":\n",
    "            esc = True\n",
    "            continue\n",
    "\n",
    "        if in_s:\n",
    "            if ch == \"'\":\n",
    "                in_s = False\n",
    "            continue\n",
    "        if in_d:\n",
    "            if ch == '\"':\n",
    "                in_d = False\n",
    "            continue\n",
    "\n",
    "        if ch == \"'\":\n",
    "            in_s = True\n",
    "            continue\n",
    "        if ch == '\"':\n",
    "            in_d = True\n",
    "            continue\n",
    "\n",
    "        if ch == \"[\":\n",
    "            depth += 1\n",
    "            continue\n",
    "        if ch == \"]\":\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end = i\n",
    "                break\n",
    "    return (start, end)\n",
    "\n",
    "def _split_top_level_commas(content: str):\n",
    "    \"\"\"Split on commas that are not inside quotes.\"\"\"\n",
    "    parts, curr = [], \"\"\n",
    "    in_s = in_d = esc = False\n",
    "    for ch in content:\n",
    "        if esc:\n",
    "            curr += ch\n",
    "            esc = False\n",
    "            continue\n",
    "        if ch == \"\\\\\":\n",
    "            curr += ch\n",
    "            esc = True\n",
    "            continue\n",
    "\n",
    "        if in_s:\n",
    "            curr += ch\n",
    "            if ch == \"'\":\n",
    "                in_s = False\n",
    "            continue\n",
    "        if in_d:\n",
    "            curr += ch\n",
    "            if ch == '\"':\n",
    "                in_d = False\n",
    "            continue\n",
    "\n",
    "        if ch == \"'\":\n",
    "            curr += ch\n",
    "            in_s = True\n",
    "            continue\n",
    "        if ch == '\"':\n",
    "            curr += ch\n",
    "            in_d = True\n",
    "            continue\n",
    "\n",
    "        if ch == \",\":\n",
    "            parts.append(curr.strip())\n",
    "            curr = \"\"\n",
    "        else:\n",
    "            curr += ch\n",
    "    parts.append(curr.strip())\n",
    "    return parts\n",
    "\n",
    "def _strip_one_layer_quotes(s: str):\n",
    "    \"\"\"Remove a single pair of outer quotes if present.\"\"\"\n",
    "    s = s.strip()\n",
    "    if len(s) >= 2 and ((s[0] == s[-1] == \"'\") or (s[0] == s[-1] == '\"')):\n",
    "        return s[1:-1]\n",
    "    return s\n",
    "\n",
    "def split_lhs_items(lhs_text: str):\n",
    "    \"\"\"\n",
    "    Input like \"['A', 'B', 'C']\" or \"['A']\" â†’ list ['A','B','C'].\n",
    "    \"\"\"\n",
    "    s = str(lhs_text)\n",
    "    start, end = _find_outer_brackets_span(s)\n",
    "    if start is None or end is None:\n",
    "        return []\n",
    "    inner = s[start + 1 : end]  # inside [...]\n",
    "    raw_items = _split_top_level_commas(inner)\n",
    "    return [_strip_one_layer_quotes(x).strip() for x in raw_items if x != \"\"]\n",
    "\n",
    "def _pad3(items):\n",
    "    \"\"\"Keep at most 3 items; right-pad with empty strings.\"\"\"\n",
    "    items = items[:3]\n",
    "    return items + [\"\"] * (3 - len(items))\n",
    "\n",
    "lhs_split = crm_df[\"LHS_features\"].apply(split_lhs_items).apply(_pad3)\n",
    "lhs_df = pd.DataFrame(lhs_split.tolist(), columns=[\"feature_1_lhs\", \"feature_2_lhs\", \"feature_3_lhs\"])\n",
    "\n",
    "# 3) Assemble expanded table\n",
    "cols_present = [c for c in [\"Dataset\", \"Labeling\", \"Feature Encoding\", \"Rule\", \"LHS_features\", \"RHS_label\"] if c in crm_df.columns]\n",
    "\n",
    "all_rules_crm_expanded = pd.concat(\n",
    "    [crm_df[cols_present].reset_index(drop=True), lhs_df.reset_index(drop=True)],\n",
    "    axis=1,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Sort if keys are available\n",
    "sort_keys = [c for c in [\"Dataset\", \"Labeling\", \"Feature Encoding\"] if c in all_rules_crm_expanded.columns]\n",
    "if sort_keys:\n",
    "    all_rules_crm_expanded = (\n",
    "        all_rules_crm_expanded.sort_values(by=sort_keys, ascending=True).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "all_rules_crm_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1159d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Expand DT and RIPPERk rules: support up to 15 LHS features ----------\n",
    "\n",
    "def extract_lhs_exact(rule_str: str) -> str:\n",
    "    \"\"\"Everything before the arrow '-->' (preserve characters exactly).\"\"\"\n",
    "    m = re.search(r\"^(.*?)(?=\\s*-->)\", str(rule_str))\n",
    "    return m.group(1).strip() if m else str(rule_str).strip()\n",
    "\n",
    "def parse_rhs_label(rule_str: str):\n",
    "    \"\"\"Return 1 for 'Label', 0 for '!Label', or None if not found.\"\"\"\n",
    "    m = re.search(r\"-->\\s*(Label|!Label)\", str(rule_str))\n",
    "    if not m:\n",
    "        return None\n",
    "    return 1 if m.group(1) == \"Label\" else 0\n",
    "\n",
    "# Splitter for DT/RIPPERK: use logical-and 'âˆ§' (U+2227); also accept ASCII '&' as fallback.\n",
    "_AND_SPLIT_RE = re.compile(r\"\\s*(?:âˆ§|&)\\s*\")\n",
    "\n",
    "def split_lhs_items_dt(lhs_text: str):\n",
    "    \"\"\"\n",
    "    For DT/RIPPERK rule format, LHS looks like:\n",
    "      [feature1 âˆ§ feature2 âˆ§ feature3 âˆ§ ...]\n",
    "    We split on 'âˆ§' (and '&' as fallback), strip outer [ ], then trim items.\n",
    "    \"\"\"\n",
    "    s = str(lhs_text).strip()\n",
    "    if len(s) >= 2 and s[0] == '[' and s[-1] == ']':\n",
    "        s = s[1:-1]\n",
    "    if not s:\n",
    "        return []\n",
    "    parts = _AND_SPLIT_RE.split(s)\n",
    "    return [p.strip() for p in parts if p.strip() != \"\"]\n",
    "\n",
    "def _padN(items, n=15):\n",
    "    items = items[:n]\n",
    "    return items + [\"\"] * (n - len(items))\n",
    "\n",
    "def _expand_df_with_lhs_rhs(df_in: pd.DataFrame, name_hint: str, max_features: int = 15):\n",
    "    \"\"\"\n",
    "    Given a dataframe with at least ['Rule'] column, produce an expanded version with:\n",
    "      - LHS_features: exact text before -->\n",
    "      - RHS_label: {1,0,None}\n",
    "      - feature_1_lhs ... feature_{max_features}_lhs (split on âˆ§ / & for DT/RIPPERK)\n",
    "    Keeps any of ['Dataset','Labeling','Feature Encoding','Rule'] that exist.\n",
    "    \"\"\"\n",
    "    if \"Rule\" not in df_in.columns:\n",
    "        raise KeyError(f\"{name_hint}: expected a 'Rule' column.\")\n",
    "\n",
    "    df = df_in.copy()\n",
    "    df[\"LHS_features\"] = df[\"Rule\"].apply(extract_lhs_exact)\n",
    "    df[\"RHS_label\"]    = df[\"Rule\"].apply(parse_rhs_label)\n",
    "\n",
    "    lhs_split = df[\"LHS_features\"].apply(split_lhs_items_dt).apply(lambda xs: _padN(xs, max_features))\n",
    "    feat_cols = [f\"feature_{i}_lhs\" for i in range(1, max_features+1)]\n",
    "    lhs_df = pd.DataFrame(lhs_split.tolist(), columns=feat_cols)\n",
    "\n",
    "    keep = [c for c in [\"Dataset\",\"Labeling\",\"Feature Encoding\",\"Rule\",\"LHS_features\",\"RHS_label\"] if c in df.columns or c in [\"LHS_features\",\"RHS_label\"]]\n",
    "    expanded = pd.concat([df[keep].reset_index(drop=True), lhs_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    sort_keys = [c for c in [\"Dataset\",\"Labeling\",\"Feature Encoding\"] if c in expanded.columns]\n",
    "    if sort_keys:\n",
    "        expanded = expanded.sort_values(by=sort_keys, ascending=True).reset_index(drop=True)\n",
    "    return expanded\n",
    "\n",
    "# Build the expanded tables (now with up to 15 features)\n",
    "all_rules_dt_expanded       = _expand_df_with_lhs_rhs(all_rules_dt, \"DT\", max_features=15)\n",
    "all_rules_ripperk_expanded  = _expand_df_with_lhs_rhs(all_rules_ripperk, \"RIPPERk\", max_features=15)\n",
    "\n",
    "# Quick peek\n",
    "display(all_rules_dt_expanded)\n",
    "display(all_rules_ripperk_expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e1868",
   "metadata": {},
   "source": [
    "## Coverage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e731512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute rule coverage using normalized labeling folder names and add per-group case IDs\n",
    "\n",
    "BASE_DIR = \"3.2_binned_features\"\n",
    "\n",
    "# Map normalized labeling â†’ folder name\n",
    "LABELING_FOLDER_MAP = {\n",
    "    \"declare\": \"declare_features\",\n",
    "    \"sequential\": \"sequential_features\",\n",
    "    \"payload\": \"payload_features\",\n",
    "}\n",
    "\n",
    "def _find_ci_subdir(parent: str, target: str) -> str | None:\n",
    "    \"\"\"Case-insensitive subdirectory lookup.\"\"\"\n",
    "    t = str(target).lower()\n",
    "    try:\n",
    "        for d in os.listdir(parent):\n",
    "            full = os.path.join(parent, d)\n",
    "            if os.path.isdir(full) and d.lower() == t:\n",
    "                return full\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _resolve_enc_path(dataset: str, labeling: str, encoding: str, base_dir: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Expected layout:\n",
    "      {base_dir}/{Dataset}/{declare|sequential|payload}_features/{Encoding}\n",
    "    Uses case-insensitive matching for the labeling/encoding folders.\n",
    "    \"\"\"\n",
    "    if dataset is None or labeling is None or encoding is None:\n",
    "        return None\n",
    "\n",
    "    ds_dir = os.path.join(base_dir, str(dataset))\n",
    "    if not os.path.isdir(ds_dir):\n",
    "        return None\n",
    "\n",
    "    lab_norm = str(labeling).strip().lower()\n",
    "    lab_folder = LABELING_FOLDER_MAP.get(lab_norm) or f\"{lab_norm}_features\"\n",
    "\n",
    "    lab_dir = os.path.join(ds_dir, lab_folder)\n",
    "    if not os.path.isdir(lab_dir):\n",
    "        lab_dir = _find_ci_subdir(ds_dir, lab_folder)\n",
    "        if not lab_dir:\n",
    "            return None\n",
    "\n",
    "    enc_exact = os.path.join(lab_dir, str(encoding))\n",
    "    if os.path.isdir(enc_exact):\n",
    "        return enc_exact\n",
    "    return _find_ci_subdir(lab_dir, str(encoding))\n",
    "\n",
    "# --- Helpers for coverage ----------------------------------------------------\n",
    "\n",
    "def _infer_case_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"Case_ID\", \"case:concept:name\", \"Case ID\", \"case_id\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\"No Case ID column found (tried: Case_ID, case:concept:name, Case ID, case_id)\")\n",
    "\n",
    "def _norm_numeric(col: pd.Series) -> pd.Series:\n",
    "    if col.dtype == bool:\n",
    "        return col.astype(int)\n",
    "    out = pd.to_numeric(col, errors=\"coerce\")\n",
    "    if out.isna().all() and col.dtype == object:\n",
    "        return col\n",
    "    return out\n",
    "\n",
    "NUM_SUFFIX_RE = re.compile(r\"_(\\-?\\d+(?:\\.\\d+)?)$\")\n",
    "\n",
    "def _match_single_feature(df: pd.DataFrame, feat: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Match a single feature token against the event-level feature columns:\n",
    "      - exact one-hot column\n",
    "      - base_<num> (equality on numeric/string)\n",
    "      - binned base_(...) or base_[...]\n",
    "    Returns a boolean mask.\n",
    "    \"\"\"\n",
    "    feat = str(feat).strip().strip('\"').strip(\"'\")\n",
    "\n",
    "    # A) exact one-hot column\n",
    "    if feat in df.columns:\n",
    "        col = _norm_numeric(df[feat])\n",
    "        return (col == 1) if pd.api.types.is_numeric_dtype(col) else (col.astype(str) == \"1\")\n",
    "\n",
    "    # B) base_<num>\n",
    "    m = NUM_SUFFIX_RE.search(feat)\n",
    "    if m:\n",
    "        base_col = feat[:m.start()]\n",
    "        desired_str = m.group(1)\n",
    "        desired = float(desired_str)\n",
    "        if base_col in df.columns:\n",
    "            col = _norm_numeric(df[base_col])\n",
    "            if pd.api.types.is_numeric_dtype(col):\n",
    "                return (col == desired).fillna(False)\n",
    "            return (col.astype(str) == desired_str).fillna(False)\n",
    "        # fallback: indicator with suffix\n",
    "        if feat in df.columns:\n",
    "            col = _norm_numeric(df[feat])\n",
    "            return ((col == 1) if pd.api.types.is_numeric_dtype(col) else (col.astype(str) == \"1\")).fillna(False)\n",
    "\n",
    "    # C) binned: base_(...) or base_[...]\n",
    "    pos1 = feat.rfind(\"_(\")\n",
    "    pos2 = feat.rfind(\"_[\")\n",
    "    split_pos = max(pos1, pos2)\n",
    "    if split_pos != -1:\n",
    "        base_col = feat[:split_pos]\n",
    "        bin_val  = feat[split_pos + 1 :]  # includes the bracket\n",
    "        if base_col in df.columns:\n",
    "            return (df[base_col].astype(str) == bin_val).fillna(False)\n",
    "\n",
    "    return pd.Series(False, index=df.index)\n",
    "\n",
    "def _match_rule(df: pd.DataFrame, features: list, rhs_label: int) -> pd.Series:\n",
    "    \"\"\"AND all feature matches and enforce RHS label (1=Label, 0=!Label).\"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for f in features:\n",
    "        if f:\n",
    "            mask &= _match_single_feature(df, f)\n",
    "            if not mask.any():\n",
    "                break\n",
    "    if rhs_label in (0, 1):\n",
    "        mask &= (pd.to_numeric(df[\"Label\"], errors=\"coerce\") == rhs_label)\n",
    "    else:\n",
    "        mask &= False\n",
    "    return mask\n",
    "\n",
    "# --- Prepare output frame ----------------------------------------------------\n",
    "\n",
    "if \"all_rules_crm_expanded\" not in globals() or not isinstance(all_rules_crm_expanded, pd.DataFrame):\n",
    "    raise ValueError(\"Expected all_rules_crm_expanded to be present as a DataFrame.\")\n",
    "\n",
    "ENC_COL = \"Encoding\" if \"Encoding\" in all_rules_crm_expanded.columns else \\\n",
    "          (\"Feature Encoding\" if \"Feature Encoding\" in all_rules_crm_expanded.columns else None)\n",
    "if ENC_COL is None:\n",
    "    raise KeyError(\"Could not find 'Encoding' or 'Feature Encoding' in all_rules_crm_expanded.\")\n",
    "\n",
    "crm_rules_all_with_coverage = all_rules_crm_expanded.copy()\n",
    "crm_rules_all_with_coverage[\"covered_case_ids\"] = [[] for _ in range(len(crm_rules_all_with_coverage))]\n",
    "crm_rules_all_with_coverage[\"n_covered_cases\"] = 0\n",
    "crm_rules_all_with_coverage[\"all_case_ids\"] = pd.Series([[]] * len(crm_rules_all_with_coverage), dtype=\"object\")\n",
    "\n",
    "# --- Compute coverage per (Dataset, Labeling, Encoding) ----------------------\n",
    "\n",
    "group_cols = [c for c in [\"Dataset\", \"Labeling\", ENC_COL] if c in crm_rules_all_with_coverage.columns]\n",
    "\n",
    "for keys, g in crm_rules_all_with_coverage.groupby(group_cols):\n",
    "    vals = dict(zip(group_cols, keys))\n",
    "    ds  = vals.get(\"Dataset\")\n",
    "    lab = vals.get(\"Labeling\")\n",
    "    enc = vals.get(ENC_COL)\n",
    "\n",
    "    enc_path = _resolve_enc_path(ds, lab, enc, BASE_DIR)\n",
    "    if enc_path is None:\n",
    "        continue\n",
    "\n",
    "    csv_files = [f for f in os.listdir(enc_path) if f.lower().endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        continue\n",
    "    csv_path = os.path.join(enc_path, csv_files[0])\n",
    "\n",
    "    df_enc = pd.read_csv(csv_path)\n",
    "    if \"Label\" not in df_enc.columns:\n",
    "        continue\n",
    "    case_col = _infer_case_col(df_enc)\n",
    "\n",
    "    # Cache all case IDs for this (dataset, labeling, encoding)\n",
    "    all_ids = df_enc[case_col].dropna().astype(str).unique().tolist()\n",
    "    crm_rules_all_with_coverage.loc[g.index, \"all_case_ids\"] = pd.Series(\n",
    "        [all_ids] * len(g), index=g.index, dtype=\"object\"\n",
    "    )\n",
    "\n",
    "    # Gather feature columns in numeric order\n",
    "    feat_cols = [c for c in crm_rules_all_with_coverage.columns if re.fullmatch(r\"feature_\\d+_lhs\", c)]\n",
    "    feat_cols = sorted(feat_cols, key=lambda x: int(re.findall(r\"\\d+\", x)[0])) if feat_cols else []\n",
    "\n",
    "    # Evaluate coverage per rule\n",
    "    for idx, row in g.iterrows():\n",
    "        feats = [row.get(c, \"\") for c in feat_cols]\n",
    "        feats = [f for f in feats if isinstance(f, str) and f.strip() != \"\"]\n",
    "        rhs = row.get(\"RHS_label\", None)\n",
    "\n",
    "        mask = _match_rule(df_enc, feats, rhs)\n",
    "        covered = df_enc.loc[mask, case_col].dropna().astype(str).unique().tolist()\n",
    "\n",
    "        crm_rules_all_with_coverage.at[idx, \"covered_case_ids\"] = covered\n",
    "        crm_rules_all_with_coverage.at[idx, \"n_covered_cases\"] = len(covered)\n",
    "\n",
    "# Optional: order for readability\n",
    "sort_keys = [c for c in [\"Dataset\", \"Labeling\", ENC_COL, \"n_covered_cases\"] if c in crm_rules_all_with_coverage.columns]\n",
    "if sort_keys:\n",
    "    crm_rules_all_with_coverage = crm_rules_all_with_coverage.sort_values(by=sort_keys).reset_index(drop=True)\n",
    "\n",
    "crm_rules_all_with_coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb73612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-rule coverage for DT & RIPPERK rules and attach all_case_ids\n",
    "# Supported feature predicates include:\n",
    "#   01_HOOFD_011 = 0\n",
    "#   alternate_precedence:(01_HOOFD_011,01_HOOFD_015):Data <= 0.0\n",
    "#   monitoringResource|first|literal_binned_(560925.0, 12941730.0] = 0/1\n",
    "#\n",
    "# Expected folder layout:\n",
    "#   3.2_binned_features/{Dataset}/{declare_features|sequential_features|payload_features}/{Encoding}/*.csv\n",
    "\n",
    "BASE_DIR = \"3.2_binned_features\"\n",
    "LABELING_FOLDER_MAP = {\n",
    "    \"declare\": \"declare_features\",\n",
    "    \"sequential\": \"sequential_features\",\n",
    "    \"payload\": \"payload_features\",\n",
    "}\n",
    "\n",
    "# -------- Path & dataframe utilities ----------------------------------------\n",
    "\n",
    "def _find_ci_subdir(parent: str, target: str) -> str | None:\n",
    "    \"\"\"Return a case-insensitive match for subdirectory `target` inside `parent`.\"\"\"\n",
    "    t = str(target).lower()\n",
    "    try:\n",
    "        for d in os.listdir(parent):\n",
    "            full = os.path.join(parent, d)\n",
    "            if os.path.isdir(full) and d.lower() == t:\n",
    "                return full\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _resolve_enc_path(dataset: str, labeling: str, encoding: str, base_dir: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Resolve:\n",
    "      {base_dir}/{Dataset}/{declare|sequential|payload}_features/{Encoding}\n",
    "    Labeling/encoding are matched case-insensitively.\n",
    "    \"\"\"\n",
    "    if dataset is None or labeling is None or encoding is None:\n",
    "        return None\n",
    "\n",
    "    ds_dir = os.path.join(base_dir, str(dataset))\n",
    "    if not os.path.isdir(ds_dir):\n",
    "        return None\n",
    "\n",
    "    lab_norm = str(labeling).strip().lower()\n",
    "    lab_folder = LABELING_FOLDER_MAP.get(lab_norm, f\"{lab_norm}_features\")\n",
    "\n",
    "    lab_dir = os.path.join(ds_dir, lab_folder)\n",
    "    if not os.path.isdir(lab_dir):\n",
    "        lab_dir = _find_ci_subdir(ds_dir, lab_folder)\n",
    "        if not lab_dir:\n",
    "            return None\n",
    "\n",
    "    enc_exact = os.path.join(lab_dir, str(encoding))\n",
    "    if os.path.isdir(enc_exact):\n",
    "        return enc_exact\n",
    "    return _find_ci_subdir(lab_dir, str(encoding))\n",
    "\n",
    "def _infer_case_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"Case_ID\", \"case:concept:name\", \"Case ID\", \"case_id\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\"No Case ID column found (tried: Case_ID, case:concept:name, Case ID, case_id)\")\n",
    "\n",
    "def _to_numeric(series: pd.Series) -> pd.Series:\n",
    "    if series.dtype == bool:\n",
    "        return series.astype(int)\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def _strip_one_layer_quotes(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if len(s) >= 2 and s[0] == s[-1] and s[0] in {\"'\", '\"'}:\n",
    "        return s[1:-1]\n",
    "    return s\n",
    "\n",
    "# -------- Feature expression parsing & evaluation ---------------------------\n",
    "\n",
    "# Pattern: \"<col> <op> <val>\"  where op âˆˆ {>=, <=, !=, ==, =, >, <}\n",
    "_OP_RE = re.compile(r\"^(?P<col>.+?)\\s*(?P<op>>=|<=|!=|==|=|>|<)\\s*(?P<val>.+?)\\s*$\")\n",
    "\n",
    "def _parse_feature_expr(expr: str):\n",
    "    \"\"\"Return (column, operator, rhs_string) or (None, None, None) if not parsable.\"\"\"\n",
    "    s = str(expr).strip()\n",
    "    m = _OP_RE.match(s)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    col = m.group(\"col\").strip()\n",
    "    op  = \"==\" if m.group(\"op\") == \"=\" else m.group(\"op\")\n",
    "    val_str = _strip_one_layer_quotes(m.group(\"val\"))\n",
    "    return col, op, val_str\n",
    "\n",
    "def _coerce_value(val_str: str):\n",
    "    \"\"\"Coerce RHS to numeric/bool when possible; otherwise keep as string.\"\"\"\n",
    "    low = str(val_str).strip().lower()\n",
    "    if low in {\"true\", \"false\"}:\n",
    "        return 1 if low == \"true\" else 0\n",
    "    try:\n",
    "        num = float(val_str)\n",
    "        return int(num) if num.is_integer() else num\n",
    "    except Exception:\n",
    "        return val_str\n",
    "\n",
    "def _cmp_op(series: pd.Series, op: str, rhs):\n",
    "    \"\"\"\n",
    "    Compare series to rhs. Equality works for numeric and string;\n",
    "    inequalities coerce series to numeric; NaNs evaluate to False.\n",
    "    \"\"\"\n",
    "    if op in (\"==\", \"!=\"):\n",
    "        rhs_is_num = isinstance(rhs, (int, float, np.number))\n",
    "        if rhs_is_num:\n",
    "            s_num = _to_numeric(series)\n",
    "            res = (s_num == rhs) if op == \"==\" else (s_num != rhs)\n",
    "            if op == \"==\":\n",
    "                s_str = series.astype(str).str.strip()\n",
    "                rhs_str = str(rhs)\n",
    "                res = res.fillna(s_str == rhs_str)\n",
    "            else:\n",
    "                res = res.fillna(True)\n",
    "            return res.fillna(False)\n",
    "        else:\n",
    "            s_str = series.astype(str).str.strip()\n",
    "            rhs_str = str(rhs).strip()\n",
    "            return (s_str == rhs_str) if op == \"==\" else (s_str != rhs_str)\n",
    "\n",
    "    # Inequalities\n",
    "    try:\n",
    "        rhs_num = float(rhs)\n",
    "    except Exception:\n",
    "        return pd.Series(False, index=series.index)\n",
    "    s_num = _to_numeric(series)\n",
    "    if op == \">\":\n",
    "        return (s_num > rhs_num).fillna(False)\n",
    "    if op == \"<\":\n",
    "        return (s_num < rhs_num).fillna(False)\n",
    "    if op == \">=\":\n",
    "        return (s_num >= rhs_num).fillna(False)\n",
    "    if op == \"<=\":\n",
    "        return (s_num <= rhs_num).fillna(False)\n",
    "    return pd.Series(False, index=series.index)\n",
    "\n",
    "# Split \"<base>_(bin)\" into (base, \"_(bin)\")\n",
    "_BINVAL_SPLIT_RE = re.compile(r\"(.+?)(_[(\\[][^)\\]]+[)\\]])$\")\n",
    "\n",
    "def _match_single_feature_dt(df: pd.DataFrame, expr: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Evaluate one DT/RIPPERK predicate against df:\n",
    "      â€¢ direct column comparisons (==, !=, >, <, >=, <=)\n",
    "      â€¢ one-hot binned columns: \"<col>_(bin) == 0/1\"\n",
    "      â€¢ label-encoded bins: base column stores \"(bin)\" â†’ interpret \"== 1\" as base == bin\n",
    "    \"\"\"\n",
    "    col, op, val_str = _parse_feature_expr(expr)\n",
    "    if col is None:\n",
    "        return pd.Series(False, index=df.index)\n",
    "\n",
    "    rhs = _coerce_value(val_str)\n",
    "\n",
    "    # Direct column\n",
    "    if col in df.columns:\n",
    "        return _cmp_op(df[col], op, rhs).fillna(False)\n",
    "\n",
    "    # Binned notation\n",
    "    m = _BINVAL_SPLIT_RE.match(col)\n",
    "    if m:\n",
    "        base_col = m.group(1)\n",
    "        bin_label = m.group(2)[1:]  # drop leading underscore\n",
    "\n",
    "        # Try case-insensitive exact column match\n",
    "        lower_map = {c.lower(): c for c in df.columns}\n",
    "        candidate = lower_map.get(col.lower())\n",
    "        if candidate:\n",
    "            return _cmp_op(df[candidate], op, rhs).fillna(False)\n",
    "\n",
    "        # Base column holds the bin string\n",
    "        if base_col in df.columns and op in (\"==\", \"!=\"):\n",
    "            base_series = df[base_col].astype(str)\n",
    "            if isinstance(rhs, (int, float, np.number)) and rhs in (0, 1):\n",
    "                is_bin = (base_series == bin_label)\n",
    "                return (is_bin if (op == \"==\" and rhs == 1) else\n",
    "                        ~is_bin if (op == \"==\" and rhs == 0) else\n",
    "                        ~is_bin if (op == \"!=\" and rhs == 1) else\n",
    "                        is_bin).fillna(False)\n",
    "\n",
    "    return pd.Series(False, index=df.index)\n",
    "\n",
    "def _match_rule_dt(df: pd.DataFrame, features: list, rhs_label: int) -> pd.Series:\n",
    "    \"\"\"Conjoin all feature matches and enforce Label == rhs_label.\"\"\"\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for f in features:\n",
    "        if f:\n",
    "            mask &= _match_single_feature_dt(df, f)\n",
    "            if not mask.any():\n",
    "                break\n",
    "    if rhs_label in (0, 1):\n",
    "        mask &= (pd.to_numeric(df[\"Label\"], errors=\"coerce\") == rhs_label)\n",
    "    else:\n",
    "        mask &= False\n",
    "    return mask\n",
    "\n",
    "def _compute_coverage_for_rules(expanded_df: pd.DataFrame, name_hint: str):\n",
    "    \"\"\"\n",
    "    Compute coverage for a DT/RIPPERK expanded table.\n",
    "    Adds: covered_case_ids, n_covered_cases, all_case_ids.\n",
    "    \"\"\"\n",
    "    if not isinstance(expanded_df, pd.DataFrame):\n",
    "        raise ValueError(f\"{name_hint}: expanded_df must be a DataFrame.\")\n",
    "\n",
    "    # Encoding column name\n",
    "    ENC_COL = \"Encoding\" if \"Encoding\" in expanded_df.columns else \\\n",
    "              (\"Feature Encoding\" if \"Feature Encoding\" in expanded_df.columns else None)\n",
    "    if ENC_COL is None:\n",
    "        raise KeyError(f\"{name_hint}: Could not find 'Encoding' or 'Feature Encoding'.\")\n",
    "\n",
    "    out = expanded_df.copy()\n",
    "    out[\"covered_case_ids\"] = [[] for _ in range(len(out))]\n",
    "    out[\"n_covered_cases\"] = 0\n",
    "    out[\"all_case_ids\"] = pd.Series([[]] * len(out), dtype=\"object\")\n",
    "\n",
    "    group_cols = [c for c in [\"Dataset\", \"Labeling\", ENC_COL] if c in out.columns]\n",
    "    feat_cols = [c for c in out.columns if re.fullmatch(r\"feature_\\d+_lhs\", c)]\n",
    "    feat_cols = sorted(feat_cols, key=lambda x: int(re.findall(r\"\\d+\", x)[0])) if feat_cols else []\n",
    "\n",
    "    for keys, g in out.groupby(group_cols):\n",
    "        vals = dict(zip(group_cols, keys))\n",
    "        ds  = vals.get(\"Dataset\")\n",
    "        lab = vals.get(\"Labeling\")\n",
    "        enc = vals.get(ENC_COL)\n",
    "\n",
    "        enc_path = _resolve_enc_path(ds, lab, enc, BASE_DIR)\n",
    "        if enc_path is None:\n",
    "            continue\n",
    "\n",
    "        csv_files = [f for f in os.listdir(enc_path) if f.lower().endswith(\".csv\")]\n",
    "        if not csv_files:\n",
    "            continue\n",
    "        csv_path = os.path.join(enc_path, csv_files[0])\n",
    "\n",
    "        df_enc = pd.read_csv(csv_path)\n",
    "        if \"Label\" not in df_enc.columns:\n",
    "            continue\n",
    "        case_col = _infer_case_col(df_enc)\n",
    "\n",
    "        # Cache case IDs once per (dataset, labeling, encoding)\n",
    "        all_ids = df_enc[case_col].dropna().astype(str).unique().tolist()\n",
    "        out.loc[g.index, \"all_case_ids\"] = pd.Series([all_ids] * len(g), index=g.index, dtype=\"object\")\n",
    "\n",
    "        for idx, row in g.iterrows():\n",
    "            feats = [row.get(c, \"\") for c in feat_cols]\n",
    "            feats = [f for f in feats if isinstance(f, str) and f.strip() != \"\"]\n",
    "            rhs = row.get(\"RHS_label\", None)\n",
    "\n",
    "            mask = _match_rule_dt(df_enc, feats, rhs)\n",
    "            covered = df_enc.loc[mask, case_col].dropna().astype(str).unique().tolist()\n",
    "\n",
    "            out.at[idx, \"covered_case_ids\"] = covered\n",
    "            out.at[idx, \"n_covered_cases\"] = len(covered)\n",
    "\n",
    "    sort_keys = [c for c in [\"Dataset\", \"Labeling\", ENC_COL, \"n_covered_cases\"] if c in out.columns]\n",
    "    if sort_keys:\n",
    "        out = out.sort_values(by=sort_keys).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---- Build coverage tables --------------------------------------------------\n",
    "if 'all_rules_dt_expanded' in globals():\n",
    "    all_rules_dt_with_coverage = _compute_coverage_for_rules(all_rules_dt_expanded, \"DT\")\n",
    "else:\n",
    "    raise ValueError(\"all_rules_dt_expanded not found. Run the expansion cell first.\")\n",
    "\n",
    "if 'all_rules_ripperk_expanded' in globals():\n",
    "    all_rules_ripperk_with_coverage = _compute_coverage_for_rules(all_rules_ripperk_expanded, \"RIPPERK\")\n",
    "else:\n",
    "    raise ValueError(\"all_rules_ripperk_expanded not found. Run the expansion cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f93c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy 80% coverage on positive class (RHS_label == 1) for CRM, DT, RIPPERK\n",
    "\n",
    "TARGET_COVERAGE = 0.80  # 80%\n",
    "\n",
    "# --- Utilities ---------------------------------------------------------------\n",
    "\n",
    "def _get_enc_col(df: pd.DataFrame) -> str:\n",
    "    if \"Encoding\" in df.columns:\n",
    "        return \"Encoding\"\n",
    "    if \"Feature Encoding\" in df.columns:\n",
    "        return \"Feature Encoding\"\n",
    "    raise KeyError(\"Neither 'Encoding' nor 'Feature Encoding' found.\")\n",
    "\n",
    "def _tie_break_score(row: pd.Series) -> tuple:\n",
    "    \"\"\"Used when two rules add the same number of new cases (higher is better).\"\"\"\n",
    "    return (\n",
    "        float(row.get(\"LB odds ratio\", -np.inf)) if pd.notna(row.get(\"LB odds ratio\", np.nan)) else -np.inf,\n",
    "        float(row.get(\"Precision\", -np.inf)) if pd.notna(row.get(\"Precision\", np.nan)) else -np.inf,\n",
    "        float(row.get(\"Recall\", -np.inf)) if pd.notna(row.get(\"Recall\", np.nan)) else -np.inf,\n",
    "        float(row.get(\"F1\", -np.inf)) if pd.notna(row.get(\"F1\", np.nan)) else -np.inf,\n",
    "        -len(str(row.get(\"Rule\", \"\"))),\n",
    "    )\n",
    "\n",
    "def _greedy_set_cover(group_df: pd.DataFrame, all_cases: set, covered_lists_col: str = \"covered_case_ids\",\n",
    "                      max_rules: int | None = None) -> tuple[list, set]:\n",
    "    \"\"\"\n",
    "    Classic greedy set cover on positive rules.\n",
    "    Returns (selected_rule_indices, covered_cases_set).\n",
    "    \"\"\"\n",
    "    if not len(all_cases):\n",
    "        return [], set()\n",
    "\n",
    "    idxs = list(group_df.index)\n",
    "    rule_sets = {i: set(map(str, group_df.at[i, covered_lists_col])) if isinstance(group_df.at[i, covered_lists_col], list) else set()\n",
    "                 for i in idxs}\n",
    "\n",
    "    selected, covered = [], set()\n",
    "    achievable = set().union(*rule_sets.values()) if rule_sets else set()\n",
    "    if not achievable:\n",
    "        return [], set()\n",
    "\n",
    "    while len(covered) / len(all_cases) < TARGET_COVERAGE:\n",
    "        best_idx, best_gain, best_tiebreak = None, 0, None\n",
    "\n",
    "        for i in idxs:\n",
    "            if i in selected:\n",
    "                continue\n",
    "            gain = len(rule_sets[i] - covered)\n",
    "            if gain > best_gain:\n",
    "                best_idx, best_gain, best_tiebreak = i, gain, _tie_break_score(group_df.loc[i])\n",
    "            elif gain == best_gain and gain > 0:\n",
    "                t = _tie_break_score(group_df.loc[i])\n",
    "                if best_tiebreak is None or t > best_tiebreak:\n",
    "                    best_idx, best_tiebreak = i, t\n",
    "\n",
    "        if best_gain == 0 or best_idx is None:\n",
    "            break\n",
    "\n",
    "        selected.append(best_idx)\n",
    "        covered |= rule_sets[best_idx]\n",
    "\n",
    "        if max_rules is not None and len(selected) >= max_rules:\n",
    "            break\n",
    "\n",
    "    return selected, covered\n",
    "\n",
    "# Paths (reused in several notebooks)\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR = \"3.2_binned_features\"\n",
    "\n",
    "try:\n",
    "    LABELING_FOLDER_MAP\n",
    "except NameError:\n",
    "    LABELING_FOLDER_MAP = {\n",
    "        \"declare\": \"declare_features\",\n",
    "        \"sequential\": \"sequential_features\",\n",
    "        \"payload\": \"payload_features\",\n",
    "    }\n",
    "\n",
    "def _find_ci_subdir(parent: str, target: str) -> str | None:\n",
    "    t = str(target).lower()\n",
    "    try:\n",
    "        for d in os.listdir(parent):\n",
    "            full = os.path.join(parent, d)\n",
    "            if os.path.isdir(full) and d.lower() == t:\n",
    "                return full\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _resolve_enc_path(dataset: str, labeling: str, encoding: str, base_dir: str) -> str | None:\n",
    "    if dataset is None or labeling is None or encoding is None:\n",
    "        return None\n",
    "    ds_dir = os.path.join(base_dir, str(dataset))\n",
    "    if not os.path.isdir(ds_dir):\n",
    "        return None\n",
    "    lab_norm = str(labeling).strip().lower()\n",
    "    lab_folder = LABELING_FOLDER_MAP.get(lab_norm, f\"{lab_norm}_features\")\n",
    "    lab_dir = os.path.join(ds_dir, lab_folder)\n",
    "    if not os.path.isdir(lab_dir):\n",
    "        lab_dir = _find_ci_subdir(ds_dir, lab_folder)\n",
    "        if not lab_dir:\n",
    "            return None\n",
    "    enc_exact = os.path.join(lab_dir, str(encoding))\n",
    "    if os.path.isdir(enc_exact):\n",
    "        return enc_exact\n",
    "    return _find_ci_subdir(lab_dir, str(encoding))\n",
    "\n",
    "def _infer_case_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"Case_ID\", \"case:concept:name\", \"Case ID\", \"case_id\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(\"No Case ID column found (tried: Case_ID, case:concept:name, Case ID, case_id)\")\n",
    "\n",
    "def _positive_universe_for_group(ds, lab, enc, group_df: pd.DataFrame, enc_col: str) -> set:\n",
    "    \"\"\"\n",
    "    Return all positive-class case IDs for (ds, lab, enc).\n",
    "    Fallback: union of covered_case_ids of positive rules when the source file is unavailable.\n",
    "    \"\"\"\n",
    "    path = _resolve_enc_path(ds, lab, enc, BASE_DIR)\n",
    "    if path:\n",
    "        csv_files = [f for f in os.listdir(path) if f.lower().endswith(\".csv\")]\n",
    "        if csv_files:\n",
    "            csv_path = os.path.join(path, csv_files[0])\n",
    "            df_enc = pd.read_csv(csv_path)\n",
    "            if \"Label\" in df_enc.columns:\n",
    "                case_col = _infer_case_col(df_enc)\n",
    "                pos_ids = (\n",
    "                    df_enc.loc[pd.to_numeric(df_enc[\"Label\"], errors=\"coerce\") == 1, case_col]\n",
    "                    .dropna().astype(str).unique().tolist()\n",
    "                )\n",
    "                return set(pos_ids)\n",
    "\n",
    "    pos_lists = group_df.loc[group_df.get(\"RHS_label\", 1) == 1, \"covered_case_ids\"]\n",
    "    pos_lists = [lst for lst in pos_lists if isinstance(lst, list)]\n",
    "    return set(chain.from_iterable(pos_lists)) if pos_lists else set()\n",
    "\n",
    "def _summarize_model_pos(df_with_cov: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Greedy selection per (Dataset, Labeling, Encoding) on positive rules.\n",
    "    Output columns:\n",
    "      Dataset, Labeling, Encoding, {MODEL}_achieved_coverage, {MODEL}_nr_rules\n",
    "    \"\"\"\n",
    "    if not isinstance(df_with_cov, pd.DataFrame):\n",
    "        raise ValueError(f\"{model_name}: input must be a DataFrame.\")\n",
    "\n",
    "    enc_col = _get_enc_col(df_with_cov)\n",
    "    out_rows = []\n",
    "    group_cols = [c for c in [\"Dataset\", \"Labeling\", enc_col] if c in df_with_cov.columns]\n",
    "\n",
    "    for keys, g in df_with_cov.groupby(group_cols, dropna=False):\n",
    "        vals = dict(zip(group_cols, keys))\n",
    "        ds, lab, enc = vals.get(\"Dataset\"), vals.get(\"Labeling\"), vals.get(enc_col)\n",
    "\n",
    "        g_pos = g[g.get(\"RHS_label\", 1) == 1]\n",
    "        pos_all = _positive_universe_for_group(ds, lab, enc, g_pos, enc_col)\n",
    "\n",
    "        selected, covered = _greedy_set_cover(g_pos, pos_all)\n",
    "        achieved = (len(covered) / len(pos_all)) if len(pos_all) else 0.0\n",
    "\n",
    "        out_rows.append({\n",
    "            \"Dataset\": ds,\n",
    "            \"Labeling\": lab,\n",
    "            \"Encoding\": enc,\n",
    "            f\"{model_name}_achieved_coverage\": round(achieved, 4),\n",
    "            f\"{model_name}_nr_rules\": len(selected),\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    if not out.empty:\n",
    "        out = out.sort_values(by=[\"Dataset\", \"Labeling\", \"Encoding\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# --- Run per-model summaries -------------------------------------------------\n",
    "\n",
    "if 'crm_rules_all_with_coverage' not in globals():\n",
    "    raise RuntimeError(\"crm_rules_all_with_coverage not found. Run the CRM coverage cell first.\")\n",
    "if 'all_rules_dt_with_coverage' not in globals():\n",
    "    raise RuntimeError(\"all_rules_dt_with_coverage not found. Run the DT coverage cell first.\")\n",
    "if 'all_rules_ripperk_with_coverage' not in globals():\n",
    "    raise RuntimeError(\"all_rules_ripperk_with_coverage not found. Run the RIPPERK coverage cell first.\")\n",
    "\n",
    "crm_summary_pos     = _summarize_model_pos(crm_rules_all_with_coverage, \"CRM\")\n",
    "dt_summary_pos      = _summarize_model_pos(all_rules_dt_with_coverage, \"DT\")\n",
    "ripperk_summary_pos = _summarize_model_pos(all_rules_ripperk_with_coverage, \"RIPPERK\")\n",
    "\n",
    "# --- Merge summaries ---------------------------------------------------------\n",
    "\n",
    "summary_pos_merged = (\n",
    "    crm_summary_pos\n",
    "    .merge(dt_summary_pos, how=\"outer\", on=[\"Dataset\", \"Labeling\", \"Encoding\"])\n",
    "    .merge(ripperk_summary_pos, how=\"outer\", on=[\"Dataset\", \"Labeling\", \"Encoding\"])\n",
    ")\n",
    "\n",
    "for col in [\"CRM_achieved_coverage\", \"DT_achieved_coverage\", \"RIPPERK_achieved_coverage\"]:\n",
    "    if col in summary_pos_merged.columns:\n",
    "        summary_pos_merged[col] = summary_pos_merged[col].fillna(0.0)\n",
    "\n",
    "for col in [\"CRM_nr_rules\", \"DT_nr_rules\", \"RIPPERK_nr_rules\"]:\n",
    "    if col in summary_pos_merged.columns:\n",
    "        summary_pos_merged[col] = summary_pos_merged[col].fillna(0).astype(int)\n",
    "\n",
    "desired_cols = [\n",
    "    \"Dataset\", \"Labeling\", \"Encoding\",\n",
    "    \"CRM_achieved_coverage\", \"CRM_nr_rules\",\n",
    "    \"DT_achieved_coverage\", \"DT_nr_rules\",\n",
    "    \"RIPPERK_achieved_coverage\", \"RIPPERK_nr_rules\",\n",
    "]\n",
    "summary_pos_merged = summary_pos_merged.reindex(columns=desired_cols)\n",
    "\n",
    "summary_pos_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export positive-class coverage to LaTeX (Traffic only), with 2-decimal coverage values\n",
    "\n",
    "if 'summary_pos_merged' not in globals() or not isinstance(summary_pos_merged, pd.DataFrame):\n",
    "    raise ValueError(\"summary_pos_merged not found. Run the cell that builds it first.\")\n",
    "\n",
    "df = summary_pos_merged.copy()\n",
    "\n",
    "# 1) Keep Traffic rows (case-insensitive)\n",
    "if \"Dataset\" not in df.columns:\n",
    "    raise KeyError(\"Expected a 'Dataset' column in summary_pos_merged.\")\n",
    "mask_traffic = df[\"Dataset\"].astype(str).str.strip().str.lower() == \"traffic\"\n",
    "df = df.loc[mask_traffic].copy()\n",
    "\n",
    "# 2) Determine the encoding column name\n",
    "enc_col = None\n",
    "for cand in (\"Encoding\", \"Feature Encoding\", \"encoding\", \"feature encoding\"):\n",
    "    if cand in df.columns:\n",
    "        enc_col = cand\n",
    "        break\n",
    "if enc_col is None:\n",
    "    raise KeyError(\"No encoding column found (looked for 'Encoding' / 'Feature Encoding').\")\n",
    "\n",
    "# 3) Make encoding LaTeX-safe without altering content\n",
    "df[enc_col] = df[enc_col].astype(str).apply(lambda s: rf\"\\detokenize{{{s}}}\")\n",
    "\n",
    "# 4) Ensure numeric types and format coverage to 2 decimals\n",
    "cov_cols = [\"CRM_achieved_coverage\", \"DT_achieved_coverage\", \"RIPPERK_achieved_coverage\"]\n",
    "for c in cov_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "int_cols = [\"CRM_nr_rules\",\"DT_nr_rules\",\"RIPPERK_nr_rules\"]\n",
    "for c in int_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "def fmt2(x):\n",
    "    return \"\" if pd.isna(x) else f\"{float(x):.2f}\"\n",
    "formatters = {c: fmt2 for c in cov_cols if c in df.columns}\n",
    "\n",
    "# 5) Drop 'Dataset' and arrange columns\n",
    "desired_order = [\n",
    "    \"Labeling\", enc_col,\n",
    "    \"CRM_achieved_coverage\", \"DT_achieved_coverage\", \"RIPPERK_achieved_coverage\",\n",
    "    \"CRM_nr_rules\", \"DT_nr_rules\", \"RIPPERK_nr_rules\",\n",
    "]\n",
    "present = [c for c in desired_order if c in df.columns]\n",
    "df = df.reindex(columns=present)\n",
    "\n",
    "# 6) Write LaTeX\n",
    "latex_path = \"5_analysis/rule_coverage_final.tex\"\n",
    "with open(latex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df.to_latex(index=False, escape=False, formatters=formatters))\n",
    "\n",
    "print(f\"LaTeX table written to: {latex_path}\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504890f5",
   "metadata": {},
   "source": [
    "### Filter best performing rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecba2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_rules_crm_path = os.path.join('5_analysis', 'random','DHL', 'dhl_features' ,'combined_sorted.csv')\n",
    "all_rules_crm_path = os.path.join('5_analysis', 'random', 'combined_sorted_all.csv')\n",
    "crm_rules = pd.read_csv(all_rules_crm_path, sep=',')\n",
    "\n",
    "# all_rules_dt_path = os.path.join('5_analysis', 'dt', 'rules_dt.csv')\n",
    "# dt_rules = pd.read_csv(all_rules_dt_path, sep=',')\n",
    "\n",
    "# all_rules_ripperk_path = os.path.join('5_analysis', 'ripperk', 'rules_ripperk.csv')\n",
    "# ripperk_rules = pd.read_csv(all_rules_ripperk_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f547102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rules whose RHS is exactly 'Label' (positive class)\n",
    "rhs = crm_rules[\"Rule\"].astype(str).str.extract(r\"-->\\s*(!?Label)\\s*$\", expand=False)\n",
    "crm_rules = crm_rules.loc[rhs.eq(\"Label\")].copy()\n",
    "\n",
    "print(f\"Kept {len(crm_rules)} positive rules (Label).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a485e0",
   "metadata": {},
   "source": [
    "### Top 10 overall per labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3363592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LAB_COL = \"Labeling\"\n",
    "METRICS = [\"LB odds ratio\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "TOP_N = 10\n",
    "TIE_BREAKERS = [\"Support LHS\", \"LB odds ratio\"]  # higher is better\n",
    "\n",
    "# Validate inputs\n",
    "missing = [c for c in [LAB_COL] + METRICS if c not in crm_rules.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "def _rank_within_group(g: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rank a single metric within each labeling group (higher = better).\n",
    "    NaNs receive the lowest rank (group_size + 1).\n",
    "    \"\"\"\n",
    "    r = g[col].rank(method=\"dense\", ascending=False)\n",
    "    return r.fillna(len(g) + 1)\n",
    "\n",
    "# Rank metrics within labeling\n",
    "df_ranked = crm_rules.copy()\n",
    "rank_cols = []\n",
    "for m in METRICS:\n",
    "    rcol = f\"rank::{m}\"\n",
    "    df_ranked[rcol] = df_ranked.groupby(LAB_COL, group_keys=False).apply(_rank_within_group, col=m)\n",
    "    rank_cols.append(rcol)\n",
    "\n",
    "# Aggregate ranks (lower = better)\n",
    "df_ranked[\"rank_agg\"] = df_ranked[rank_cols].mean(axis=1)\n",
    "\n",
    "# Select top-N per labeling (primary: rank_agg asc; ties: higher tie-breakers)\n",
    "sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in df_ranked.columns]\n",
    "ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "\n",
    "top10_by_labeling = {\n",
    "    lab: grp.sort_values(sort_cols, ascending=ascending).head(TOP_N).reset_index(drop=True)\n",
    "    for lab, grp in df_ranked.groupby(LAB_COL, dropna=False)\n",
    "}\n",
    "\n",
    "# Convenience handles\n",
    "top10_declare    = top10_by_labeling.get(\"declare\")\n",
    "top10_sequential = top10_by_labeling.get(\"sequential\")\n",
    "top10_payload    = top10_by_labeling.get(\"payload\")\n",
    "\n",
    "# Preview\n",
    "for lab, df_top in top10_by_labeling.items():\n",
    "    print(f\"\\nTop {TOP_N} for labeling = {lab}\")\n",
    "    display(df_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee204139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "out_dir = Path(\"5_analysis\")\n",
    "out_path = out_dir / \"top10_labeling_tables.tex\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Numeric columns to format to 2 decimals (if present)\n",
    "TARGET_DEC_COLS = [\"LB OR\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "\n",
    "# Columns that wonâ€™t be included in the LaTeX export\n",
    "DROP_COLS = {\n",
    "    \"labeling\", \"Labeling\",\n",
    "    \"Odds ratio\", \"UB OR\", \"n12\", \"n21\",\n",
    "    \"Fair set count\", \"Stratified\",\n",
    "    \"rank::LB OR\", \"rank::Support LHS\", \"rank::Confidence\",\n",
    "    \"rank::Lift\", \"rank::Conviction\", \"rank_agg\",\n",
    "}\n",
    "\n",
    "def detok(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Wrap strings in \\\\detokenize{...}; keep NaN as empty.\"\"\"\n",
    "    return s.astype(str).fillna(\"\").map(lambda x: rf\"\\detokenize{{{x}}}\" if x != \"\" else \"\")\n",
    "\n",
    "def prepare_for_latex(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop unneeded columns, apply \\\\detokenize to 'Rule' and encoding,\n",
    "    and coerce target metric columns to numeric for consistent formatting.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    df2 = df.drop(columns=[c for c in df.columns if c in DROP_COLS], errors=\"ignore\").copy()\n",
    "\n",
    "    if \"Rule\" in df2.columns:\n",
    "        df2[\"Rule\"] = detok(df2[\"Rule\"])\n",
    "\n",
    "    if \"Encoding\" in df2.columns:\n",
    "        df2[\"Encoding\"] = detok(df2[\"Encoding\"])\n",
    "    elif \"Feature Encoding\" in df2.columns:\n",
    "        df2[\"Feature Encoding\"] = detok(df2[\"Feature Encoding\"])\n",
    "\n",
    "    for col in TARGET_DEC_COLS:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "    return df2\n",
    "\n",
    "def df_to_latex_block(df: pd.DataFrame, title: str) -> str:\n",
    "    \"\"\"Render a DataFrame to LaTeX with 2-decimal formatting for target columns.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return f\"% {title}: (no rows)\\n\\n\"\n",
    "\n",
    "    formatters = {\n",
    "        col: (lambda x: \"\" if pd.isna(x) else f\"{float(x):.2f}\")\n",
    "        for col in TARGET_DEC_COLS\n",
    "        if col in df.columns\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        f\"% ===== {title} =====\\n\"\n",
    "        + df.to_latex(index=False, escape=False, formatters=formatters)\n",
    "        + \"\\n\\n\"\n",
    "    )\n",
    "\n",
    "# Prepare per-labeling tables\n",
    "declare_tex_df    = prepare_for_latex(top10_declare)\n",
    "sequential_tex_df = prepare_for_latex(top10_sequential)\n",
    "payload_tex_df    = prepare_for_latex(top10_payload)\n",
    "\n",
    "# Write a single .tex file with three blocks\n",
    "latex_parts = [\n",
    "    df_to_latex_block(declare_tex_df,    \"Top 10 â€” declare\"),\n",
    "    df_to_latex_block(sequential_tex_df, \"Top 10 â€” sequential\"),\n",
    "    df_to_latex_block(payload_tex_df,    \"Top 10 â€” payload\"),\n",
    "]\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"% Auto-generated top-10 tables per labeling\\n\\n\")\n",
    "    f.writelines(latex_parts)\n",
    "\n",
    "print(f\"Saved LaTeX tables to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a017319",
   "metadata": {},
   "source": [
    "### DHL top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d74439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration\n",
    "# METRICS = [\"LB OR\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "# TOP_N = 20\n",
    "# TIE_BREAKERS = [\"Support LHS\", \"LB OR\"]  # higher is better\n",
    "\n",
    "# # Validate inputs\n",
    "# missing = [c for c in METRICS if c not in crm_rules.columns]\n",
    "# if missing:\n",
    "#     raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# def _rank_global(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Global ranking for a single metric (higher = better).\n",
    "#     NaNs receive the lowest rank (n + 1).\n",
    "#     \"\"\"\n",
    "#     r = df[col].rank(method=\"dense\", ascending=False)\n",
    "#     return r.fillna(len(df) + 1)\n",
    "\n",
    "# # Rank each metric globally\n",
    "# df_ranked = crm_rules.copy()\n",
    "# rank_cols = []\n",
    "# for m in METRICS:\n",
    "#     rcol = f\"rank::{m}\"\n",
    "#     df_ranked[rcol] = _rank_global(df_ranked, m)\n",
    "#     rank_cols.append(rcol)\n",
    "\n",
    "# # Aggregate ranks (lower = better)\n",
    "# df_ranked[\"rank_agg\"] = df_ranked[rank_cols].mean(axis=1)\n",
    "\n",
    "# # Overall top-N: primary sort by aggregate rank, ties resolved by TIE_BREAKERS (desc)\n",
    "# sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in df_ranked.columns]\n",
    "# ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "\n",
    "# top10_overall = (\n",
    "#     df_ranked\n",
    "#     .sort_values(sort_cols, ascending=ascending)\n",
    "#     .head(TOP_N)\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "# display(top10_overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output file\n",
    "# out_dir = Path(\"5_analysis\")\n",
    "# out_path = out_dir / \"top10_overall.tex\"\n",
    "# out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Numeric columns to format to 2 decimals (if present)\n",
    "# TARGET_DEC_COLS = [\"LB OR\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "\n",
    "# # Columns to drop from export\n",
    "# DROP_COLS = {\n",
    "#     \"labeling\", \"Labeling\",\n",
    "#     \"Odds ratio\", \"UB OR\", \"n12\", \"n21\",\n",
    "#     \"Fair set count\", \"Stratified\",\n",
    "#     \"rank::LB OR\", \"rank::Support LHS\", \"rank::Confidence\",\n",
    "#     \"rank::Lift\", \"rank::Conviction\", \"rank_agg\",\n",
    "# }\n",
    "\n",
    "# def detok(s: pd.Series) -> pd.Series:\n",
    "#     \"\"\"Wrap string values in \\detokenize{...}; keep NaN as empty.\"\"\"\n",
    "#     return s.astype(str).fillna(\"\").map(lambda x: rf\"\\detokenize{{{x}}}\" if x != \"\" else \"\")\n",
    "\n",
    "# def prepare_for_latex(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Remove non-essential columns, apply \\detokenize to text fields,\n",
    "#     and coerce metric columns to numeric for consistent formatting.\n",
    "#     \"\"\"\n",
    "#     if df is None or len(df) == 0:\n",
    "#         return df\n",
    "\n",
    "#     df2 = df.drop(columns=[c for c in df.columns if c in DROP_COLS], errors=\"ignore\").copy()\n",
    "\n",
    "#     if \"Rule\" in df2.columns:\n",
    "#         df2[\"Rule\"] = detok(df2[\"Rule\"])\n",
    "\n",
    "#     if \"Encoding\" in df2.columns:\n",
    "#         df2[\"Encoding\"] = detok(df2[\"Encoding\"])\n",
    "#     elif \"Feature Encoding\" in df2.columns:\n",
    "#         df2[\"Feature Encoding\"] = detok(df2[\"Feature Encoding\"])\n",
    "\n",
    "#     for col in TARGET_DEC_COLS:\n",
    "#         if col in df2.columns:\n",
    "#             df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "#     # Put Rule first if present\n",
    "#     cols = list(df2.columns)\n",
    "#     if \"Rule\" in cols:\n",
    "#         df2 = df2[[\"Rule\"] + [c for c in cols if c != \"Rule\"]]\n",
    "\n",
    "#     return df2\n",
    "\n",
    "# def df_to_latex_block(df: pd.DataFrame, title: str) -> str:\n",
    "#     \"\"\"Render DataFrame to LaTeX with 2-decimal formatting for target columns.\"\"\"\n",
    "#     if df is None or len(df) == 0:\n",
    "#         return f\"% {title}: (no rows)\\n\\n\"\n",
    "\n",
    "#     formatters = {\n",
    "#         col: (lambda x: \"\" if pd.isna(x) else f\"{float(x):.2f}\")\n",
    "#         for col in TARGET_DEC_COLS\n",
    "#         if col in df.columns\n",
    "#     }\n",
    "\n",
    "#     return (\n",
    "#         f\"% ===== {title} =====\\n\"\n",
    "#         + df.to_latex(index=False, escape=False, formatters=formatters)\n",
    "#         + \"\\n\\n\"\n",
    "#     )\n",
    "\n",
    "# # Build LaTeX and write file (expects `top10_overall` from previous step)\n",
    "# overall_tex_df = prepare_for_latex(top10_overall)\n",
    "# latex_text = df_to_latex_block(overall_tex_df, \"Top 10 â€” Overall\")\n",
    "\n",
    "# with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"% Auto-generated top-10 table (overall)\\n\\n\")\n",
    "#     f.write(latex_text)\n",
    "\n",
    "# print(f\"Saved LaTeX table to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76afb43",
   "metadata": {},
   "source": [
    "### Top 3 per encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration\n",
    "# LAB_COL = \"labeling\"   # normalized values: 'declare', 'sequential', 'payload'\n",
    "# METRICS = [\"LB OR\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "# TOP_PER_ENCODING = 3\n",
    "# TIE_BREAKERS = [\"Support LHS\", \"LB OR\"]  # higher is better\n",
    "\n",
    "# # Resolve encoding column name\n",
    "# ENC_CANDIDATES = [\"Encoding\", \"Feature Encoding\", \"Feature encoding\", \"encoding\", \"feature encoding\"]\n",
    "# ENC_COL = next((c for c in ENC_CANDIDATES if c in crm_rules.columns), None)\n",
    "# if ENC_COL is None:\n",
    "#     raise ValueError(f\"Could not find an encoding column among: {ENC_CANDIDATES}\")\n",
    "\n",
    "# # Validate inputs\n",
    "# missing = [c for c in [LAB_COL, ENC_COL] + METRICS if c not in crm_rules.columns]\n",
    "# if missing:\n",
    "#     raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "# def _rank_within_group(g: pd.DataFrame, col: str) -> pd.Series:\n",
    "#     \"\"\"Rank one metric within each labeling group (higher = better).\"\"\"\n",
    "#     r = g[col].rank(method=\"dense\", ascending=False)\n",
    "#     return r.fillna(len(g) + 1)\n",
    "\n",
    "# # Rank metrics within labeling\n",
    "# df_ranked = crm_rules.copy()\n",
    "# rank_cols = []\n",
    "# for m in METRICS:\n",
    "#     rcol = f\"rank::{m}\"\n",
    "#     df_ranked[rcol] = df_ranked.groupby(LAB_COL, group_keys=False).apply(_rank_within_group, col=m)\n",
    "#     rank_cols.append(rcol)\n",
    "\n",
    "# # Aggregate ranks (lower = better; rank 1 is best)\n",
    "# df_ranked[\"rank_agg\"] = df_ranked[rank_cols].mean(axis=1)\n",
    "\n",
    "# # Sort keys: primary by aggregate rank (asc), then tie-breakers (desc)\n",
    "# sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in df_ranked.columns]\n",
    "# ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "\n",
    "# # Select top-K per encoding inside each labeling\n",
    "# def _topk_per_encoding_within_label(grp: pd.DataFrame) -> pd.DataFrame:\n",
    "#     srt = grp.sort_values(sort_cols, ascending=ascending)\n",
    "#     out = srt.groupby(ENC_COL, group_keys=False).head(TOP_PER_ENCODING)\n",
    "#     return out.sort_values(by=ENC_COL).reset_index(drop=True)\n",
    "\n",
    "# top_by_labeling = {\n",
    "#     lab: _topk_per_encoding_within_label(grp).reset_index(drop=True)\n",
    "#     for lab, grp in df_ranked.groupby(LAB_COL, dropna=False)\n",
    "# }\n",
    "\n",
    "# # Convenience handles\n",
    "# top3_declare    = top_by_labeling.get(\"declare\")\n",
    "# top3_sequential = top_by_labeling.get(\"sequential\")\n",
    "# top3_payload    = top_by_labeling.get(\"payload\")\n",
    "\n",
    "# # Preview\n",
    "# for lab, df_top in top_by_labeling.items():\n",
    "#     print(f\"\\nTop {TOP_PER_ENCODING} per encoding for labeling = {lab}\")\n",
    "#     display(df_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10230e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "out_dir = Path(\"5_analysis\")\n",
    "out_path = out_dir / \"top10_labeling_tables.tex\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Numeric columns to format to two decimals (if present)\n",
    "TARGET_DEC_COLS = [\"LB OR\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "\n",
    "# Columns not needed in the export\n",
    "DROP_COLS = {\n",
    "    \"labeling\", \"Labeling\",\n",
    "    \"Odds ratio\", \"UB OR\", \"n12\", \"n21\",\n",
    "    \"Fair set count\", \"Stratified\",\n",
    "    \"rank::LB OR\", \"rank::Support LHS\", \"rank::Confidence\",\n",
    "    \"rank::Lift\", \"rank::Conviction\", \"rank_agg\",\n",
    "}\n",
    "\n",
    "def detok(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Wrap strings in \\detokenize{...}; keep NaN as empty.\"\"\"\n",
    "    return s.astype(str).fillna(\"\").map(lambda x: rf\"\\detokenize{{{x}}}\" if x != \"\" else \"\")\n",
    "\n",
    "def prepare_for_latex(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove non-essential columns, apply \\detokenize to text fields,\n",
    "    and coerce metric columns to numeric for consistent formatting.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    df2 = df.drop(columns=[c for c in df.columns if c in DROP_COLS], errors=\"ignore\").copy()\n",
    "\n",
    "    if \"Rule\" in df2.columns:\n",
    "        df2[\"Rule\"] = detok(df2[\"Rule\"])\n",
    "\n",
    "    if \"Encoding\" in df2.columns:\n",
    "        df2[\"Encoding\"] = detok(df2[\"Encoding\"])\n",
    "    elif \"Feature Encoding\" in df2.columns:\n",
    "        df2[\"Feature Encoding\"] = detok(df2[\"Feature Encoding\"])\n",
    "\n",
    "    for col in TARGET_DEC_COLS:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "    return df2\n",
    "\n",
    "def df_to_latex_block(df: pd.DataFrame, title: str) -> str:\n",
    "    \"\"\"Render a DataFrame to LaTeX with two-decimal formatting for target columns.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        return f\"% {title}: (no rows)\\n\\n\"\n",
    "\n",
    "    formatters = {\n",
    "        col: (lambda x: \"\" if pd.isna(x) else f\"{float(x):.2f}\")\n",
    "        for col in TARGET_DEC_COLS\n",
    "        if col in df.columns\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        f\"% ===== {title} =====\\n\"\n",
    "        + df.to_latex(index=False, escape=False, formatters=formatters)\n",
    "        + \"\\n\\n\"\n",
    "    )\n",
    "\n",
    "# Prepare per-labeling tables\n",
    "declare_tex_df    = prepare_for_latex(top10_declare)\n",
    "sequential_tex_df = prepare_for_latex(top10_sequential)\n",
    "payload_tex_df    = prepare_for_latex(top10_payload)\n",
    "\n",
    "# Write a single .tex file with three blocks\n",
    "latex_parts = [\n",
    "    df_to_latex_block(declare_tex_df,    \"Top 10 â€” declare\"),\n",
    "    df_to_latex_block(sequential_tex_df, \"Top 10 â€” sequential\"),\n",
    "    df_to_latex_block(payload_tex_df,    \"Top 10 â€” payload\"),\n",
    "]\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"% Auto-generated top-10 tables per labeling\\n\\n\")\n",
    "    f.writelines(latex_parts)\n",
    "\n",
    "print(f\"Saved LaTeX tables to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
