{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84fdc31",
   "metadata": {},
   "source": [
    "### DHL qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc299047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Input file\n",
    "csv_path = Path(r\"5_analysis\\random\\DHL\\dhl_features\\k3\\combined_sorted.csv\")\n",
    "\n",
    "# 2) Load data\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 3) Apply rule-quality filter: keep rules with LB OR > 1 and Conviction > 1\n",
    "filtered = df[(df[\"LB odds ratio\"] > 1) & (df[\"Conviction\"] > 1)].copy()\n",
    "\n",
    "print(f\"Loaded {len(df)} rules, kept {len(filtered)} after LB OR > 1 and Conviction > 1 filter.\")\n",
    "filtered.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbdbab",
   "metadata": {},
   "source": [
    "### Ranking per encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed06076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the filtered DataFrame from the previous cell if available; otherwise fall back to df\n",
    "_base = globals().get(\"filtered\", globals().get(\"df\"))\n",
    "if _base is None:\n",
    "    raise RuntimeError(\"No DataFrame named 'filtered' or 'df' found. Run the previous cell first.\")\n",
    "data = _base.copy()\n",
    "\n",
    "# 1) Configuration\n",
    "ENC_COL = \"Feature Encoding\"\n",
    "METRICS = [\"LB odds ratio\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "TOP_K = 2  # number of top rules to keep per encoding\n",
    "TIE_BREAKERS = [\"Support LHS\", \"LB odds ratio\"]  # used to resolve ties (higher is better)\n",
    "\n",
    "# 2) Sanity checks\n",
    "missing = [c for c in [ENC_COL] + METRICS if c not in data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "def _rank_within_group(g: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rank a single metric within each encoding group (higher = better).\n",
    "    NaNs receive the lowest rank (group_size + 1).\n",
    "    \"\"\"\n",
    "    r = g[col].rank(method=\"dense\", ascending=False)\n",
    "    return r.fillna(len(g) + 1)\n",
    "\n",
    "# 3) Rank each metric within its encoding group\n",
    "rank_cols = []\n",
    "for m in METRICS:\n",
    "    rcol = f\"rank::{m}\"\n",
    "    data[rcol] = data.groupby(ENC_COL, group_keys=False).apply(_rank_within_group, col=m)\n",
    "    rank_cols.append(rcol)\n",
    "\n",
    "# 4) Aggregate ranks (lower = better)\n",
    "data[\"rank_agg\"] = data[rank_cols].mean(axis=1)\n",
    "\n",
    "# 5) Sort primarily by aggregate rank, then by tie-breakers\n",
    "sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in data.columns]\n",
    "ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "\n",
    "scored = data.sort_values(sort_cols, ascending=ascending)\n",
    "\n",
    "# 6) Select top-K rules per encoding\n",
    "topk_per_encoding = (\n",
    "    scored.groupby(ENC_COL, group_keys=False)\n",
    "          .head(TOP_K)\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Selected top-{TOP_K} rules per encoding using rank aggregation over: {', '.join(METRICS)}\")\n",
    "topk_per_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42bc08",
   "metadata": {},
   "source": [
    "### Ranking over all encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f84ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer the previously filtered DataFrame; otherwise use the raw 'df'\n",
    "_base = globals().get(\"filtered\", globals().get(\"df\"))\n",
    "if _base is None:\n",
    "    raise RuntimeError(\"No DataFrame named 'filtered' or 'df' found. Run the previous cell first.\")\n",
    "data = _base.copy()\n",
    "\n",
    "# Configuration\n",
    "METRICS = [\"Odds ratio\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "TOP_K = 5  # number of top rules to return overall\n",
    "TIE_BREAKERS = [\"Support LHS\", \"Odds ratio\"]  # applied when aggregate ranks tie\n",
    "ENC_COL = \"Feature Encoding\"\n",
    "RULE_COL = \"Rule\"\n",
    "\n",
    "# Validate inputs\n",
    "missing = [c for c in METRICS if c not in data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected metric columns: {missing}\")\n",
    "\n",
    "# 1) Rank each metric globally (higher value → better rank)\n",
    "rank_cols = []\n",
    "for m in METRICS:\n",
    "    rcol = f\"rank::{m}\"\n",
    "    data[rcol] = data[m].rank(method=\"dense\", ascending=False)\n",
    "    rank_cols.append(rcol)\n",
    "\n",
    "# 2) Aggregate ranks (lower is better because rank 1 is best)\n",
    "data[\"rank_agg\"] = data[rank_cols].mean(axis=1)\n",
    "\n",
    "# 3) Sort by aggregate rank, then apply tie-breakers\n",
    "sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in data.columns]\n",
    "ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "scored_overall = data.sort_values(sort_cols, ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "# 4) Select top-K overall\n",
    "topk_overall = scored_overall.head(TOP_K).copy()\n",
    "print(f\"Selected top-{TOP_K} rules overall using rank aggregation over: {', '.join(METRICS)}\")\n",
    "\n",
    "# 5) Arrange columns for display\n",
    "display_cols = []\n",
    "if RULE_COL in topk_overall.columns:\n",
    "    display_cols.append(RULE_COL)\n",
    "if ENC_COL in topk_overall.columns:\n",
    "    display_cols.append(ENC_COL)\n",
    "display_cols += [c for c in METRICS if c in topk_overall.columns] + [\"rank_agg\"]\n",
    "\n",
    "topk_overall.loc[:, display_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3d612",
   "metadata": {},
   "source": [
    "### Global but then only positive deviance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer the previously filtered DataFrame; otherwise use 'df'\n",
    "_base = globals().get(\"filtered\", globals().get(\"df\"))\n",
    "if _base is None:\n",
    "    raise RuntimeError(\"No DataFrame named 'filtered' or 'df' found. Run the previous cell first.\")\n",
    "data = _base.copy()\n",
    "\n",
    "# 0) Keep only rules with RHS 'Label' (exclude '!Label')\n",
    "# Matches patterns ending with \"... --> Label\" (trailing spaces allowed)\n",
    "rhs_label_mask = data[\"Rule\"].astype(str).str.contains(r\"-->\\s*Label\\s*$\", regex=True, na=False)\n",
    "data = data[rhs_label_mask].copy()\n",
    "\n",
    "# Configuration\n",
    "METRICS = [\"Odds ratio\", \"Support LHS\", \"Confidence\", \"Lift\", \"Conviction\"]\n",
    "TOP_K = 50  # number of top rules to return overall\n",
    "TIE_BREAKERS = [\"Support LHS\", \"Odds ratio\"]  # applied when aggregate ranks tie\n",
    "ENC_COL = \"Feature Encoding\"\n",
    "RULE_COL = \"Rule\"\n",
    "\n",
    "# Validate inputs\n",
    "missing = [c for c in METRICS if c not in data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected metric columns: {missing}\")\n",
    "\n",
    "# 1) Rank each metric globally (higher value → better rank)\n",
    "rank_cols = []\n",
    "for m in METRICS:\n",
    "    rcol = f\"rank::{m}\"\n",
    "    data[rcol] = data[m].rank(method=\"dense\", ascending=False)\n",
    "    rank_cols.append(rcol)\n",
    "\n",
    "# 2) Aggregate ranks (lower is better; rank 1 is best)\n",
    "data[\"rank_agg\"] = data[rank_cols].mean(axis=1)\n",
    "\n",
    "# 3) Sort by aggregate rank and apply tie-breakers\n",
    "sort_cols = [\"rank_agg\"] + [c for c in TIE_BREAKERS if c in data.columns]\n",
    "ascending = [True] + [False] * (len(sort_cols) - 1)\n",
    "scored_overall = data.sort_values(sort_cols, ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "# 4) Select top-K overall\n",
    "topk_overall = scored_overall.head(TOP_K).copy()\n",
    "print(\n",
    "    f\"Selected top-{TOP_K} rules overall with RHS == 'Label', \"\n",
    "    f\"using rank aggregation over: {', '.join(METRICS)} \"\n",
    "    f\"(kept {len(data)} rules after RHS filter).\"\n",
    ")\n",
    "\n",
    "# 5) Arrange columns for display\n",
    "display_cols = []\n",
    "if RULE_COL in topk_overall.columns:\n",
    "    display_cols.append(RULE_COL)\n",
    "if ENC_COL in topk_overall.columns:\n",
    "    display_cols.append(ENC_COL)\n",
    "display_cols += [c for c in METRICS if c in topk_overall.columns] + [\"rank_agg\"]\n",
    "\n",
    "topk_overall.loc[:, display_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b90c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Output paths\n",
    "OUT_DIR = Path(\"5_analysis\")\n",
    "OUT_NAME = \"top_deviant_rules_dhl.tex\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PATH = OUT_DIR / OUT_NAME\n",
    "\n",
    "# 2) Select columns to export\n",
    "wanted_cols = [\n",
    "    \"Rule\",\n",
    "    \"Feature Encoding\",\n",
    "    \"Odds ratio\",\n",
    "    \"Support LHS\",\n",
    "    \"Confidence\",\n",
    "    \"Lift\",\n",
    "    \"Conviction\",\n",
    "]\n",
    "available_cols = [c for c in wanted_cols if c in topk_overall.columns]\n",
    "df_export = topk_overall[available_cols].copy()\n",
    "\n",
    "# 3) LaTeX-safe wrapper for text columns\n",
    "def detok(series: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        series.astype(str)\n",
    "        .fillna(\"\")\n",
    "        .map(lambda x: rf\"\\detokenize{{{x}}}\" if x != \"\" else \"\")\n",
    "    )\n",
    "\n",
    "# 4) Apply \\detokenize to textual fields\n",
    "for col in [\"Rule\", \"Feature Encoding\"]:\n",
    "    if col in df_export.columns:\n",
    "        df_export[col] = detok(df_export[col])\n",
    "\n",
    "# 5) Render LaTeX (keep \\detokenize unescaped)\n",
    "latex_str = df_export.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    longtable=False,\n",
    "    float_format=\"%.3f\",\n",
    ")\n",
    "\n",
    "# 6) Write file\n",
    "OUT_PATH.write_text(latex_str, encoding=\"utf-8\")\n",
    "print(f\"Wrote LaTeX table to: {OUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
