{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5e6c7f",
   "metadata": {},
   "source": [
    "## This notebook requires to be runned in Python 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74488b6f",
   "metadata": {},
   "source": [
    "## 0) Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72913232",
   "metadata": {},
   "source": [
    "### 0.1) Configure environment and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries for filesystem and module management\n",
    "import os            \n",
    "import sys \n",
    "import csv          \n",
    "import shutil       \n",
    "import importlib\n",
    "from functools import reduce\n",
    "from typing import List, Dict\n",
    "import pandas as pd \n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "# Add repository root to PYTHONPATH to enable absolute imports of local modules\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd()))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d6330",
   "metadata": {},
   "source": [
    "### 0.2) Define paths, constants and create output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(os.path.join(PROJECT_ROOT, 'config', \"config_feature_extraction.yaml\"), \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# File and folder paths for each processing stage\n",
    "EVENT_LOG           = cfg[\"event_log\"]\n",
    "EVENT_LOG_SETTINGS  = cfg[\"event_log_settings\"]\n",
    "exp_name            = cfg[\"experiment_name\"]\n",
    "RAW_LOG             = os.path.join(PROJECT_ROOT, '0_raw_log', cfg[\"raw_log\"])\n",
    "UNIQUE_DIR          = os.path.join(PROJECT_ROOT, '1_unique_log')\n",
    "LABELED_DIR         = os.path.join(PROJECT_ROOT, '2_labelled_logs', EVENT_LOG)\n",
    "FEATURES_DIR        = os.path.join(PROJECT_ROOT, '3_extracted_features', EVENT_LOG)\n",
    "\n",
    "# Parameters controlling data split and sequence filtering\n",
    "SPLIT_RATIO   = 0.7  # fraction of events used for training\n",
    "SEQ_THRESHOLD = 5    # minimum events in a sequence to keep\n",
    "MAX_SPLITS    = 1    # number of times to split the log\n",
    "\n",
    "# Create folders for downstream outputs if they don't already exist\n",
    "os.makedirs(UNIQUE_DIR,  exist_ok=True)\n",
    "os.makedirs(LABELED_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# print configuration settings\n",
    "print(f\"Event Log: {EVENT_LOG}\")\n",
    "#print(f\"Event Log Settings: {conf_settings}\")\n",
    "print(f\"Experiment: {exp_name}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ca18b",
   "metadata": {},
   "source": [
    "### 0.3) Load modules for log preparation, tagging, splitting, and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a13ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Log preparation and tagging utilities ===\n",
    "from helper_functions.preparation.RetagLogWithUniqueIds         import changeLog\n",
    "from helper_functions.preparation.TaggingStrategy               import TaggingStrategy\n",
    "from helper_functions.preparation.ConfigurationFile             import ConfigurationFile\n",
    "from helper_functions.preparation.PayloadType                   import PayloadType\n",
    "from helper_functions.preparation.declaretemplates_new          import template_response, template_exist\n",
    "from helper_functions.preparation.LogTaggingViaPredicates       import (\n",
    "    tagLogWithOccurrence,\n",
    "    tagLogWithValueEqOverEventAttn,\n",
    "    tagLogWithSatAllProp,\n",
    "    tagLogWithExactOccurrence,\n",
    "    tagLogWithValueEqOverIthEventAttn,\n",
    "    SatCases\n",
    ")\n",
    "\n",
    "# === Log splitting and feature extraction helpers ===\n",
    "from helper_functions.feature_extraction.deviancecommon         import read_XES_log\n",
    "from helper_functions.feature_extraction.FileNameUtils          import csv_trace_encodings\n",
    "from helper_functions.feature_extraction.TraceUtils             import getTraceId\n",
    "from helper_functions.feature_extraction.LogUtils               import (\n",
    "    xes_to_propositional_split,\n",
    "    xes_to_data_propositional_split,\n",
    "    xes_to_tracelist_split\n",
    ")\n",
    "\n",
    "# Feauture extraction helpers\n",
    "from helper_functions.feature_extraction.baseline_runner         import baseline_embedding\n",
    "from helper_functions.feature_extraction.sequence_runner         import generateSequences, run_sequences\n",
    "from helper_functions.feature_extraction.declaredevmining        import declare_embedding\n",
    "from helper_functions.feature_extraction.ddm_newmethod_fixed_new import declare_data_aware_embedding\n",
    "from helper_functions.feature_extraction.payload_extractor       import payload_embedding\n",
    "\n",
    "# File utilities\n",
    "from helper_functions.feature_extraction.PandaExpress            import dataframe_multiway_equijoin, fast_csv_parse, ensureDataFrameQuality\n",
    "from helper_functions.feature_extraction.DumpUtils               import read_single_arff_dump, read_arff_embedding_dump, dump_custom_dataframes\n",
    "from helper_functions.extra_functions                            import (\n",
    "    build_conf,\n",
    "    _make_value_eq,\n",
    "    _make_occurrence,\n",
    "    _make_sat_all_prop,\n",
    "    read_sequence_log_via_arff,\n",
    "    internal_to_folder,\n",
    "    read_all_numeric_columns_except_one,\n",
    "    read_feature_csv,\n",
    "    dataframe_join_with_checks,\n",
    "    dataframe_multiway_equijoin,\n",
    "    multijoined_dump_no_splits,\n",
    "    dump_all_compositions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19361b",
   "metadata": {},
   "source": [
    "## 1) Tagging & Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad833c01",
   "metadata": {},
   "source": [
    "### 1.1) Retag raw log with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply unique IDs to each case and event in the raw XES log\n",
    "unique_log, log_obj = changeLog(RAW_LOG, output_dir=UNIQUE_DIR)\n",
    "\n",
    "# Confirm where the retagged file was saved\n",
    "print(f\"✔ Retagged log written to: {unique_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b014d",
   "metadata": {},
   "source": [
    "### 1.2) Build default configuration and define labelling strategies (Uncomment one per run based on which log is being processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfbd1d",
   "metadata": {},
   "source": [
    "#### TRAFFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a89036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: create a ConfigurationFile pre-populated with shared Traffic settings\n",
    "def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "    cf = ConfigurationFile()\n",
    "    cf.setExperimentName(exp_name)\n",
    "    cf.setLogName(os.path.basename(unique_log))      # use the retagged filename\n",
    "    cf.setOutputFolder(\"TRAFFIC\")                    # base output subfolder\n",
    "    cf.setMaxDepth(5)                                # decision-tree max depth\n",
    "    cf.setMinLeaf(5)                                 # minimum samples per leaf\n",
    "    cf.setSequenceThreshold(SEQ_THRESHOLD)           # filter short sequences\n",
    "    cf.setPayloadType(PayloadType.both)              # include control flow & data payload\n",
    "    cf.setAutoIgnore([                               \n",
    "        \"time:timestamp\", \"concept: name\", \"Label\",  # ignore these event attributes\n",
    "        \"Start date\", \"End date\", \"Diagnosis\", \n",
    "        \"Diagnosis code\", \"Diagnosis Treatment\", \n",
    "        \"Combination ID\", \"Treatment code\", \"Activity code\"\n",
    "    ])\n",
    "    cf.setPayloadSettings(\"traffic_settings.cfg\")   # load custom payload rules\n",
    "    return cf\n",
    "\n",
    "# Dictionary containing the labeling functions for writing labels to the log\n",
    "labeling_map = {\n",
    "    \"traffic_payload_Pay36\": lambda log: tagLogWithValueEqOverEventAttn(\n",
    "        log, \"paymentAmount\", 36.0\n",
    "    ),\n",
    "    \"traffic_mr_tr\": lambda log: tagLogWithOccurrence(\n",
    "        log, [\"Add penalty\", \"Payment\"], 1\n",
    "    ),\n",
    "    \"traffic_decl3\": lambda log: tagLogWithSatAllProp(\n",
    "        log,\n",
    "        [(template_response, [\"Insert Date Appeal to Prefecture\", \"Add penalty\"])],\n",
    "        SatCases.NotVacuitySat\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482f52b",
   "metadata": {},
   "source": [
    "#### SEPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: create a ConfigurationFile pre-populated with shared Sepsis settings\n",
    "def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "    cf = ConfigurationFile()\n",
    "    cf.setExperimentName(exp_name)\n",
    "    cf.setLogName(os.path.basename(unique_log))       # use the retagged filename\n",
    "    cf.setOutputFolder(\"SEPSIS\")                     # base output subfolder\n",
    "    cf.setMaxDepth(5)                                 # decision-tree max depth\n",
    "    cf.setMinLeaf(5)                                  # minimum samples per leaf\n",
    "    cf.setSequenceThreshold(SEQ_THRESHOLD)            # filter short sequences\n",
    "    cf.setPayloadType(PayloadType.both)               # include control flow & data payload\n",
    "    cf.setAutoIgnore([\n",
    "        \"Diagnosis\", \"Diagnose\",                     # ignore these attributes\n",
    "        \"time:timestamp\", \"concept: name\", \"Label\",\n",
    "        \"lifecycle: transition\"\n",
    "    ])\n",
    "    cf.setPayloadSettings(\"sepsis_settings.cfg\")      # load custom payload rules\n",
    "    return cf\n",
    "\n",
    "# Dictionary containing the labeling functions for writing labels to the log\n",
    "labeling_map = {\n",
    "    \"sepsis_payload2\": lambda log: tagLogWithValueEqOverIthEventAttn(\n",
    "        log, \"DisfuncOrg\", True, 0\n",
    "    ),\n",
    "    \"sepsis_mr_tr\": lambda log: tagLogWithExactOccurrence(\n",
    "        log, [\"Admission NC\", \"CRP\", \"Leucocytes\"], 1\n",
    "    ),\n",
    "    \"sepsis_decl\": lambda log: tagLogWithSatAllProp(\n",
    "        log,\n",
    "        [\n",
    "            (template_response, [\"IV Antibiotics\", \"Leucocytes\"]),\n",
    "            (template_response, [\"LacticAcid\", \"IV Antibiotics\"]),\n",
    "            (template_response, [\"ER Triage\", \"CRP\"])\n",
    "        ],\n",
    "        SatCases.NotVacuitySat\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113633d",
   "metadata": {},
   "source": [
    "#### BPI15A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f082162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper: create a ConfigurationFile pre-populated with shared BPI15A settings\n",
    "# def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "#     cf = ConfigurationFile()\n",
    "#     cf.setExperimentName(exp_name)\n",
    "#     cf.setLogName(os.path.basename(unique_log))        # use the retagged filename\n",
    "#     cf.setOutputFolder(\"BPI15A\")                       # base output subfolder\n",
    "#     cf.setMaxDepth(5)                                  # decision-tree max depth\n",
    "#     cf.setMinLeaf(5)                                   # minimum samples per leaf\n",
    "#     cf.setSequenceThreshold(SEQ_THRESHOLD)             # filter short sequences\n",
    "#     cf.setPayloadType(PayloadType.both)                # include control flow & data payload\n",
    "#     cf.setAutoIgnore([\n",
    "#         \"time:timestamp\", \"concept: name\", \"Label\",   # ignore these event attributes\n",
    "#         \"Start date\", \"End date\", \"Diagnosis\",\n",
    "#         \"Diagnosis code\", \"Diagnosis Treatment\",\n",
    "#         \"Combination ID\", \"Treatment code\", \"Activity code\",\n",
    "#         \"dateFinished\", \"panned\", \"dueDate\"\n",
    "#     ])\n",
    "#     cf.setPayloadSettings(\"bpi2015_settings.cfg\")     # load custom payload rules\n",
    "#     return cf\n",
    "\n",
    "# # Dictionary containing the labeling functions for writing labels to the log\n",
    "# labeling_map = {\n",
    "#     \"bpi15A_payload_560925\": lambda log: tagLogWithValueEqOverEventAttn(\n",
    "#         log, \"monitoringResource\", \"560925\"\n",
    "#     ),\n",
    "#     \"bpi15A_mr_tr\": lambda log: tagLogWithOccurrence(\n",
    "#         log, [\"08_AWB45_005\", \"01_HOOFD_200\"], 1\n",
    "#     ),\n",
    "#     \"bpi15A_decl2\": lambda log: tagLogWithSatAllProp(\n",
    "#         log,\n",
    "#         [(template_exist, [\"01_HOOFD_011\"])],\n",
    "#         SatCases.NotVacuitySat\n",
    "#     )\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64533470",
   "metadata": {},
   "source": [
    "### 1.3) Apply each labeling strategy and write out labeled logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ea0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each experiment name and its associated labeling function\n",
    "for exp_name, label_func in labeling_map.items():\n",
    "    print(f\"\\n▶ Processing label set: {exp_name}\")\n",
    "    \n",
    "    # Build and serialize the configuration for this labeling experiment\n",
    "    cf = build_conf(exp_name)\n",
    "    json_path = os.path.join(LABELED_DIR, f\"{exp_name}.json\")\n",
    "    cf.dump(json_path)\n",
    "    \n",
    "    # Initialize and execute the tagging strategy on the log object\n",
    "    tagger = TaggingStrategy(exp_name, label_func)\n",
    "    tagger(LABELED_DIR, cf, log_obj)\n",
    "    \n",
    "    # Assemble and print the paths of the outputs just generated\n",
    "    xes_path = os.path.join(LABELED_DIR, tagger.logname)\n",
    "    print(f\"   • Labeled XES → {xes_path}\")\n",
    "    print(f\"   • Config JSON → {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c7d84",
   "metadata": {},
   "source": [
    "## 2) Preparing Feature Extraction & Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b53c66",
   "metadata": {},
   "source": [
    "### 2.1) Prepare labeled log and output directory for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name      = cfg[\"experiment_name\"]\n",
    "\n",
    "# Paths to the labeled XES file and its configuration\n",
    "labeled_log = os.path.join(LABELED_DIR, f'{exp_name}.xes')\n",
    "json_config = os.path.join(LABELED_DIR, f'{exp_name}.json')\n",
    "\n",
    "# Create a folder to store extracted features for this experiment\n",
    "features_folder  = os.path.join(FEATURES_DIR, f'{exp_name}_features')\n",
    "os.makedirs(features_folder, exist_ok=True)\n",
    "\n",
    "# Load the labeled XES log into memory as a list of traces\n",
    "log = read_XES_log(labeled_log)\n",
    "print(f\"✔ Loaded '{exp_name}' → {len(log)} total traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0593b8",
   "metadata": {},
   "source": [
    "### 2.2) Generate propositional and canonical representations of the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all trace IDs for downstream reference\n",
    "trace_ids = {getTraceId(t) for t in log}\n",
    "\n",
    "# Convert each trace into a propositional format for sequence & baseline (IA) models\n",
    "propositional_log = xes_to_propositional_split(log)\n",
    "print(\"✔ Propositional traces ready\")\n",
    "\n",
    "# Include event data attributes in the propositional conversion (required for DWD)\n",
    "data_log = xes_to_data_propositional_split(log, doForce=False)\n",
    "print(\"✔ Data propositional traces ready\")\n",
    "\n",
    "# Create a simple list of traces in case-based format (used by certain embeddings)\n",
    "trace_list = xes_to_tracelist_split(log)\n",
    "print(\"✔ Canonical trace list ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d108519",
   "metadata": {},
   "source": [
    "## 3) Individual‐Activity Frequencies (Baseline Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dda167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subfolder for Individual‐Activity Frequencies features within the experiment’s output directory\n",
    "baseline_folder = os.path.join(features_folder, 'baseline')\n",
    "os.makedirs(baseline_folder, exist_ok=True)\n",
    "\n",
    "# Run the baseline embedding procedure on the full propositional log\n",
    "processed_ids = baseline_embedding(\n",
    "    baseline_folder,\n",
    "    propositional_log,\n",
    "    None\n",
    ")\n",
    "\n",
    "# Display a summary of what was generated\n",
    "print(f\"✔ Baseline features written to: {baseline_folder}\")\n",
    "print(f\"  • {len(processed_ids)} total traces processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c429e5f",
   "metadata": {},
   "source": [
    "## 4) Sequential Encoding (MR / TR / TRA / MRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839009f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "log_file_name      = f\"{exp_name}.xes\"\n",
    "sequence_folder    = os.path.join(features_folder, \"sequence\")\n",
    "error_log_folder   = os.path.join(sequence_folder, \"errors\")\n",
    "\n",
    "# os.makedirs(sequence_folder, exist_ok=True)\n",
    "# os.makedirs(error_log_folder, exist_ok=True)\n",
    "\n",
    "# # Run it\n",
    "# strategies = run_sequences(\n",
    "#     inp_path=LABELED_DIR,\n",
    "#     log_path=log_file_name,\n",
    "#     results_folder=sequence_folder,\n",
    "#     sequence_threshold=SEQ_THRESHOLD,\n",
    "#     err_logger=error_log_folder,\n",
    "# )\n",
    "\n",
    "strategies = generateSequences(\n",
    "    inp_path=LABELED_DIR,\n",
    "    log_path=log_file_name,\n",
    "    results_folder=sequence_folder,\n",
    "    sequence_threshold=SEQ_THRESHOLD,\n",
    "    err_logger=error_log_folder\n",
    ")\n",
    "\n",
    "print(\"→ Mined strategies:\", strategies)\n",
    "print(\"🔍 If anything went wrong, check logs in:\", error_log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664b50e",
   "metadata": {},
   "source": [
    "### 4.1) Reorganize strategy folders and clean up intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base folders for feature outputs and the intermediate split\n",
    "split_folder   = os.path.join(sequence_folder, 'split1')\n",
    "\n",
    "# Only proceed if the first split exists\n",
    "if os.path.isdir(split_folder):\n",
    "    for strategy in os.listdir(split_folder):\n",
    "        src = os.path.join(split_folder, strategy)\n",
    "        dst = os.path.join(features_folder, strategy)\n",
    "\n",
    "        # Remove any pre-existing folder at the destination\n",
    "        if os.path.exists(dst):\n",
    "            shutil.rmtree(dst)\n",
    "\n",
    "        # Move each strategy folder up one level\n",
    "        shutil.move(src, dst)\n",
    "        print(f\"→ Moved {strategy} to {dst}\")\n",
    "\n",
    "    # Remove the now-empty intermediate sequence directory\n",
    "    shutil.rmtree(sequence_folder)\n",
    "    print(f\"→ Removed sequence folder at {sequence_folder}\")\n",
    "else:\n",
    "    print(f\"No split directory found at {split_folder}, nothing moved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfe822",
   "metadata": {},
   "source": [
    "### 4.2) Write CSV for every individual sequential encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['mr', 'mra', 'tr', 'tra']\n",
    "\n",
    "# Loop through each sequence encoding strategy\n",
    "for enc in encodings:\n",
    "    # Ensure the folder for this encoding exists\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "    os.makedirs(enc_folder, exist_ok=True)\n",
    "\n",
    "    # Load train/test DataFrames for the current encoding\n",
    "    tr_df, te_df = read_sequence_log_via_arff(features_folder, enc)\n",
    "    # Combine both sets into a full dataset\n",
    "    total_df = pd.concat([tr_df, te_df])\n",
    "\n",
    "    # Define output file paths for total, train, and test splits\n",
    "    paths = {\n",
    "        'total': os.path.join(enc_folder, f'{enc}.csv'),\n",
    "        'train': os.path.join(enc_folder, f'{enc}_train.csv'),\n",
    "        'test':  os.path.join(enc_folder, f'{enc}_test.csv'),\n",
    "    }\n",
    "\n",
    "    # Write each DataFrame to disk with semicolon separators\n",
    "    total_df.to_csv(paths['total'], index=False)\n",
    "    tr_df.to_csv(paths['train'], index=False)\n",
    "    te_df.to_csv(paths['test'], index=False)\n",
    "\n",
    "    # Print confirmation of written files\n",
    "    print(f\"✔ {enc}: wrote total → {paths['total']}\")\n",
    "    print(f\"       train → {paths['train']}\")\n",
    "    print(f\"       test  → {paths['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a448acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sequence encoding strategies\n",
    "encodings = ['mr', 'mra', 'tr', 'tra']\n",
    "\n",
    "# Loop through each encoding\n",
    "for enc in encodings:\n",
    "    # Ensure the folder for this encoding exists\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "    os.makedirs(enc_folder, exist_ok=True)\n",
    "\n",
    "    # Load train/test DataFrames for the current encoding\n",
    "    tr_df, te_df = read_sequence_log_via_arff(features_folder, enc)\n",
    "    # Combine both sets into a full dataset\n",
    "    total_df = pd.concat([tr_df, te_df])\n",
    "\n",
    "    # Define output file path for total split\n",
    "    total_path = os.path.join(enc_folder, f'{enc}.csv')\n",
    "\n",
    "    # Write only the total DataFrame to disk with semicolon separators\n",
    "    total_df.to_csv(total_path, index=False)\n",
    "\n",
    "    # Print confirmation of written file\n",
    "    print(f\"✔ {enc}: wrote total → {total_path}\")\n",
    "\n",
    "    # # Delete any other CSV files in the folder so only the total remains\n",
    "    # for file_path in glob.glob(os.path.join(enc_folder, '*.csv')):\n",
    "    #     if file_path != total_path:\n",
    "    #         os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183e51",
   "metadata": {},
   "source": [
    "### 4.3) Provide proper column naming (no collisions when merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for enc in encodings:\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "    \n",
    "    # 1) load the mapping.txt\n",
    "    mapping_path = os.path.join(enc_folder, 'mapping.txt')\n",
    "    mapping_df   = pd.read_csv(mapping_path, sep=';', header=0)\n",
    "    code_to_name = dict(zip(mapping_df['activityCode'], mapping_df['activityName']))\n",
    "    \n",
    "    # 2) for each split, read → rename → write\n",
    "    for split, suffix in [('total', ''), ('train', '_train'), ('test', '_test')]:\n",
    "        csv_path = os.path.join(enc_folder, f\"{enc}{suffix}.csv\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Build new column names\n",
    "        new_columns = []\n",
    "        for col in df.columns:\n",
    "            # Split the column name into 3-character codes\n",
    "            codes = [col[i:i+3] for i in range(0, len(col), 3)]\n",
    "            # Map codes to activity names (skip codes not in mapping)\n",
    "            names = [code_to_name[c] for c in codes if c in code_to_name]\n",
    "            if names:\n",
    "                new_col = f\"{enc}[{', '.join(names)}]\"\n",
    "            else:\n",
    "                new_col = col  # Leave unchanged if no codes found\n",
    "            new_columns.append(new_col)\n",
    "        \n",
    "        df.columns = new_columns\n",
    "        \n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"✔ {enc}{suffix}.csv  →  columns renamed and saved\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9a467",
   "metadata": {},
   "source": [
    "## 5) Declarative Encoding (using Declare templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output folder for Declare features\n",
    "declare_folder = os.path.join(features_folder, 'declare')\n",
    "os.makedirs(declare_folder, exist_ok=True)\n",
    "\n",
    "# Run the Declare embedding on the full propositional log\n",
    "processed_ids = declare_embedding(\n",
    "    declare_folder,\n",
    "    propositional_log,   # full-log, no split\n",
    "    None,                # legacy placeholder\n",
    "    None,                # no ExperimentRunner\n",
    "    reencode=False,\n",
    "    candidate_threshold=SPLIT_RATIO,\n",
    "    constraint_threshold=SPLIT_RATIO\n",
    ")\n",
    "\n",
    "# Print output summary\n",
    "print(f\"✔ Declare features written to: {declare_folder}\")\n",
    "print(f\"  • {len(processed_ids)} total traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811321a4",
   "metadata": {},
   "source": [
    "## 6) Data-Aware Declare Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output folder for Data-Aware Declare features\n",
    "dwd_folder = os.path.join(features_folder, 'dwd')\n",
    "os.makedirs(dwd_folder, exist_ok=True)\n",
    "\n",
    "# Attributes to ignore\n",
    "ignored_attrs = [\n",
    "    \"time:timestamp\", \"concept: name\", \"Label\", \"Start date\", \"End date\",\n",
    "    \"Diagnosis\", \"Diagnosis code\", \"Diagnosis Treatment\", \"Combination ID\",\n",
    "    \"Treatment code\", \"Activity code\"\n",
    "]\n",
    "\n",
    "# Run the Data-Aware Declare embedding on the full data_log\n",
    "processed_ids = declare_data_aware_embedding(\n",
    "    ignored_attrs,\n",
    "    dwd_folder,\n",
    "    data_log,               # entire log, no split\n",
    "    None,                   # legacy placeholder\n",
    "    missing_literal=None,\n",
    "    self=None,              # in-memory runner (or your runner instance)\n",
    "    candidate_threshold=SPLIT_RATIO,\n",
    "    constraint_threshold=SPLIT_RATIO\n",
    ")\n",
    "\n",
    "# Output summary\n",
    "print(f\"✔ Data-Aware Declare features written to: {dwd_folder}\")\n",
    "print(f\" • {len(processed_ids)} total traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289ab28",
   "metadata": {},
   "source": [
    "## 7) Payload Encoding (Event & Trace Attribute Aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f15c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output folder for payload features\n",
    "payload_folder = os.path.join(features_folder, 'payload')\n",
    "os.makedirs(payload_folder, exist_ok=True)\n",
    "\n",
    "# Define the path to the payload configuration file (empty for traffic)\n",
    "payload_settings = os.path.join(PROJECT_ROOT, 'log_settings', EVENT_LOG_SETTINGS)\n",
    "#payload_settings = \"\"\n",
    "\n",
    "processed_ids = payload_embedding(\n",
    "    payload_folder,\n",
    "    payload_settings,\n",
    "    trace_list,\n",
    "    None,\n",
    "    None\n",
    ")\n",
    "\n",
    "# Output summary\n",
    "print(f\"✔ Payload features written to: {payload_folder}\")\n",
    "print(f\"  • {len(processed_ids)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d6e1e",
   "metadata": {},
   "source": [
    "## 8) Feature concatenations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28faa96",
   "metadata": {},
   "source": [
    "### 8.1) Combine sequential feature encodings across all strategies into 'sequential_combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac37873",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = ['mr', 'mra', 'tr', 'tra']\n",
    "\n",
    "sequential_combined_folder = os.path.join(features_folder, 'seq_combined')\n",
    "os.makedirs(sequential_combined_folder, exist_ok=True)\n",
    "\n",
    "splits = {\n",
    "    \"total\": \"\",\n",
    "    \"train\": \"_train\",\n",
    "    \"test\": \"_test\"\n",
    "}\n",
    "\n",
    "def load_and_merge(split_suffix):\n",
    "    dfs = []\n",
    "    for i, enc in enumerate(encodings):\n",
    "        csv_path = os.path.join(features_folder, enc, f\"{enc}{split_suffix}.csv\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if i > 0 and 'Label' in df.columns:\n",
    "            df = df.drop(columns=['Label'])\n",
    "        dfs.append(df)\n",
    "    merged = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged = pd.merge(merged, df, on='Case_ID', how='outer')\n",
    "    return merged\n",
    "\n",
    "for split, suffix in splits.items():\n",
    "    combined_df = load_and_merge(suffix)\n",
    "    out_path = os.path.join(sequential_combined_folder, f\"seq_combined{suffix}.csv\")\n",
    "    combined_df.to_csv(out_path, index=False)\n",
    "    print(f\"✔ Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0635810",
   "metadata": {},
   "source": [
    "### 8.2) Make sure all Case_ID's are set to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all strategies inside the features folder\n",
    "strategies = [d for d in os.listdir(features_folder) if os.path.isdir(os.path.join(features_folder, d))]\n",
    "\n",
    "# For every csv, loop and set the index to Case_ID\n",
    "for strat in strategies:\n",
    "    # Define the path to the CSV file for this strategy\n",
    "    csv_path = os.path.join(features_folder, strat, f\"{strat}.csv\")\n",
    "    \n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Set Case_ID as the index\n",
    "    df.set_index('Case_ID', inplace=True)\n",
    "    \n",
    "    # Save the modified DataFrame back to CSV\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "    print(f\"✔ Set index for {strat} → {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2f7ac",
   "metadata": {},
   "source": [
    "### 8.3) Concatenating new combinations of extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composition = cfg[\"dataset_composition\"]\n",
    "folder_to_internal = cfg[\"folder_to_internal\"]\n",
    "internal_to_folder_map = internal_to_folder(folder_to_internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to join a single composition:\n",
    "# joined_df = multijoined_dump_no_splits(\n",
    "#     key=\"baseline_data\",\n",
    "#     dataset_list=dataset_composition[\"baseline_data\"],\n",
    "#     base_folder=features_folder,\n",
    "#     folder_to_internal_map=folder_to_internal\n",
    "# )\n",
    "\n",
    "# or to dump all:\n",
    "dump_all_compositions(\n",
    "    dataset_composition,\n",
    "    base_folder=features_folder,\n",
    "    folder_to_internal_map=folder_to_internal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece14ac6",
   "metadata": {},
   "source": [
    "## 9) Some final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all strategies inside the features folder\n",
    "strategies = [d for d in os.listdir(features_folder) if os.path.isdir(os.path.join(features_folder, d))]\n",
    "\n",
    "# For every csv, loop and set the index to Case_ID\n",
    "for strat in strategies:\n",
    "    # Define the path to the CSV file for this strategy\n",
    "    csv_path = os.path.join(features_folder, strat, f\"{strat}.csv\")\n",
    "    \n",
    "    # Read the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Set Case_ID as the index\n",
    "    df.set_index('Case_ID', inplace=True)\n",
    "    \n",
    "    # Save the modified DataFrame back to CSV\n",
    "    df.to_csv(csv_path)\n",
    "    \n",
    "    print(f\"✔ Set index for {strat} → {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
