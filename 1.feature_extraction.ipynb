{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74488b6f",
   "metadata": {},
   "source": [
    "## 0) Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72913232",
   "metadata": {},
   "source": [
    "### 0.1) Configure environment and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import shutil\n",
    "import importlib\n",
    "from functools import reduce\n",
    "from typing import List, Dict\n",
    "import glob\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Add repository root to PYTHONPATH to enable absolute imports of local modules\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d6330",
   "metadata": {},
   "source": [
    "### 0.2) Define paths, constants and create output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(os.path.join(PROJECT_ROOT, \"config\", \"config_feature_extraction.yaml\"), \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Paths\n",
    "EVENT_LOG          = cfg[\"event_log\"]\n",
    "EVENT_LOG_SETTINGS = cfg[\"event_log_settings\"]\n",
    "exp_name           = cfg[\"experiment_name\"]\n",
    "RAW_LOG            = os.path.join(PROJECT_ROOT, \"0_raw_log\", cfg[\"raw_log\"])\n",
    "UNIQUE_DIR         = os.path.join(PROJECT_ROOT, \"1_unique_log\")\n",
    "LABELED_DIR        = os.path.join(PROJECT_ROOT, \"2_labelled_logs\", EVENT_LOG)\n",
    "FEATURES_DIR       = os.path.join(PROJECT_ROOT, \"3_extracted_features\", EVENT_LOG)\n",
    "\n",
    "# Processing parameters\n",
    "SPLIT_RATIO   = 0.7  # fraction of events used for training\n",
    "SEQ_THRESHOLD = 5    # minimum number of events required per sequence\n",
    "MAX_SPLITS    = 1    # number of train/test splits\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(UNIQUE_DIR, exist_ok=True)\n",
    "os.makedirs(LABELED_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"Event Log: {EVENT_LOG}\")\n",
    "print(f\"Experiment: {exp_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ca18b",
   "metadata": {},
   "source": [
    "### 0.3) Load modules for log preparation, tagging, splitting, and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a13ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log preparation and tagging utilities\n",
    "from helper_functions.preparation.RetagLogWithUniqueIds         import changeLog\n",
    "from helper_functions.preparation.TaggingStrategy               import TaggingStrategy\n",
    "from helper_functions.preparation.ConfigurationFile             import ConfigurationFile\n",
    "from helper_functions.preparation.PayloadType                   import PayloadType\n",
    "from helper_functions.preparation.declaretemplates_new          import template_response, template_exist\n",
    "from helper_functions.preparation.LogTaggingViaPredicates       import (\n",
    "    tagLogWithOccurrence,\n",
    "    tagLogWithValueEqOverEventAttn,\n",
    "    tagLogWithSatAllProp,\n",
    "    tagLogWithExactOccurrence,\n",
    "    tagLogWithValueEqOverIthEventAttn,\n",
    "    SatCases\n",
    ")\n",
    "\n",
    "# Log splitting and feature extraction helpers\n",
    "from helper_functions.feature_extraction.deviancecommon         import read_XES_log\n",
    "from helper_functions.feature_extraction.FileNameUtils          import csv_trace_encodings\n",
    "from helper_functions.feature_extraction.TraceUtils             import getTraceId\n",
    "from helper_functions.feature_extraction.LogUtils               import (\n",
    "    xes_to_propositional_split,\n",
    "    xes_to_data_propositional_split,\n",
    "    xes_to_tracelist_split\n",
    ")\n",
    "\n",
    "# Feauture extraction helpers\n",
    "from helper_functions.feature_extraction.baseline_runner         import baseline_embedding\n",
    "from helper_functions.feature_extraction.sequence_runner         import generateSequences, run_sequences\n",
    "from helper_functions.feature_extraction.declaredevmining        import declare_embedding\n",
    "from helper_functions.feature_extraction.ddm_newmethod_fixed_new import declare_data_aware_embedding\n",
    "from helper_functions.feature_extraction.payload_extractor       import payload_embedding\n",
    "\n",
    "# File utilities\n",
    "from helper_functions.feature_extraction.PandaExpress            import dataframe_multiway_equijoin, fast_csv_parse, ensureDataFrameQuality\n",
    "from helper_functions.feature_extraction.DumpUtils               import read_single_arff_dump, read_arff_embedding_dump, dump_custom_dataframes\n",
    "from helper_functions.extra_functions                            import (\n",
    "    build_conf,\n",
    "    _make_value_eq,\n",
    "    _make_occurrence,\n",
    "    _make_sat_all_prop,\n",
    "    read_sequence_log_via_arff,\n",
    "    internal_to_folder,\n",
    "    read_all_numeric_columns_except_one,\n",
    "    read_feature_csv,\n",
    "    dataframe_join_with_checks,\n",
    "    dataframe_multiway_equijoin,\n",
    "    multijoined_dump_no_splits,\n",
    "    dump_all_compositions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19361b",
   "metadata": {},
   "source": [
    "## 1) Tagging & Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad833c01",
   "metadata": {},
   "source": [
    "### 1.1) Retag raw log with unique identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign stable case and event identifiers to the raw XES log\n",
    "unique_log, log_obj = changeLog(RAW_LOG, output_dir=UNIQUE_DIR)\n",
    "\n",
    "# Report output path\n",
    "print(f\"Retagged log written to: {unique_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b014d",
   "metadata": {},
   "source": [
    "### 1.2) Build default configuration and define labelling strategies (Uncomment one per run based on which log is being processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfbd1d",
   "metadata": {},
   "source": [
    "#### TRAFFIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a89036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ConfigurationFile with shared settings for the Traffic experiments\n",
    "def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "    cf = ConfigurationFile()\n",
    "    cf.setExperimentName(exp_name)\n",
    "    cf.setLogName(os.path.basename(unique_log))     # use the retagged filename\n",
    "    cf.setOutputFolder(\"TRAFFIC\")                   # base output subfolder\n",
    "    cf.setMaxDepth(5)                               # decision-tree max depth\n",
    "    cf.setMinLeaf(5)                                # minimum samples per leaf\n",
    "    cf.setSequenceThreshold(SEQ_THRESHOLD)          # drop short sequences\n",
    "    cf.setPayloadType(PayloadType.both)             # include control-flow and payload\n",
    "    cf.setAutoIgnore([\n",
    "        \"time:timestamp\", \"concept: name\", \"Label\", # ignore non-informative attributes\n",
    "        \"Start date\", \"End date\", \"Diagnosis\",\n",
    "        \"Diagnosis code\", \"Diagnosis Treatment\",\n",
    "        \"Combination ID\", \"Treatment code\", \"Activity code\"\n",
    "    ])\n",
    "    cf.setPayloadSettings(\"traffic_settings.cfg\")   # custom payload rules\n",
    "    return cf\n",
    "\n",
    "# Labeling functions used to annotate the log for different experiments\n",
    "labeling_map = {\n",
    "    \"traffic_payload_Pay36\": lambda log: tagLogWithValueEqOverEventAttn(\n",
    "        log, \"paymentAmount\", 36.0\n",
    "    ),\n",
    "    \"traffic_mr_tr\": lambda log: tagLogWithOccurrence(\n",
    "        log, [\"Add penalty\", \"Payment\"], 1\n",
    "    ),\n",
    "    \"traffic_decl3\": lambda log: tagLogWithSatAllProp(\n",
    "        log,\n",
    "        [(template_response, [\"Insert Date Appeal to Prefecture\", \"Add penalty\"])],\n",
    "        SatCases.NotVacuitySat\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482f52b",
   "metadata": {},
   "source": [
    "#### SEPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a ConfigurationFile with shared settings for Sepsis experiments\n",
    "# def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "#     cf = ConfigurationFile()\n",
    "#     cf.setExperimentName(exp_name)\n",
    "#     cf.setLogName(os.path.basename(unique_log))      # retagged filename\n",
    "#     cf.setOutputFolder(\"SEPSIS\")\n",
    "#     cf.setMaxDepth(5)                                # decision-tree max depth\n",
    "#     cf.setMinLeaf(5)                                 # minimum samples per leaf\n",
    "#     cf.setSequenceThreshold(SEQ_THRESHOLD)           # drop short sequences\n",
    "#     cf.setPayloadType(PayloadType.both)              # control-flow + payload\n",
    "#     cf.setAutoIgnore([\n",
    "#         \"Diagnosis\", \"Diagnose\",\n",
    "#         \"time:timestamp\", \"concept: name\", \"Label\",\n",
    "#         \"lifecycle: transition\",\n",
    "#     ])\n",
    "#     cf.setPayloadSettings(\"sepsis_settings.cfg\")\n",
    "#     return cf\n",
    "\n",
    "# # Labeling functions for Sepsis experiments\n",
    "# labeling_map = {\n",
    "#     \"sepsis_payload2\": lambda log: tagLogWithValueEqOverIthEventAttn(\n",
    "#         log, \"DisfuncOrg\", True, 0\n",
    "#     ),\n",
    "#     \"sepsis_mr_tr\": lambda log: tagLogWithExactOccurrence(\n",
    "#         log, [\"Admission NC\", \"CRP\", \"Leucocytes\"], 1\n",
    "#     ),\n",
    "#     \"sepsis_decl\": lambda log: tagLogWithSatAllProp(\n",
    "#         log,\n",
    "#         [\n",
    "#             (template_response, [\"IV Antibiotics\", \"Leucocytes\"]),\n",
    "#             (template_response, [\"LacticAcid\", \"IV Antibiotics\"]),\n",
    "#             (template_response, [\"ER Triage\", \"CRP\"]),\n",
    "#         ],\n",
    "#         SatCases.NotVacuitySat,\n",
    "#     ),\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113633d",
   "metadata": {},
   "source": [
    "#### BPI15A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f082162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a ConfigurationFile with shared settings for BPI15A experiments\n",
    "# def build_conf(exp_name: str) -> ConfigurationFile:\n",
    "#     cf = ConfigurationFile()\n",
    "#     cf.setExperimentName(exp_name)\n",
    "#     cf.setLogName(os.path.basename(unique_log))      # retagged filename\n",
    "#     cf.setOutputFolder(\"BPI15A\")\n",
    "#     cf.setMaxDepth(5)                                # decision-tree max depth\n",
    "#     cf.setMinLeaf(5)                                 # minimum samples per leaf\n",
    "#     cf.setSequenceThreshold(SEQ_THRESHOLD)           # drop short sequences\n",
    "#     cf.setPayloadType(PayloadType.both)              # control-flow + payload\n",
    "#     cf.setAutoIgnore([\n",
    "#         \"time:timestamp\", \"concept: name\", \"Label\",\n",
    "#         \"Start date\", \"End date\", \"Diagnosis\",\n",
    "#         \"Diagnosis code\", \"Diagnosis Treatment\",\n",
    "#         \"Combination ID\", \"Treatment code\", \"Activity code\",\n",
    "#         \"dateFinished\", \"panned\", \"dueDate\",\n",
    "#     ])\n",
    "#     cf.setPayloadSettings(\"bpi2015_settings.cfg\")\n",
    "#     return cf\n",
    "\n",
    "# # Labeling functions for BPI15A experiments\n",
    "# labeling_map = {\n",
    "#     \"bpi15A_payload_560925\": lambda log: tagLogWithValueEqOverEventAttn(\n",
    "#         log, \"monitoringResource\", \"560925\"\n",
    "#     ),\n",
    "#     \"bpi15A_mr_tr\": lambda log: tagLogWithOccurrence(\n",
    "#         log, [\"08_AWB45_005\", \"01_HOOFD_200\"], 1\n",
    "#     ),\n",
    "#     \"bpi15A_decl2\": lambda log: tagLogWithSatAllProp(\n",
    "#         log,\n",
    "#         [(template_exist, [\"01_HOOFD_011\"])],\n",
    "#         SatCases.NotVacuitySat,\n",
    "#     ),\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64533470",
   "metadata": {},
   "source": [
    "### 1.3) Apply each labeling strategy and write out labeled logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ea0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each labeling experiment and persist its outputs\n",
    "for exp_name, label_func in labeling_map.items():\n",
    "    print(f\"\\nProcessing label set: {exp_name}\")\n",
    "\n",
    "    # Build and save configuration for this experiment\n",
    "    cf = build_conf(exp_name)\n",
    "    json_path = os.path.join(LABELED_DIR, f\"{exp_name}.json\")\n",
    "    cf.dump(json_path)\n",
    "\n",
    "    # Apply labeling to the log\n",
    "    tagger = TaggingStrategy(exp_name, label_func)\n",
    "    tagger(LABELED_DIR, cf, log_obj)\n",
    "\n",
    "    # Report generated artifacts\n",
    "    xes_path = os.path.join(LABELED_DIR, tagger.logname)\n",
    "    print(f\"  Labeled XES:  {xes_path}\")\n",
    "    print(f\"  Config JSON:  {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c7d84",
   "metadata": {},
   "source": [
    "## 2) Preparing Feature Extraction & Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b53c66",
   "metadata": {},
   "source": [
    "### 2.1) Prepare labeled log and output directory for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fadca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = cfg[\"experiment_name\"]\n",
    "\n",
    "# Paths to labeled log and its config\n",
    "labeled_log = os.path.join(LABELED_DIR, f\"{exp_name}.xes\")\n",
    "json_config = os.path.join(LABELED_DIR, f\"{exp_name}.json\")\n",
    "\n",
    "# Output folder for extracted features\n",
    "features_folder = os.path.join(FEATURES_DIR, f\"{exp_name}_features\")\n",
    "os.makedirs(features_folder, exist_ok=True)\n",
    "\n",
    "# Load labeled log\n",
    "log = read_XES_log(labeled_log)\n",
    "print(f\"Loaded '{exp_name}': {len(log)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0593b8",
   "metadata": {},
   "source": [
    "### 2.2) Generate propositional and canonical representations of the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect trace identifiers for later joins and checks\n",
    "trace_ids = {getTraceId(t) for t in log}\n",
    "\n",
    "# Propositional representation for sequence/baseline models\n",
    "propositional_log = xes_to_propositional_split(log)\n",
    "print(\"Propositional traces ready\")\n",
    "\n",
    "# Propositional representation including event attributes (for DWD)\n",
    "data_log = xes_to_data_propositional_split(log, doForce=False)\n",
    "print(\"Data-propositional traces ready\")\n",
    "\n",
    "# Case-based list representation (used by specific embeddings)\n",
    "trace_list = xes_to_tracelist_split(log)\n",
    "print(\"Trace list ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d108519",
   "metadata": {},
   "source": [
    "## 3) Individual‐Activity Frequencies (Baseline Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dda167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline (Individual-Activity Frequencies) features\n",
    "baseline_folder = os.path.join(features_folder, \"baseline\")\n",
    "os.makedirs(baseline_folder, exist_ok=True)\n",
    "\n",
    "# Compute baseline embedding on the propositional log\n",
    "processed_ids = baseline_embedding(\n",
    "    baseline_folder,\n",
    "    propositional_log,\n",
    "    None\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"Baseline features written to: {baseline_folder}\")\n",
    "print(f\"Processed traces: {len(processed_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c429e5f",
   "metadata": {},
   "source": [
    "## 4) Sequential Encoding (MR / TR / TRA / MRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839009f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "log_file_name    = f\"{exp_name}.xes\"\n",
    "sequence_folder  = os.path.join(features_folder, \"sequence\")\n",
    "error_log_folder = os.path.join(sequence_folder, \"errors\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(sequence_folder, exist_ok=True)\n",
    "os.makedirs(error_log_folder, exist_ok=True)\n",
    "\n",
    "# Mine sequential strategies (n-gram–like patterns) from the labeled log\n",
    "# Note: an alternative runner `run_sequences(...)` is available but not used here.\n",
    "strategies = generateSequences(\n",
    "    inp_path=LABELED_DIR,\n",
    "    log_path=log_file_name,\n",
    "    results_folder=sequence_folder,\n",
    "    sequence_threshold=SEQ_THRESHOLD,\n",
    "    err_logger=error_log_folder,\n",
    ")\n",
    "\n",
    "print(\"Mined strategies:\", strategies)\n",
    "print(\"Error logs (if any):\", error_log_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664b50e",
   "metadata": {},
   "source": [
    "### 4.1) Reorganize strategy folders and clean up intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote sequence outputs from the intermediate split folder to the features root\n",
    "split_folder = os.path.join(sequence_folder, \"split1\")\n",
    "\n",
    "if os.path.isdir(split_folder):\n",
    "    for strategy in os.listdir(split_folder):\n",
    "        src = os.path.join(split_folder, strategy)\n",
    "        dst = os.path.join(features_folder, strategy)\n",
    "\n",
    "        # Replace any existing destination with the fresh result\n",
    "        if os.path.exists(dst):\n",
    "            shutil.rmtree(dst)\n",
    "\n",
    "        shutil.move(src, dst)\n",
    "        print(f\"Moved {strategy} → {dst}\")\n",
    "\n",
    "    # Clean up the intermediate directory\n",
    "    shutil.rmtree(sequence_folder)\n",
    "    print(f\"Removed intermediate folder: {sequence_folder}\")\n",
    "else:\n",
    "    print(f\"No split directory at {split_folder}; nothing to move.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfe822",
   "metadata": {},
   "source": [
    "### 4.2) Write CSV for every individual sequential encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = [\"mr\", \"mra\", \"tr\", \"tra\"]\n",
    "\n",
    "for enc in encodings:\n",
    "    # Output folder for this encoding\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "    os.makedirs(enc_folder, exist_ok=True)\n",
    "\n",
    "    # Load train/test splits and combine to a full set\n",
    "    tr_df, te_df = read_sequence_log_via_arff(features_folder, enc)\n",
    "    total_df = pd.concat([tr_df, te_df])\n",
    "\n",
    "    # Output paths\n",
    "    paths = {\n",
    "        \"total\": os.path.join(enc_folder, f\"{enc}.csv\"),\n",
    "        \"train\": os.path.join(enc_folder, f\"{enc}_train.csv\"),\n",
    "        \"test\":  os.path.join(enc_folder, f\"{enc}_test.csv\"),\n",
    "    }\n",
    "\n",
    "    # Persist to CSV (semicolon separator expected downstream)\n",
    "    total_df.to_csv(paths[\"total\"], index=False, sep=\";\")\n",
    "    tr_df.to_csv(paths[\"train\"], index=False, sep=\";\")\n",
    "    te_df.to_csv(paths[\"test\"],  index=False, sep=\";\")\n",
    "\n",
    "    # Log written files\n",
    "    print(f\"{enc}: wrote total → {paths['total']}\")\n",
    "    print(f\"     train → {paths['train']}\")\n",
    "    print(f\"     test  → {paths['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a448acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence encodings to export\n",
    "encodings = [\"mr\", \"mra\", \"tr\", \"tra\"]\n",
    "\n",
    "for enc in encodings:\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "    os.makedirs(enc_folder, exist_ok=True)\n",
    "\n",
    "    # Load train/test splits and combine to a full set\n",
    "    tr_df, te_df = read_sequence_log_via_arff(features_folder, enc)\n",
    "    total_df = pd.concat([tr_df, te_df])\n",
    "\n",
    "    # Write combined split to CSV (semicolon delimiter expected downstream)\n",
    "    total_path = os.path.join(enc_folder, f\"{enc}.csv\")\n",
    "    total_df.to_csv(total_path, index=False, sep=\";\")\n",
    "\n",
    "    print(f\"{enc}: wrote total → {total_path}\")\n",
    "\n",
    "    # Optional: keep only the combined file\n",
    "    # for file_path in glob.glob(os.path.join(enc_folder, \"*.csv\")):\n",
    "    #     if file_path != total_path:\n",
    "    #         os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f183e51",
   "metadata": {},
   "source": [
    "### 4.3) Provide proper column naming (no collisions when merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for enc in encodings:\n",
    "    enc_folder = os.path.join(features_folder, enc)\n",
    "\n",
    "    # Load activity code → name mapping\n",
    "    mapping_path = os.path.join(enc_folder, \"mapping.txt\")\n",
    "    mapping_df   = pd.read_csv(mapping_path, sep=\";\", header=0)\n",
    "    code_to_name = dict(zip(mapping_df[\"activityCode\"], mapping_df[\"activityName\"]))\n",
    "\n",
    "    # Rename feature columns in each split based on mapped activity names\n",
    "    for split, suffix in [(\"total\", \"\"), (\"train\", \"_train\"), (\"test\", \"_test\")]:\n",
    "        csv_path = os.path.join(enc_folder, f\"{enc}{suffix}.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue  # split may not exist\n",
    "\n",
    "        # Preserve delimiter consistency (files were written with ';')\n",
    "        df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "        # Translate 3-char activity codes in column names to readable labels\n",
    "        new_columns = []\n",
    "        for col in df.columns:\n",
    "            codes = [col[i:i+3] for i in range(0, len(col), 3)]\n",
    "            names = [code_to_name[c] for c in codes if c in code_to_name]\n",
    "            new_columns.append(f\"{enc}[{', '.join(names)}]\" if names else col)\n",
    "\n",
    "        df.columns = new_columns\n",
    "        df.to_csv(csv_path, index=False, sep=\";\")\n",
    "        print(f\"{enc}{suffix}.csv → columns renamed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9a467",
   "metadata": {},
   "source": [
    "## 5) Declarative Encoding (using Declare templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cdb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare features\n",
    "declare_folder = os.path.join(features_folder, \"declare\")\n",
    "os.makedirs(declare_folder, exist_ok=True)\n",
    "\n",
    "# Compute Declare embedding on the full propositional log\n",
    "processed_ids = declare_embedding(\n",
    "    declare_folder,\n",
    "    propositional_log,        # full log (no split)\n",
    "    None,                     # placeholder for legacy API\n",
    "    None,                     # no ExperimentRunner\n",
    "    reencode=False,\n",
    "    candidate_threshold=SPLIT_RATIO,\n",
    "    constraint_threshold=SPLIT_RATIO,\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"Declare features written to: {declare_folder}\")\n",
    "print(f\"Total traces: {len(processed_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811321a4",
   "metadata": {},
   "source": [
    "## 6) Data-Aware Declare Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-Aware Declare (DWD) features\n",
    "dwd_folder = os.path.join(features_folder, \"dwd\")\n",
    "os.makedirs(dwd_folder, exist_ok=True)\n",
    "\n",
    "# Event attributes to exclude from data-aware constraints\n",
    "ignored_attrs = [\n",
    "    \"time:timestamp\", \"concept: name\", \"Label\", \"Start date\", \"End date\",\n",
    "    \"Diagnosis\", \"Diagnosis code\", \"Diagnosis Treatment\", \"Combination ID\",\n",
    "    \"Treatment code\", \"Activity code\",\n",
    "]\n",
    "\n",
    "# Compute DWD embedding on the full data-aware propositional log\n",
    "processed_ids = declare_data_aware_embedding(\n",
    "    ignored_attrs,\n",
    "    dwd_folder,\n",
    "    data_log,                 # full log (no split)\n",
    "    None,                     # legacy placeholder\n",
    "    missing_literal=None,\n",
    "    self=None,                # in-memory runner (or a runner instance)\n",
    "    candidate_threshold=SPLIT_RATIO,\n",
    "    constraint_threshold=SPLIT_RATIO,\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"Data-Aware Declare features written to: {dwd_folder}\")\n",
    "print(f\"Total traces: {len(processed_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289ab28",
   "metadata": {},
   "source": [
    "## 7) Payload Encoding (Event & Trace Attribute Aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f15c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payload features\n",
    "payload_folder = os.path.join(features_folder, \"payload\")\n",
    "os.makedirs(payload_folder, exist_ok=True)\n",
    "\n",
    "# Path to payload configuration (empty for some logs, e.g., Traffic)\n",
    "payload_settings = os.path.join(PROJECT_ROOT, \"log_settings\", EVENT_LOG_SETTINGS)\n",
    "# payload_settings = \"\"  # optional override\n",
    "\n",
    "processed_ids = payload_embedding(\n",
    "    payload_folder,\n",
    "    payload_settings,\n",
    "    trace_list,\n",
    "    None,\n",
    "    None,\n",
    ")\n",
    "\n",
    "# Summary\n",
    "print(f\"Payload features written to: {payload_folder}\")\n",
    "print(f\"Total traces: {len(processed_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d6e1e",
   "metadata": {},
   "source": [
    "## 8) Feature concatenations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28faa96",
   "metadata": {},
   "source": [
    "### 8.1) Combine sequential feature encodings across all strategies into 'sequential_combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac37873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sequential encodings (mr, mra, tr, tra) into a single feature set per split\n",
    "encodings = [\"mr\", \"mra\", \"tr\", \"tra\"]\n",
    "\n",
    "sequential_combined_folder = os.path.join(features_folder, \"seq_combined\")\n",
    "os.makedirs(sequential_combined_folder, exist_ok=True)\n",
    "\n",
    "# Split suffixes used in filenames\n",
    "splits = {\"total\": \"\", \"train\": \"_train\", \"test\": \"_test\"}\n",
    "\n",
    "def load_and_merge(split_suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"Load each encoding for a given split and merge on Case_ID.\"\"\"\n",
    "    dfs = []\n",
    "    for i, enc in enumerate(encodings):\n",
    "        csv_path = os.path.join(features_folder, enc, f\"{enc}{split_suffix}.csv\")\n",
    "        df = pd.read_csv(csv_path, sep=\";\")\n",
    "        # Keep a single Label column (from the first dataframe)\n",
    "        if i > 0 and \"Label\" in df.columns:\n",
    "            df = df.drop(columns=[\"Label\"])\n",
    "        dfs.append(df)\n",
    "\n",
    "    merged = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        merged = pd.merge(merged, df, on=\"Case_ID\", how=\"outer\")\n",
    "    return merged\n",
    "\n",
    "for split, suffix in splits.items():\n",
    "    combined_df = load_and_merge(suffix)\n",
    "    out_path = os.path.join(sequential_combined_folder, f\"seq_combined{suffix}.csv\")\n",
    "    combined_df.to_csv(out_path, index=False, sep=\";\")\n",
    "    print(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0635810",
   "metadata": {},
   "source": [
    "### 8.2) Make sure all Case_ID's are set to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Case_ID as the index for each strategy's combined CSV\n",
    "strategies = [d for d in os.listdir(features_folder)\n",
    "              if os.path.isdir(os.path.join(features_folder, d))]\n",
    "\n",
    "for strat in strategies:\n",
    "    csv_path = os.path.join(features_folder, strat, f\"{strat}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue  # skip folders without a combined CSV\n",
    "\n",
    "    # Files in this pipeline use semicolon as delimiter\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "    df.set_index(\"Case_ID\", inplace=True)\n",
    "    df.to_csv(csv_path, sep=\";\")\n",
    "\n",
    "    print(f\"Set index for {strat} → {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2f7ac",
   "metadata": {},
   "source": [
    "### 8.3) Concatenating new combinations of extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_composition = cfg[\"dataset_composition\"]\n",
    "folder_to_internal = cfg[\"folder_to_internal\"]\n",
    "internal_to_folder_map = internal_to_folder(folder_to_internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to join a single composition:\n",
    "# joined_df = multijoined_dump_no_splits(\n",
    "#     key=\"baseline_data\",\n",
    "#     dataset_list=dataset_composition[\"baseline_data\"],\n",
    "#     base_folder=features_folder,\n",
    "#     folder_to_internal_map=folder_to_internal\n",
    "# )\n",
    "\n",
    "# or to dump all:\n",
    "dump_all_compositions(\n",
    "    dataset_composition,\n",
    "    base_folder=features_folder,\n",
    "    folder_to_internal_map=folder_to_internal\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece14ac6",
   "metadata": {},
   "source": [
    "## 9) Some final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure each strategy's CSV uses Case_ID as the index\n",
    "strategies = [\n",
    "    d for d in os.listdir(features_folder)\n",
    "    if os.path.isdir(os.path.join(features_folder, d))\n",
    "]\n",
    "\n",
    "for strat in strategies:\n",
    "    csv_path = os.path.join(features_folder, strat, f\"{strat}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        continue\n",
    "\n",
    "    # Files in this pipeline use a semicolon delimiter\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "    df.set_index(\"Case_ID\", inplace=True)\n",
    "    df.to_csv(csv_path, sep=\";\")\n",
    "\n",
    "    print(f\"Set index for {strat} → {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
