{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6f1fee",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Add repository root to PYTHONPATH for local imports\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Columns excluded from uniqueness counts\n",
    "exclude = [\"Label\", \"Case_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd69dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_bins(series, target, k_min=2, k_max=10):\n",
    "    \"\"\"\n",
    "    For a numeric pandas Series and a discrete target array/Series,\n",
    "    compute mutual information for each k in [k_min..k_max]\n",
    "    using equal-frequency bins, and return the best k.\n",
    "    \"\"\"\n",
    "    mi_scores = {}\n",
    "    # drop NaNs in both series & align\n",
    "    valid = series.notna() & pd.Series(target).notna()\n",
    "    x = series[valid]\n",
    "    y = pd.Series(target)[valid].values\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        try:\n",
    "            # qcut may drop bins if too many duplicates → use 'duplicates=\"drop\"'\n",
    "            bins = pd.qcut(x, q=k, duplicates=\"drop\")\n",
    "        except ValueError:\n",
    "            # e.g., not enough unique values to form k bins\n",
    "            continue\n",
    "\n",
    "        # codes: 0..(n_bins-1)\n",
    "        codes = bins.cat.codes\n",
    "        # compute MI\n",
    "        mi = mutual_info_score(y, codes)\n",
    "        mi_scores[k] = mi\n",
    "\n",
    "    if not mi_scores:\n",
    "        return None, {}\n",
    "    # pick k with highest MI\n",
    "    best_k = max(mi_scores, key=mi_scores.get)\n",
    "    return best_k, mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144a400",
   "metadata": {},
   "source": [
    "## Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a942c",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_folders = {\n",
    "    'decl3': 'traffic_decl3_features',\n",
    "    'mr_tr': 'traffic_mr_tr_features',\n",
    "    'payload': 'traffic_payload_Pay36_features'\n",
    "}\n",
    "\n",
    "# Initialize dict for all DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Remove all files in each subfolder except the one named <subfolder>.csv\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(\n",
    "        PROJECT_ROOT,\n",
    "        '3.1_selected_features',\n",
    "        'traffic',\n",
    "        folder_name\n",
    "    )\n",
    "    # Loop over each subfolder (e.g. 'mr', 'dc_data', etc.)\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        # The only file we want to keep\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "\n",
    "        # Delete everything else in this folder\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"Removed unwanted file: {fpath}\")\n",
    "\n",
    "    # Find all CSV files one level down (e.g. bs_data/bs_data.csv, dc_data/dc_data.csv, etc.)\n",
    "    csv_paths = glob.glob(os.path.join(base_dir, '*', '*.csv'))\n",
    "    \n",
    "    for path in csv_paths:\n",
    "        # Extract the subfolder name, e.g. 'bs_data'\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        # Create a prefixed key for the DataFrame dict\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"Loaded {key!r}: shape={df.shape}\")\n",
    "\n",
    "# Example access:\n",
    "# dataframes['decl3_bs_data'], dataframes['mr_tr_dc_data'], dataframes['payload_baseline'], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5513e",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbe060",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}\n",
    "\n",
    "def show_unique_counts(name):\n",
    "    if name not in unique_counts_dict:\n",
    "        print(f\"No dataset named {name!r}. Available: {list(dataframes.keys())}\")\n",
    "        return\n",
    "    counts = unique_counts_dict[name]\n",
    "    total  = total_unique_dict[name]\n",
    "    print(f\"Unique values per column in {name!r}:\")\n",
    "    print(counts)\n",
    "    print(f\"\\nTotal unique values (excluding {exclude}): {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('mr_tr_bs_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6f35a",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afe6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=10)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "# for col, info in all_results['decl3_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177fd29",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_unique = 10\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # skip any dataset without the target\n",
    "    if 'Label' not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # you can also skip Label and Case_ID explicitly\n",
    "        if col in ('Label','Case_ID'):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "        results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: inspect bs_data results\n",
    "print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "for col, info in all_results['decl3_bs_data'].items():\n",
    "    print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "    for col, info in results.items():\n",
    "        k = info['best_k']\n",
    "        # skip features where we couldn’t find a valid k\n",
    "        if k is None:\n",
    "            continue\n",
    "\n",
    "        # apply equal-frequency bins with qcut\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(\n",
    "            df[col],\n",
    "            q=k,\n",
    "            duplicates=\"drop\"\n",
    "        )\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "    # drop all the originals at once (ignore any that might be missing)\n",
    "    df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "    # save back if you want to overwrite\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed1d8e",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 3\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "# for col, info in all_results['decl3_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf092dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b8c8c",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adee912",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('decl3_bs_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d03c7",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c41f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output root (parallel to '3_processed_logs')\n",
    "output_root = os.path.join(PROJECT_ROOT, '3.2_binned_features', 'traffic')\n",
    "\n",
    "# Save each DataFrame to its mirrored folder structure\n",
    "for key, df in dataframes.items():\n",
    "    # Determine which prefix this key uses\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Extract subfolder name (everything after prefix + underscore)\n",
    "    subfolder = key[len(prefix) + 1:]\n",
    "\n",
    "    # Map to the actual folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Build the target directory path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output CSV path (same name as subfolder)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV without the index column\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310ddb5",
   "metadata": {},
   "source": [
    "## BPI15A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2c254",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee335e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_folders = {\n",
    "    'decl2': 'BPI15A_decl2_features',\n",
    "    'mr_tr': 'BPI15A_mr_tr_features',\n",
    "    'payload': 'BPI15A_payload_560925_features'\n",
    "}\n",
    "\n",
    "# Initialize dict for all DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Remove all files in each subfolder except the one named <subfolder>.csv\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(\n",
    "        PROJECT_ROOT,\n",
    "        '3.1_selected_features',\n",
    "        'BPI15A',\n",
    "        folder_name\n",
    "    )\n",
    "    # Loop over each subfolder (e.g. 'mr', 'dc_data', etc.)\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        # The only file we want to keep\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "\n",
    "        # Delete everything else in this folder\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"Removed unwanted file: {fpath}\")\n",
    "\n",
    "    # Find all CSV files one level down (e.g. bs_data/bs_data.csv, dc_data/dc_data.csv, etc.)\n",
    "    csv_paths = glob.glob(os.path.join(base_dir, '*', '*.csv'))\n",
    "    \n",
    "    for path in csv_paths:\n",
    "        # Extract the subfolder name, e.g. 'bs_data'\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        # Create a prefixed key for the DataFrame dict\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"Loaded {key!r}: shape={df.shape}\")\n",
    "\n",
    "# Example access:\n",
    "# dataframes['decl3_bs_data'], dataframes['mr_tr_dc_data'], dataframes['payload_baseline'], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150812aa",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37309b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4af0d9",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce873b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=10)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "# for col, info in all_results['decl2_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e284a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac2de6",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 10\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "# for col, info in all_results['decl2_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b21f",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_unique = 3\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # skip any dataset without the target\n",
    "    if 'Label' not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # you can also skip Label and Case_ID explicitly\n",
    "        if col in ('Label','Case_ID'):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "        results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42186537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: inspect bs_data results\n",
    "print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "for col, info in all_results['decl2_bs_data'].items():\n",
    "    print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "    for col, info in results.items():\n",
    "        k = info['best_k']\n",
    "        # skip features where we couldn’t find a valid k\n",
    "        if k is None:\n",
    "            continue\n",
    "\n",
    "        # apply equal-frequency bins with qcut\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(\n",
    "            df[col],\n",
    "            q=k,\n",
    "            duplicates=\"drop\"\n",
    "        )\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "    # drop all the originals at once (ignore any that might be missing)\n",
    "    df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "    # save back if you want to overwrite\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7dc78",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62818287",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output root (parallel to '3_processed_logs')\n",
    "output_root = os.path.join(PROJECT_ROOT, '3.2_binned_features', 'BPI15A')\n",
    "\n",
    "# Save each DataFrame to its mirrored folder structure\n",
    "for key, df in dataframes.items():\n",
    "    # Determine which prefix this key uses\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Extract subfolder name (everything after prefix + underscore)\n",
    "    subfolder = key[len(prefix) + 1:]\n",
    "\n",
    "    # Map to the actual folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Build the target directory path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output CSV path (same name as subfolder)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV without the index column\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d43d97",
   "metadata": {},
   "source": [
    "## Sepsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd1868",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_folders = {\n",
    "    'decl': 'sepsis_decl_features',\n",
    "    'mr_tr': 'sepsis_mr_tr_features',\n",
    "    'payload': 'sepsis_payload2_features'\n",
    "}\n",
    "\n",
    "# Initialize dict for all DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Remove all files in each subfolder except the one named <subfolder>.csv\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(\n",
    "        PROJECT_ROOT,\n",
    "        '3.1_selected_features',\n",
    "        'sepsis',\n",
    "        folder_name\n",
    "    )\n",
    "    # Loop over each subfolder (e.g. 'mr', 'dc_data', etc.)\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        # The only file we want to keep\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "\n",
    "        # Delete everything else in this folder\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"Removed unwanted file: {fpath}\")\n",
    "                \n",
    "    # Find all CSV files one level down (e.g. bs_data/bs_data.csv, dc_data/dc_data.csv, etc.)\n",
    "    csv_paths = glob.glob(os.path.join(base_dir, '*', '*.csv'))\n",
    "    \n",
    "    for path in csv_paths:\n",
    "        # Extract the subfolder name, e.g. 'bs_data'\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        # Create a prefixed key for the DataFrame dict\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"Loaded {key!r}: shape={df.shape}\")\n",
    "\n",
    "# Example access:\n",
    "# dataframes['decl3_bs_data'], dataframes['mr_tr_dc_data'], dataframes['payload_baseline'], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c7ba",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d52f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba934b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95e734",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec149b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=10)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl_bs_data':\")\n",
    "# for col, info in all_results['decl_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d437ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6c4b2",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 10\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect IMPresseD results\n",
    "# print(\"Optimal bins for 'IMPresseD':\")\n",
    "# for col, info in all_results['decl_IMPresseD'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbe6ac",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_unique = 3\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # skip any dataset without the target\n",
    "    if 'Label' not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # you can also skip Label and Case_ID explicitly\n",
    "        if col in ('Label','Case_ID'):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "        results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl_bs_data':\")\n",
    "# for col, info in all_results['decl_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "    for col, info in results.items():\n",
    "        k = info['best_k']\n",
    "        # skip features where we couldn’t find a valid k\n",
    "        if k is None:\n",
    "            continue\n",
    "\n",
    "        # apply equal-frequency bins with qcut\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(\n",
    "            df[col],\n",
    "            q=k,\n",
    "            duplicates=\"drop\"\n",
    "        )\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "    # drop all the originals at once (ignore any that might be missing)\n",
    "    df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "    # save back if you want to overwrite\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242659d",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d82a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('mr_tr_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa7665",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output root (parallel to '3_processed_logs')\n",
    "output_root = os.path.join(PROJECT_ROOT, '3.2_binned_features', 'sepsis')\n",
    "\n",
    "# Save each DataFrame to its mirrored folder structure\n",
    "for key, df in dataframes.items():\n",
    "    # Determine which prefix this key uses\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Extract subfolder name (everything after prefix + underscore)\n",
    "    subfolder = key[len(prefix) + 1:]\n",
    "\n",
    "    # Map to the actual folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Build the target directory path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define output CSV path (same name as subfolder)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV without the index column\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4931e9",
   "metadata": {},
   "source": [
    "## DHL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423c71a",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c758b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_folders = {\n",
    "#     'dhl': 'dhl_features',\n",
    "# }\n",
    "\n",
    "# # Initialize dict for all DataFrames\n",
    "# dataframes = {}\n",
    "\n",
    "# # Remove all files in each subfolder except the one named <subfolder>.csv\n",
    "# for prefix, folder_name in feature_folders.items():\n",
    "#     base_dir = os.path.join(\n",
    "#         PROJECT_ROOT,\n",
    "#         '3.1_selected_features',\n",
    "#         'DHL',\n",
    "#         folder_name\n",
    "#     )\n",
    "#     # Loop over each subfolder (e.g. 'mr', 'dc_data', etc.)\n",
    "#     for subfolder in os.listdir(base_dir):\n",
    "#         subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "#         if not os.path.isdir(subfolder_dir):\n",
    "#             continue\n",
    "\n",
    "#         # The only file we want to keep\n",
    "#         keep_name = f\"{subfolder}.csv\"\n",
    "\n",
    "#         # Delete everything else in this folder\n",
    "#         for fname in os.listdir(subfolder_dir):\n",
    "#             fpath = os.path.join(subfolder_dir, fname)\n",
    "#             if os.path.isfile(fpath) and fname != keep_name:\n",
    "#                 os.remove(fpath)\n",
    "#                 print(f\"Removed unwanted file: {fpath}\")\n",
    "                \n",
    "#     # Find all CSV files one level down (e.g. bs_data/bs_data.csv, dc_data/dc_data.csv, etc.)\n",
    "#     csv_paths = glob.glob(os.path.join(base_dir, '*', '*.csv'))\n",
    "    \n",
    "#     for path in csv_paths:\n",
    "#         # Extract the subfolder name, e.g. 'bs_data'\n",
    "#         subfolder = os.path.basename(os.path.dirname(path))\n",
    "#         # Create a prefixed key for the DataFrame dict\n",
    "#         key = f\"{prefix}_{subfolder}\"\n",
    "#         # Read the CSV\n",
    "#         df = pd.read_csv(path)\n",
    "#         dataframes[key] = df\n",
    "#         print(f\"Loaded {key!r}: shape={df.shape}\")\n",
    "\n",
    "# # Example access:\n",
    "# # dataframes['decl3_bs_data'], dataframes['mr_tr_dc_data'], dataframes['payload_baseline'], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a9948",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_unique = 10\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # skip any dataset without the target\n",
    "#     if 'Label' not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # you can also skip Label and Case_ID explicitly\n",
    "#         if col in ('Label','Case_ID'):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df['Label'], k_min=2, k_max=3)\n",
    "#         results[col] = {'best_k': best_k, 'mi_by_k': scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: inspect baseline results\n",
    "# print(\"Optimal bins for 'baseline':\")\n",
    "# for col, info in all_results['dhl_baseline'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "#     for col, info in results.items():\n",
    "#         k = info['best_k']\n",
    "#         # skip features where we couldn’t find a valid k\n",
    "#         if k is None:\n",
    "#             continue\n",
    "\n",
    "#         # apply equal-frequency bins with qcut\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(\n",
    "#             df[col],\n",
    "#             q=k,\n",
    "#             duplicates=\"drop\"\n",
    "#         )\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} with {k} bins\")\n",
    "\n",
    "#     # drop all the originals at once (ignore any that might be missing)\n",
    "#     df = df.drop(columns=binned_from, errors='ignore')\n",
    "\n",
    "#     # save back if you want to overwrite\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec12d5",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define output root (parallel to '3_processed_logs')\n",
    "# output_root = os.path.join(PROJECT_ROOT, '3.2_binned_features', 'DHL')\n",
    "\n",
    "# # Save each DataFrame to its mirrored folder structure\n",
    "# for key, df in dataframes.items():\n",
    "#     # Determine which prefix this key uses\n",
    "#     prefix = None\n",
    "#     for p in feature_folders:\n",
    "#         if key.startswith(f\"{p}_\"):\n",
    "#             prefix = p\n",
    "#             break\n",
    "#     if prefix is None:\n",
    "#         raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "#     # Extract subfolder name (everything after prefix + underscore)\n",
    "#     subfolder = key[len(prefix) + 1:]\n",
    "\n",
    "#     # Map to the actual folder name\n",
    "#     folder_name = feature_folders[prefix]\n",
    "\n",
    "#     # Build the target directory path\n",
    "#     output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Define output CSV path (same name as subfolder)\n",
    "#     csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "#     # Write CSV without the index column\n",
    "#     df.to_csv(csv_path, index=False)\n",
    "#     print(f\"Saved {key!r} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498fb1b",
   "metadata": {},
   "source": [
    "### Create overview of binning for report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218a496",
   "metadata": {},
   "source": [
    "#### Summarized overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3910e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config ===\n",
    "BINNED_BASE_DIR   = '3.2_binned_features'\n",
    "ORIGINAL_BASE_DIR = '3.1_selected_features'  # pre-binning counterpart\n",
    "summary_dir  = os.path.join(BINNED_BASE_DIR, 'short_summary')\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "summary_path = os.path.join(summary_dir, 'short_summary.csv')\n",
    "\n",
    "# Name for the new binned-unique-results column (edit each run)\n",
    "UNIQUE_COL_NAME = 'Unique values – strategy 2'  # e.g., 'Unique values – strategy 5'\n",
    "\n",
    "# Encodings to ignore (case-insensitive exact folder names)\n",
    "EXCLUDED_ENCODINGS = {'impressed', 'mr', 'mra', 'tr', 'tra'}\n",
    "\n",
    "DROP_COLS = ['Case_ID', 'Label']\n",
    "KEYS = ['Event Log', 'Labeling']\n",
    "\n",
    "def _normalize_labeling(val: str):\n",
    "    s = str(val).lower()\n",
    "    if 'mr_tr' in s:\n",
    "        return 'sequential'\n",
    "    if 'decl' in s:\n",
    "        return 'declare'\n",
    "    if 'payload' in s:\n",
    "        return 'payload'\n",
    "    return val\n",
    "\n",
    "def _sum_unique_from_folder(folder_path: str) -> int:\n",
    "    \"\"\"Sum unique counts across all CSVs in a folder (dropping Case_ID/Label).\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return 0\n",
    "    total_unique = 0\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(folder_path, fname))\n",
    "        df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        total_unique += df.nunique().sum()\n",
    "    return total_unique\n",
    "\n",
    "# === Load existing summary (if any), normalize, and coerce to (Event Log, Labeling) grain ===\n",
    "if os.path.exists(summary_path):\n",
    "    existing = pd.read_csv(summary_path)\n",
    "    if not existing.empty and 'Labeling' in existing.columns:\n",
    "        existing['Labeling'] = existing['Labeling'].apply(_normalize_labeling)\n",
    "    # Drop legacy \"Total Features\" if present\n",
    "    if 'Total Features' in existing.columns:\n",
    "        existing = existing.drop(columns=['Total Features'])\n",
    "    # If an older file still has 'Encoding', aggregate it away\n",
    "    if 'Encoding' in existing.columns:\n",
    "        numeric_cols = existing.select_dtypes(include='number').columns.tolist()\n",
    "        existing = existing.groupby(KEYS, as_index=False)[numeric_cols].sum()\n",
    "else:\n",
    "    existing = pd.DataFrame()\n",
    "\n",
    "# Build a lookup for already-known Original unique values so we don't recompute them\n",
    "existing_orig_lookup = {}\n",
    "if not existing.empty and 'Original unique values' in existing.columns:\n",
    "    tmp = existing[KEYS + ['Original unique values']].dropna(subset=['Original unique values'])\n",
    "    if not tmp.empty:\n",
    "        # If duplicates exist, aggregate (sum) at labeling-grain to be safe\n",
    "        tmp = tmp.groupby(KEYS, as_index=False)['Original unique values'].sum()\n",
    "        existing_orig_lookup = {(r['Event Log'], r['Labeling']): r['Original unique values'] for _, r in tmp.iterrows()}\n",
    "\n",
    "# === Build fresh binned unique counts per encoding (skipping excluded), then aggregate per labeling ===\n",
    "records = []\n",
    "\n",
    "for event_log in sorted(os.listdir(BINNED_BASE_DIR)):\n",
    "    el_path = os.path.join(BINNED_BASE_DIR, event_log)\n",
    "    if not os.path.isdir(el_path) or event_log == 'short_summary':\n",
    "        continue\n",
    "\n",
    "    label_strats = sorted(d for d in os.listdir(el_path) if os.path.isdir(os.path.join(el_path, d)))\n",
    "    if not label_strats:\n",
    "        continue\n",
    "\n",
    "    for labeling in label_strats:\n",
    "        strat_path_binned = os.path.join(el_path, labeling)\n",
    "\n",
    "        for encoding in sorted(os.listdir(strat_path_binned)):\n",
    "            if encoding.lower() in EXCLUDED_ENCODINGS:\n",
    "                continue\n",
    "            enc_path_binned = os.path.join(strat_path_binned, encoding)\n",
    "            if not os.path.isdir(enc_path_binned):\n",
    "                continue\n",
    "\n",
    "            # Binned unique values for this encoding\n",
    "            binned_unique = _sum_unique_from_folder(enc_path_binned)\n",
    "            # Keep per-encoding; we'll aggregate to (Event Log, Labeling) below\n",
    "            records.append({\n",
    "                'Event Log': event_log,\n",
    "                'Labeling': labeling,\n",
    "                'Encoding': encoding,\n",
    "                UNIQUE_COL_NAME: binned_unique,\n",
    "            })\n",
    "\n",
    "# Per-encoding -> DataFrame\n",
    "if records:\n",
    "    df_enc = pd.DataFrame(records).sort_values(['Event Log', 'Labeling', 'Encoding']).reset_index(drop=True)\n",
    "else:\n",
    "    df_enc = pd.DataFrame(columns=['Event Log', 'Labeling', 'Encoding', UNIQUE_COL_NAME])\n",
    "\n",
    "# Normalize labeling\n",
    "df_enc['Labeling'] = df_enc['Labeling'].apply(_normalize_labeling)\n",
    "\n",
    "# Aggregate per (Event Log, Labeling) for the new strategy column\n",
    "if not df_enc.empty:\n",
    "    new_agg = df_enc.groupby(KEYS, as_index=False)[[UNIQUE_COL_NAME]].sum()\n",
    "else:\n",
    "    new_agg = pd.DataFrame(columns=KEYS + [UNIQUE_COL_NAME])\n",
    "\n",
    "# Compute Original unique values ONLY for keys that don't already have it\n",
    "orig_values = []\n",
    "for _, row in new_agg.iterrows():\n",
    "    key = (row['Event Log'], row['Labeling'])\n",
    "    if key in existing_orig_lookup:\n",
    "        orig_values.append(existing_orig_lookup[key])\n",
    "    else:\n",
    "        # Compute once per (Event Log, Labeling) by summing across all encodings (skipping excluded)\n",
    "        strat_path_orig = os.path.join(ORIGINAL_BASE_DIR, row['Event Log'], row['Labeling'])\n",
    "        total_orig_unique = 0\n",
    "        if os.path.isdir(strat_path_orig):\n",
    "            for enc in sorted(os.listdir(strat_path_orig)):\n",
    "                if enc.lower() in EXCLUDED_ENCODINGS:\n",
    "                    continue\n",
    "                enc_path_orig = os.path.join(strat_path_orig, enc)\n",
    "                if os.path.isdir(enc_path_orig):\n",
    "                    total_orig_unique += _sum_unique_from_folder(enc_path_orig)\n",
    "        orig_values.append(total_orig_unique)\n",
    "\n",
    "new_agg['Original unique values'] = orig_values\n",
    "\n",
    "# === Merge/update with existing (append/overwrite strategy column, keep existing Original unique values) ===\n",
    "if existing.empty:\n",
    "    updated = new_agg.copy()\n",
    "else:\n",
    "    ex = existing.set_index(KEYS)\n",
    "    nw = new_agg.set_index(KEYS)\n",
    "\n",
    "    # Ensure required columns exist in existing\n",
    "    if 'Original unique values' not in ex.columns:\n",
    "        ex['Original unique values'] = pd.NA\n",
    "    if UNIQUE_COL_NAME not in ex.columns:\n",
    "        ex[UNIQUE_COL_NAME] = pd.NA\n",
    "\n",
    "    # Update the strategy column for matching rows\n",
    "    common_idx = nw.index.intersection(ex.index)\n",
    "    ex.loc[common_idx, UNIQUE_COL_NAME] = nw.loc[common_idx, UNIQUE_COL_NAME]\n",
    "\n",
    "    # Only fill Original unique values where missing in existing\n",
    "    need_orig_mask = ex.loc[common_idx, 'Original unique values'].isna()\n",
    "    if need_orig_mask.any():\n",
    "        idx_to_fill = need_orig_mask[need_orig_mask].index\n",
    "        ex.loc[idx_to_fill, 'Original unique values'] = nw.loc[idx_to_fill, 'Original unique values']\n",
    "\n",
    "    # Append any new rows\n",
    "    missing_idx = nw.index.difference(ex.index)\n",
    "    ex = pd.concat([ex, nw.loc[missing_idx]], axis=0)\n",
    "\n",
    "    updated = ex.reset_index()\n",
    "\n",
    "# Final ordering: Event Log, Labeling, Original unique values, then the rest (incl. the new strategy column)\n",
    "front = ['Event Log', 'Labeling', 'Original unique values']\n",
    "rest = [c for c in updated.columns if c not in front]\n",
    "updated = updated[front + rest].sort_values(KEYS).reset_index(drop=True)\n",
    "\n",
    "# Ensure legacy columns are gone\n",
    "if 'Total Features' in updated.columns:\n",
    "    updated = updated.drop(columns=['Total Features'])\n",
    "if 'Encoding' in updated.columns:\n",
    "    updated = updated.drop(columns=['Encoding'])\n",
    "\n",
    "# Save\n",
    "updated.to_csv(summary_path, index=False)\n",
    "print(f\"✅ Summary (per Labeling) updated at: {summary_path}\")\n",
    "print(f\"🆕 Binned column written: {UNIQUE_COL_NAME}\")\n",
    "print(\"ℹ️ 'Original unique values' preserved/reused when already present.\")\n",
    "updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2d65f",
   "metadata": {},
   "source": [
    "#### Long overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Config/paths ===\n",
    "BINNED_BASE_DIR   = '3.2_binned_features'\n",
    "ORIGINAL_BASE_DIR = '3.1_selected_features'\n",
    "long_dir  = os.path.join(BINNED_BASE_DIR, 'long_overview')\n",
    "os.makedirs(long_dir, exist_ok=True)\n",
    "long_path = os.path.join(long_dir, 'long_overview.csv')\n",
    "\n",
    "# Encodings to ignore (case-insensitive exact folder names)\n",
    "EXCLUDED_ENCODINGS = {'impressed', 'mr', 'mra', 'tr', 'tra'}\n",
    "\n",
    "DROP_COLS = ['Case_ID', 'Label']\n",
    "KEYS = ['Event Log', 'Labeling', 'Encoding']\n",
    "\n",
    "# === Reuse UNIQUE_COL_NAME from previous cell; try to infer if missing ===\n",
    "try:\n",
    "    UNIQUE_COL_NAME\n",
    "except NameError:\n",
    "    ss_path = os.path.join(BINNED_BASE_DIR, 'short_summary', 'short_summary.csv')\n",
    "    if os.path.exists(ss_path):\n",
    "        _ss = pd.read_csv(ss_path)\n",
    "        cand = [c for c in _ss.columns if isinstance(c, str) and c.lower().startswith('unique values')]\n",
    "        UNIQUE_COL_NAME = cand[-1] if cand else 'Unique values – strategy'\n",
    "    else:\n",
    "        UNIQUE_COL_NAME = 'Unique values – strategy'\n",
    "\n",
    "def _normalize_labeling(val: str):\n",
    "    s = str(val).lower()\n",
    "    if 'mr_tr' in s:\n",
    "        return 'sequential'\n",
    "    if 'decl' in s:\n",
    "        return 'declare'\n",
    "    if 'payload' in s:\n",
    "        return 'payload'\n",
    "    return val\n",
    "\n",
    "def _sum_unique_from_folder(folder_path: str) -> int:\n",
    "    \"\"\"Sum unique counts across all CSVs in a folder (dropping Case_ID/Label).\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return 0\n",
    "    total_unique = 0\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith('.csv'):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(folder_path, fname))\n",
    "        df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        total_unique += df.nunique().sum()\n",
    "    return total_unique\n",
    "\n",
    "# === Load existing long_overview (if any) ===\n",
    "if os.path.exists(long_path):\n",
    "    existing = pd.read_csv(long_path)\n",
    "    if not existing.empty and 'Labeling' in existing.columns:\n",
    "        existing['Labeling'] = existing['Labeling'].apply(_normalize_labeling)\n",
    "    # Drop legacy columns if present\n",
    "    for col in ('Total Features',):\n",
    "        if col in existing.columns:\n",
    "            existing = existing.drop(columns=[col])\n",
    "else:\n",
    "    existing = pd.DataFrame()\n",
    "\n",
    "# Build lookup to avoid recomputing \"Original unique values\"\n",
    "existing_orig_lookup = {}\n",
    "if not existing.empty and 'Original unique values' in existing.columns:\n",
    "    tmp = existing[KEYS + ['Original unique values']].dropna(subset=['Original unique values'])\n",
    "    if not tmp.empty:\n",
    "        # If duplicates exist, take the last non-null (or sum). We'll take last to preserve prior values.\n",
    "        tmp = tmp.drop_duplicates(subset=KEYS, keep='last')\n",
    "        existing_orig_lookup = {\n",
    "            (r['Event Log'], r['Labeling'], r['Encoding']): r['Original unique values']\n",
    "            for _, r in tmp.iterrows()\n",
    "        }\n",
    "\n",
    "# === Build fresh per-encoding records ===\n",
    "records = []\n",
    "for event_log in sorted(os.listdir(BINNED_BASE_DIR)):\n",
    "    if event_log in ('short_summary', 'long_overview'):\n",
    "        continue\n",
    "    el_path = os.path.join(BINNED_BASE_DIR, event_log)\n",
    "    if not os.path.isdir(el_path):\n",
    "        continue\n",
    "\n",
    "    label_strats = sorted(d for d in os.listdir(el_path) if os.path.isdir(os.path.join(el_path, d)))\n",
    "    if not label_strats:\n",
    "        continue\n",
    "\n",
    "    for labeling in label_strats:\n",
    "        # use non-normalized label for pathing; normalized for the row value\n",
    "        labeling_path_binned = os.path.join(el_path, labeling)\n",
    "        labeling_path_orig   = os.path.join(ORIGINAL_BASE_DIR, event_log, labeling)\n",
    "        labeling_norm = _normalize_labeling(labeling)\n",
    "\n",
    "        for encoding in sorted(os.listdir(labeling_path_binned)):\n",
    "            if encoding.lower() in EXCLUDED_ENCODINGS:\n",
    "                continue\n",
    "            enc_path_binned = os.path.join(labeling_path_binned, encoding)\n",
    "            if not os.path.isdir(enc_path_binned):\n",
    "                continue\n",
    "\n",
    "            # binned unique values for this encoding\n",
    "            binned_unique = _sum_unique_from_folder(enc_path_binned)\n",
    "\n",
    "            # original unique values (reuse existing if available)\n",
    "            key = (event_log, labeling_norm, encoding)\n",
    "            if key in existing_orig_lookup:\n",
    "                orig_unique = existing_orig_lookup[key]\n",
    "            else:\n",
    "                enc_path_orig = os.path.join(labeling_path_orig, encoding)\n",
    "                orig_unique = _sum_unique_from_folder(enc_path_orig)\n",
    "\n",
    "            records.append({\n",
    "                'Event Log': event_log,\n",
    "                'Labeling': labeling_norm,\n",
    "                'Encoding': encoding,\n",
    "                'Original unique values': orig_unique,\n",
    "                UNIQUE_COL_NAME: binned_unique,\n",
    "            })\n",
    "\n",
    "# New dataframe\n",
    "if records:\n",
    "    new_df = pd.DataFrame(records).sort_values(KEYS).reset_index(drop=True)\n",
    "else:\n",
    "    new_df = pd.DataFrame(columns=KEYS + ['Original unique values', UNIQUE_COL_NAME])\n",
    "\n",
    "# === Merge/update with existing ===\n",
    "if existing.empty:\n",
    "    updated = new_df.copy()\n",
    "else:\n",
    "    ex = existing.set_index(KEYS)\n",
    "    nw = new_df.set_index(KEYS)\n",
    "\n",
    "    # Ensure columns exist\n",
    "    if 'Original unique values' not in ex.columns:\n",
    "        ex['Original unique values'] = pd.NA\n",
    "    if UNIQUE_COL_NAME not in ex.columns:\n",
    "        ex[UNIQUE_COL_NAME] = pd.NA\n",
    "\n",
    "    common_idx = nw.index.intersection(ex.index)\n",
    "\n",
    "    # Update strategy column\n",
    "    ex.loc[common_idx, UNIQUE_COL_NAME] = nw.loc[common_idx, UNIQUE_COL_NAME]\n",
    "\n",
    "    # Only fill original unique where missing\n",
    "    need_orig_mask = ex.loc[common_idx, 'Original unique values'].isna()\n",
    "    if need_orig_mask.any():\n",
    "        idx_to_fill = need_orig_mask[need_orig_mask].index\n",
    "        ex.loc[idx_to_fill, 'Original unique values'] = nw.loc[idx_to_fill, 'Original unique values']\n",
    "\n",
    "    # Append new rows\n",
    "    missing_idx = nw.index.difference(ex.index)\n",
    "    ex = pd.concat([ex, nw.loc[missing_idx]], axis=0)\n",
    "\n",
    "    updated = ex.reset_index()\n",
    "\n",
    "# Order columns: Event Log, Labeling, Encoding, Original unique values, then the rest\n",
    "front = ['Event Log', 'Labeling', 'Encoding', 'Original unique values']\n",
    "rest  = [c for c in updated.columns if c not in front]\n",
    "updated = updated[front + rest].sort_values(KEYS).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "updated.to_csv(long_path, index=False)\n",
    "print(f\"✅ Long overview (per Encoding) updated at: {long_path}\")\n",
    "print(f\"🆕 Binned column written: {UNIQUE_COL_NAME}\")\n",
    "print(\"ℹ️ 'Original unique values' preserved/reused when already present.\")\n",
    "updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdef24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- helper for TeX-safe cell values ---\n",
    "def fmt_rule(x):\n",
    "    return r'\\detokenize{' + str(x) + '}'\n",
    "\n",
    "# export targets\n",
    "fp_short_tex = os.path.join('3.2_binned_features', 'short_summary', 'short_summary.tex')\n",
    "fp_long_tex  = os.path.join('3.2_binned_features', 'long_overview',  'long_summary.tex')\n",
    "\n",
    "# source CSVs\n",
    "fp_short_csv = os.path.join('3.2_binned_features', 'short_summary', 'short_summary.csv')\n",
    "fp_long_csv  = os.path.join('3.2_binned_features', 'long_overview',  'long_overview.csv')\n",
    "\n",
    "def _colalign(df: pd.DataFrame) -> str:\n",
    "    return ''.join('r' if pd.api.types.is_numeric_dtype(df[c]) else 'l' for c in df.columns)\n",
    "\n",
    "def _to_integers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    num_cols = out.select_dtypes(include='number').columns\n",
    "    for c in num_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors='coerce').round(0).astype('Int64')\n",
    "    return out\n",
    "\n",
    "def _reorder_unique_value_strategy_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find columns named like 'Unique values – strategy N' (N=1,2,3), sort by N,\n",
    "    and place them back in that order where the first one originally appeared.\n",
    "    Works with hyphen '-' or en dash '–', case-insensitive.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    # capture (n, colname) for matching columns\n",
    "    pat = re.compile(r'^\\s*Unique\\s+values\\s*[–-]\\s*strategy\\s*(\\d+)\\s*$', re.IGNORECASE)\n",
    "    matches = [(int(pat.match(str(c)).group(1)), c) for c in cols if pat.match(str(c))]\n",
    "    if not matches:\n",
    "        return df  # nothing to do\n",
    "\n",
    "    # desired order by numeric suffix\n",
    "    matches_sorted = sorted(matches, key=lambda t: t[0])\n",
    "    uv_names_sorted = [c for _, c in matches_sorted]\n",
    "\n",
    "    # where to reinsert (position of the first of these columns)\n",
    "    first_pos = min(cols.index(c) for _, c in matches)\n",
    "    # remove them from the current order\n",
    "    base = [c for c in cols if c not in [name for _, name in matches]]\n",
    "    # reinsert in sorted order\n",
    "    base[first_pos:first_pos] = uv_names_sorted\n",
    "    return df[base]\n",
    "\n",
    "def _df_to_longtable_tex(df: pd.DataFrame, formatters: dict | None) -> str:\n",
    "    tex = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        longtable=True,\n",
    "        multicolumn=False,\n",
    "        column_format=_colalign(df),\n",
    "        na_rep='',\n",
    "        formatters=formatters\n",
    "    )\n",
    "    return tex\n",
    "\n",
    "def _save_tex(df: pd.DataFrame, out_path: str, formatters: dict | None):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(_df_to_longtable_tex(df, formatters))\n",
    "\n",
    "# ---- SHORT SUMMARY ----\n",
    "if os.path.exists(fp_short_csv):\n",
    "    short_df = pd.read_csv(fp_short_csv)\n",
    "    short_df = _reorder_unique_value_strategy_cols(short_df)\n",
    "    short_df = _to_integers(short_df)\n",
    "    all_fmt = {col: fmt_rule for col in short_df.columns}\n",
    "    _save_tex(short_df, fp_short_tex, all_fmt)\n",
    "    print(f\"✅ Wrote short_summary TeX → {fp_short_tex}\")\n",
    "else:\n",
    "    print(f\"⚠️ Not found: {fp_short_csv}\")\n",
    "\n",
    "# ---- LONG OVERVIEW ----\n",
    "if os.path.exists(fp_long_csv):\n",
    "    long_df = pd.read_csv(fp_long_csv)\n",
    "    long_df = _reorder_unique_value_strategy_cols(long_df)\n",
    "    long_df = _to_integers(long_df)\n",
    "    enc_fmt = {'Encoding': fmt_rule} if 'Encoding' in long_df.columns else None\n",
    "    _save_tex(long_df, fp_long_tex, enc_fmt)\n",
    "    print(f\"✅ Wrote long_overview TeX → {fp_long_tex}\")\n",
    "else:\n",
    "    print(f\"⚠️ Not found: {fp_long_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752ba8e",
   "metadata": {},
   "source": [
    "### Checking unique values before binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate unique-value counts PER ENCODING, excluding some encodings,\n",
    "# and skipping 'Case_ID' and 'Label' columns from the counts.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path(\"3.1_selected_features\")\n",
    "exclude_encodings = {\"impressed\", \"mr\", \"mra\", \"tr\", \"tra\", \"dhl\"}  # case-insensitive compare\n",
    "exclude_cols = {\"case_id\", \"label\"}  # columns to skip (case-insensitive)\n",
    "\n",
    "rows = []\n",
    "missing = []\n",
    "\n",
    "for log_dir in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "    log = log_dir.name\n",
    "    for lab_dir in sorted([p for p in log_dir.iterdir() if p.is_dir()]):\n",
    "        labeling = lab_dir.name\n",
    "        for enc_dir in sorted([p for p in lab_dir.iterdir() if p.is_dir()]):\n",
    "            encoding = enc_dir.name\n",
    "            if encoding.lower() in exclude_encodings:\n",
    "                continue\n",
    "\n",
    "            csv_files = sorted(enc_dir.glob(\"*.csv\"))\n",
    "            if not csv_files:\n",
    "                missing.append((log, labeling, encoding, \"⚠️ no CSV found\"))\n",
    "                continue\n",
    "\n",
    "            for csv_fp in csv_files:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_fp, low_memory=False)\n",
    "                except Exception as e:\n",
    "                    missing.append((log, labeling, encoding, f\"❌ failed to read: {csv_fp.name} ({e})\"))\n",
    "                    continue\n",
    "\n",
    "                per_col_counts = []\n",
    "                for col in df.columns:\n",
    "                    if col.strip().lower() in exclude_cols:\n",
    "                        continue\n",
    "                    try:\n",
    "                        per_col_counts.append(df[col].nunique(dropna=False))\n",
    "                    except Exception:\n",
    "                        per_col_counts.append(df[col].astype(str).nunique(dropna=False))\n",
    "\n",
    "                total_unique = int(sum(int(x) for x in per_col_counts)) if per_col_counts else 0\n",
    "                rows.append({\n",
    "                    \"Log\": log,\n",
    "                    \"Labeling\": labeling,\n",
    "                    \"Encoding\": encoding,\n",
    "                    \"Columns counted\": int(len(per_col_counts)),\n",
    "                    \"Unique values (total across columns)\": total_unique,\n",
    "                    \"Source file\": csv_fp.name,\n",
    "                })\n",
    "\n",
    "# Build aggregated result\n",
    "agg_per_encoding = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"Log\", \"Labeling\", \"Encoding\", \"Columns counted\", \"Unique values (total across columns)\", \"Source file\"]\n",
    ")\n",
    "\n",
    "# Collapse to single row per Log/Labeling/Encoding if multiple CSVs exist\n",
    "if not agg_per_encoding.empty:\n",
    "    agg_per_encoding = (\n",
    "        agg_per_encoding\n",
    "        .groupby([\"Log\", \"Labeling\", \"Encoding\"], as_index=False)\n",
    "        .agg({\n",
    "            \"Columns counted\": \"sum\",\n",
    "            \"Unique values (total across columns)\": \"sum\",\n",
    "        })\n",
    "        .sort_values([\"Log\", \"Labeling\", \"Encoding\"], kind=\"stable\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"✅ Encodings summarized: {len(agg_per_encoding):,}\")\n",
    "if missing:\n",
    "    print(\"Notes/warnings:\")\n",
    "    for item in missing:\n",
    "        print(\"  -\", \" / \".join(map(str, item)))\n",
    "\n",
    "# Optional: save to disk\n",
    "# agg_per_encoding.to_csv(\"3.1_selected_features_unique_counts_per_encoding.csv\", index=False)\n",
    "\n",
    "agg_per_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec29cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Further aggregate by LABELING (across encodings within each log/labeling)\n",
    "\n",
    "# # If you ran the previous cell, `agg_per_encoding` already exists.\n",
    "# # Otherwise, you can run that first or wrap it here.\n",
    "\n",
    "# if 'agg_per_encoding' not in globals() or agg_per_encoding.empty:\n",
    "#     raise RuntimeError(\"agg_per_encoding not found or empty. Run the previous cell first.\")\n",
    "\n",
    "# # How many encodings contribute to each (Log, Labeling) group?\n",
    "# # (each row in agg_per_encoding is a single encoding after the prior collapse)\n",
    "# agg_per_labeling = (\n",
    "#     agg_per_encoding\n",
    "#     .groupby([\"Log\", \"Labeling\"], as_index=False)\n",
    "#     .agg(\n",
    "#         Encodings_count=(\"Encoding\", \"nunique\"),\n",
    "#         Columns_counted_total=(\"Columns counted\", \"sum\"),\n",
    "#         Unique_values_total=(\"Unique values (total across columns)\", \"sum\"),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Optional derived metrics\n",
    "# agg_per_labeling[\"Avg columns per encoding\"] = (\n",
    "#     agg_per_labeling[\"Columns_counted_total\"] / agg_per_labeling[\"Encodings_count\"]\n",
    "# ).round(2)\n",
    "\n",
    "# agg_per_labeling[\"Avg unique-values per encoding\"] = (\n",
    "#     agg_per_labeling[\"Unique_values_total\"] / agg_per_labeling[\"Encodings_count\"]\n",
    "# ).round(2)\n",
    "\n",
    "# # Sort for readability\n",
    "# agg_per_labeling = agg_per_labeling.sort_values([\"Log\", \"Labeling\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "# print(f\"✅ Labeling groups summarized: {len(agg_per_labeling):,}\")\n",
    "\n",
    "# # Optional: save\n",
    "# # agg_per_labeling.to_csv(\"3.1_selected_features_unique_counts_per_labeling.csv\", index=False)\n",
    "\n",
    "# agg_per_labeling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
