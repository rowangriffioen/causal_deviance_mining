{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6f1fee",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Add repository root to PYTHONPATH for local imports\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Columns excluded from uniqueness counts\n",
    "exclude = [\"Label\", \"Case_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd69dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_bins(series, target, k_min=2, k_max=10):\n",
    "    \"\"\"\n",
    "    For a numeric Series and a discrete target, evaluate mutual information (MI)\n",
    "    across equal-frequency binnings with k in [k_min, k_max]. Return (best_k, mi_scores).\n",
    "    \"\"\"\n",
    "    mi_scores = {}\n",
    "\n",
    "    # Align non-missing values in feature and target\n",
    "    valid = series.notna() & pd.Series(target).notna()\n",
    "    x = series[valid]\n",
    "    y = pd.Series(target)[valid].values\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        try:\n",
    "            # Equal-frequency bins; allow fewer bins if duplicates collapse categories\n",
    "            bins = pd.qcut(x, q=k, duplicates=\"drop\")\n",
    "        except ValueError:\n",
    "            # Not enough unique values for k bins\n",
    "            continue\n",
    "\n",
    "        # Discretized codes 0..(n_bins-1)\n",
    "        codes = bins.cat.codes\n",
    "\n",
    "        # Mutual information between target and discretized feature\n",
    "        mi = mutual_info_score(y, codes)\n",
    "        mi_scores[k] = mi\n",
    "\n",
    "    if not mi_scores:\n",
    "        return None, {}\n",
    "    best_k = max(mi_scores, key=mi_scores.get)\n",
    "    return best_k, mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144a400",
   "metadata": {},
   "source": [
    "## Traffic (choose a single strategy, comment the rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a942c",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect selected feature CSVs and load them into a dict of DataFrames\n",
    "\n",
    "feature_folders = {\n",
    "    \"decl3\":   \"traffic_decl3_features\",\n",
    "    \"mr_tr\":   \"traffic_mr_tr_features\",\n",
    "    \"payload\": \"traffic_payload_Pay36_features\",\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(PROJECT_ROOT, \"3.1_selected_features\", \"traffic\", folder_name)\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"[WARN] Missing folder: {base_dir}\")\n",
    "        continue\n",
    "\n",
    "    # Per encoding subfolder (e.g., 'mr', 'dc_data', ...): keep only <subfolder>.csv\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"[CLEAN] Removed: {fpath}\")\n",
    "\n",
    "    # Load remaining CSVs (one per subfolder)\n",
    "    csv_paths = sorted(glob.glob(os.path.join(base_dir, \"*\", \"*.csv\")))\n",
    "    for path in csv_paths:\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        df = pd.read_csv(path)  # uses file's delimiter; upstream exports default to comma\n",
    "        dataframes[key] = df\n",
    "        print(f\"[LOAD] {key}: shape={df.shape}\")\n",
    "\n",
    "# Example keys:\n",
    "#   'decl3_bs_data', 'mr_tr_dc_data', 'payload_baseline', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5513e",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebbe060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-column and total uniqueness (excluding ID/label columns)\n",
    "\n",
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}\n",
    "\n",
    "def show_unique_counts(name: str) -> None:\n",
    "    \"\"\"Print per-column and total unique counts for a loaded dataset.\"\"\"\n",
    "    if name not in unique_counts_dict:\n",
    "        print(f\"No dataset named {name!r}. Available: {list(dataframes.keys())}\")\n",
    "        return\n",
    "    counts = unique_counts_dict[name]\n",
    "    total  = total_unique_dict[name]\n",
    "    print(f\"Unique values per column in {name!r}:\")\n",
    "    print(counts)\n",
    "    print(f\"\\nTotal unique values (excluding {exclude}): {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6f35a",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afe6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=10)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c372a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "# for col, info in all_results['decl3_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc76edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid bin count found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Remove the original continuous columns (keep IDs/labels)\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Update in-memory store\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7177fd29",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "threshold_unique = 10\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Skip datasets without a target\n",
    "    if \"Label\" not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # Skip ID/target and non-numeric or low-cardinality features\n",
    "        if col in (\"Label\", \"Case_ID\"):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "        results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect bs_data results\n",
    "print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "for col, info in all_results['decl3_bs_data'].items():\n",
    "    print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "\n",
    "    for col, info in results.items():\n",
    "        k = info[\"best_k\"]\n",
    "        if k is None:\n",
    "            continue  # no valid bin count found\n",
    "\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "    # Remove the original continuous columns (keep IDs/labels)\n",
    "    df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "    # Update in-memory store\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffed1d8e",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 3\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl3_bs_data':\")\n",
    "# for col, info in all_results['decl3_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf092dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid bin count found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Remove the original continuous columns (keep IDs/labels)\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Update in-memory store\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b8c8c",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adee912",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('decl3_bs_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d03c7",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c41f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binned feature sets to a mirrored folder structure\n",
    "\n",
    "output_root = os.path.join(PROJECT_ROOT, \"3.2_binned_features\", \"traffic\")\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Identify prefix (decl3 / mr_tr / payload)\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Subfolder name is everything after \"<prefix>_\"\n",
    "    subfolder = key[len(prefix) + 1 :]\n",
    "\n",
    "    # Map prefix to source folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Target directory and file path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV (no index)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310ddb5",
   "metadata": {},
   "source": [
    "## BPI15A (choose a single strategy, comment the rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2c254",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee335e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected feature CSVs for BPI15A into a dictionary of DataFrames\n",
    "\n",
    "feature_folders = {\n",
    "    \"decl2\":   \"BPI15A_decl2_features\",\n",
    "    \"mr_tr\":   \"BPI15A_mr_tr_features\",\n",
    "    \"payload\": \"BPI15A_payload_560925_features\",\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(PROJECT_ROOT, \"3.1_selected_features\", \"BPI15A\", folder_name)\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"[WARN] Missing folder: {base_dir}\")\n",
    "        continue\n",
    "\n",
    "    # In each encoding subfolder (e.g., 'mr', 'dc_data', ...), keep only <subfolder>.csv\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"[CLEAN] Removed: {fpath}\")\n",
    "\n",
    "    # Load remaining CSVs (one per subfolder)\n",
    "    csv_paths = sorted(glob.glob(os.path.join(base_dir, \"*\", \"*.csv\")))\n",
    "    for path in csv_paths:\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        df = pd.read_csv(path)  # upstream exports typically use comma delimiter\n",
    "        dataframes[key] = df\n",
    "        print(f\"[LOAD] {key}: shape={df.shape}\")\n",
    "\n",
    "# Example keys:\n",
    "#   'decl2_bs_data', 'mr_tr_dc_data', 'payload_baseline', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150812aa",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37309b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4af0d9",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce873b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=10)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "# for col, info in all_results['decl2_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e284a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid bin count found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Remove the original continuous columns (keep IDs/labels)\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Update in-memory store\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac2de6",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "threshold_unique = 10\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Skip datasets without a target\n",
    "    if \"Label\" not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # Skip ID/target and non-numeric or low-cardinality features\n",
    "        if col in (\"Label\", \"Case_ID\"):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "        results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect bs_data results\n",
    "print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "for col, info in all_results['decl2_bs_data'].items():\n",
    "    print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae3139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "\n",
    "    for col, info in results.items():\n",
    "        k = info[\"best_k\"]\n",
    "        if k is None:\n",
    "            continue  # no valid bin count found\n",
    "\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "    # Remove the original continuous columns (keep IDs/labels)\n",
    "    df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "    # Update in-memory store\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46b21f",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 3\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42186537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl2_bs_data':\")\n",
    "# for col, info in all_results['decl2_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid bin count found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Remove the original continuous columns (keep IDs/labels)\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Update in-memory store\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7dc78",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8acbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62818287",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binned feature sets for BPI15A using a mirrored folder structure\n",
    "\n",
    "output_root = os.path.join(PROJECT_ROOT, \"3.2_binned_features\", \"BPI15A\")\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Identify prefix (decl2 / mr_tr / payload)\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Subfolder name is everything after \"<prefix>_\"\n",
    "    subfolder = key[len(prefix) + 1 :]\n",
    "\n",
    "    # Map prefix to source folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Target directory and file path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV (no index)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d43d97",
   "metadata": {},
   "source": [
    "## Sepsis (choose a single strategy, comment the rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd1868",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected feature CSVs for Sepsis into a dictionary of DataFrames\n",
    "\n",
    "feature_folders = {\n",
    "    \"decl\":    \"sepsis_decl_features\",\n",
    "    \"mr_tr\":   \"sepsis_mr_tr_features\",\n",
    "    \"payload\": \"sepsis_payload2_features\",\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for prefix, folder_name in feature_folders.items():\n",
    "    base_dir = os.path.join(PROJECT_ROOT, \"3.1_selected_features\", \"sepsis\", folder_name)\n",
    "    if not os.path.isdir(base_dir):\n",
    "        print(f\"[WARN] Missing folder: {base_dir}\")\n",
    "        continue\n",
    "\n",
    "    # In each encoding subfolder (e.g., 'mr', 'dc_data', ...), keep only <subfolder>.csv\n",
    "    for subfolder in os.listdir(base_dir):\n",
    "        subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "        if not os.path.isdir(subfolder_dir):\n",
    "            continue\n",
    "\n",
    "        keep_name = f\"{subfolder}.csv\"\n",
    "        for fname in os.listdir(subfolder_dir):\n",
    "            fpath = os.path.join(subfolder_dir, fname)\n",
    "            if os.path.isfile(fpath) and fname != keep_name:\n",
    "                os.remove(fpath)\n",
    "                print(f\"[CLEAN] Removed: {fpath}\")\n",
    "\n",
    "    # Load remaining CSVs (one per subfolder)\n",
    "    csv_paths = sorted(glob.glob(os.path.join(base_dir, \"*\", \"*.csv\")))\n",
    "    for path in csv_paths:\n",
    "        subfolder = os.path.basename(os.path.dirname(path))\n",
    "        key = f\"{prefix}_{subfolder}\"\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes[key] = df\n",
    "        print(f\"[LOAD] {key}: shape={df.shape}\")\n",
    "\n",
    "# Example keys:\n",
    "#   'decl_bs_data', 'mr_tr_dc_data', 'payload_baseline', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c7ba",
   "metadata": {},
   "source": [
    "### Show unique values per df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d52f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba934b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('payload_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95e734",
   "metadata": {},
   "source": [
    "### Strategy 1: Look for the optimal number of bins for variables with +50 unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec149b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 50\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=10)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl_bs_data':\")\n",
    "# for col, info in all_results['decl_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d437ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid bin count found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Remove the original continuous columns (keep IDs/labels)\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Update in-memory store\n",
    "#     dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6c4b2",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "threshold_unique = 10\n",
    "all_results = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Skip datasets without a target\n",
    "    if \"Label\" not in df.columns:\n",
    "        print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "        continue\n",
    "\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        # Skip ID/target and non-numeric or low-cardinality features\n",
    "        if col in (\"Label\", \"Case_ID\"):\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            continue\n",
    "        if df[col].nunique() <= threshold_unique:\n",
    "            continue\n",
    "\n",
    "        best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "        results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "    all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect IMPresseD results\n",
    "print(\"Optimal bins for 'IMPresseD':\")\n",
    "for col, info in all_results['decl_IMPresseD'].items():\n",
    "    print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "\n",
    "    for col, info in results.items():\n",
    "        k = info[\"best_k\"]\n",
    "        if k is None:\n",
    "            continue  # no valid bin count found\n",
    "\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "    # Remove the original continuous columns (keep IDs/labels)\n",
    "    df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "    # Update in-memory store\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbe6ac",
   "metadata": {},
   "source": [
    "### Strategy 3: Hard coding 2/3 bins and a threshold of 3+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c6aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate MI-guided binning for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 3\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     # Skip datasets without a target\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target and non-numeric or low-cardinality features\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect bs_data results\n",
    "# print(\"Optimal bins for 'decl_bs_data':\")\n",
    "# for col, info in all_results['decl_bs_data'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MI-guided equal-frequency binning and replace original columns\n",
    "\n",
    "for name, results in all_results.items():\n",
    "    df = dataframes[name]\n",
    "    binned_from = []\n",
    "\n",
    "    for col, info in results.items():\n",
    "        k = info[\"best_k\"]\n",
    "        if k is None:\n",
    "            continue  # no valid bin count found\n",
    "\n",
    "        binned_col = f\"{col}_binned\"\n",
    "        df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "        binned_from.append(col)\n",
    "        print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "    # Remove the original continuous columns (keep IDs/labels)\n",
    "    df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "    # Update in-memory store\n",
    "    dataframes[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242659d",
   "metadata": {},
   "source": [
    "### Show unique values per df after binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts_dict = {\n",
    "    name: df.drop(columns=exclude, errors=\"ignore\").nunique()\n",
    "    for name, df in dataframes.items()\n",
    "}\n",
    "\n",
    "total_unique_dict = {\n",
    "    name: counts.sum()\n",
    "    for name, counts in unique_counts_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d82a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_unique_counts('mr_tr_IMPresseD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa7665",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export binned feature sets for Sepsis using a mirrored folder structure\n",
    "\n",
    "output_root = os.path.join(PROJECT_ROOT, \"3.2_binned_features\", \"sepsis\")\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    # Identify prefix (decl / mr_tr / payload)\n",
    "    prefix = None\n",
    "    for p in feature_folders:\n",
    "        if key.startswith(f\"{p}_\"):\n",
    "            prefix = p\n",
    "            break\n",
    "    if prefix is None:\n",
    "        raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "    # Subfolder name is everything after \"<prefix>_\"\n",
    "    subfolder = key[len(prefix) + 1 :]\n",
    "\n",
    "    # Map prefix to source folder name\n",
    "    folder_name = feature_folders[prefix]\n",
    "\n",
    "    # Target directory and file path\n",
    "    output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "    # Write CSV (no index)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {key!r} to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4931e9",
   "metadata": {},
   "source": [
    "## DHL Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423c71a",
   "metadata": {},
   "source": [
    "### Load each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c758b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load selected feature CSVs for DHL into a dictionary of DataFrames\n",
    "\n",
    "# feature_folders = {\n",
    "#     \"dhl\": \"dhl_features\",\n",
    "# }\n",
    "\n",
    "# dataframes = {}\n",
    "\n",
    "# for prefix, folder_name in feature_folders.items():\n",
    "#     base_dir = os.path.join(PROJECT_ROOT, \"3.1_selected_features\", \"DHL\", folder_name)\n",
    "#     if not os.path.isdir(base_dir):\n",
    "#         print(f\"[WARN] Missing folder: {base_dir}\")\n",
    "#         continue\n",
    "\n",
    "#     # In each encoding subfolder, keep only <subfolder>.csv\n",
    "#     for subfolder in os.listdir(base_dir):\n",
    "#         subfolder_dir = os.path.join(base_dir, subfolder)\n",
    "#         if not os.path.isdir(subfolder_dir):\n",
    "#             continue\n",
    "\n",
    "#         keep_name = f\"{subfolder}.csv\"\n",
    "#         for fname in os.listdir(subfolder_dir):\n",
    "#             fpath = os.path.join(subfolder_dir, fname)\n",
    "#             if os.path.isfile(fpath) and fname != keep_name:\n",
    "#                 os.remove(fpath)\n",
    "#                 print(f\"[CLEAN] Removed: {fpath}\")\n",
    "\n",
    "#     # Load remaining CSVs (one per subfolder)\n",
    "#     csv_paths = sorted(glob.glob(os.path.join(base_dir, \"*\", \"*.csv\")))\n",
    "#     for path in csv_paths:\n",
    "#         subfolder = os.path.basename(os.path.dirname(path))\n",
    "#         key = f\"{prefix}_{subfolder}\"\n",
    "#         df = pd.read_csv(path)\n",
    "#         dataframes[key] = df\n",
    "#         print(f\"[LOAD] {key}: shape={df.shape}\")\n",
    "\n",
    "# # Example keys:\n",
    "# #   'dhl_bs_data', 'dhl_dc_data', 'dhl_baseline', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a9948",
   "metadata": {},
   "source": [
    "### Strategy 2: Hard coding 2/3 bins and a threshold of 10+ unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MI-guided binning search for high-cardinality numeric features\n",
    "\n",
    "# threshold_unique = 10\n",
    "# all_results = {}\n",
    "\n",
    "# for name, df in dataframes.items():\n",
    "#     if \"Label\" not in df.columns:\n",
    "#         print(f\"Skipping {name!r}: no 'Label' column\")\n",
    "#         continue\n",
    "\n",
    "#     results = {}\n",
    "#     for col in df.columns:\n",
    "#         # Skip ID/target, non-numeric, or low-cardinality columns\n",
    "#         if col in (\"Label\", \"Case_ID\"):\n",
    "#             continue\n",
    "#         if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "#             continue\n",
    "#         if df[col].nunique() <= threshold_unique:\n",
    "#             continue\n",
    "\n",
    "#         best_k, scores = find_optimal_bins(df[col], df[\"Label\"], k_min=2, k_max=3)\n",
    "#         results[col] = {\"best_k\": best_k, \"mi_by_k\": scores}\n",
    "\n",
    "#     all_results[name] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect baseline results\n",
    "# print(\"Optimal bins for 'baseline':\")\n",
    "# for col, info in all_results['dhl_baseline'].items():\n",
    "#     print(f\" • {col}: {info['best_k']} bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply equal-frequency binning using the MI-selected k and replace originals\n",
    "\n",
    "# for name, results in all_results.items():\n",
    "#     df = dataframes[name]\n",
    "#     binned_from = []\n",
    "\n",
    "#     for col, info in results.items():\n",
    "#         k = info[\"best_k\"]\n",
    "#         if k is None:\n",
    "#             continue  # no valid number of bins found\n",
    "\n",
    "#         binned_col = f\"{col}_binned\"\n",
    "#         df[binned_col] = pd.qcut(df[col], q=k, duplicates=\"drop\")\n",
    "#         binned_from.append(col)\n",
    "#         print(f\"[{name}] {col!r} → {binned_col!r} ({k} bins)\")\n",
    "\n",
    "#     # Drop the original continuous columns\n",
    "#     df = df.drop(columns=binned_from, errors=\"ignore\")\n",
    "\n",
    "#     # Store updated DataFrame\n",
    "#     dataframes[name] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec12d5",
   "metadata": {},
   "source": [
    "### Export to 3.1_binned_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export binned feature sets for DHL using a mirrored folder structure\n",
    "\n",
    "# output_root = os.path.join(PROJECT_ROOT, \"3.2_binned_features\", \"DHL\")\n",
    "\n",
    "# for key, df in dataframes.items():\n",
    "#     # Identify prefix from key (e.g., \"dhl_<subfolder>\")\n",
    "#     prefix = None\n",
    "#     for p in feature_folders:\n",
    "#         if key.startswith(f\"{p}_\"):\n",
    "#             prefix = p\n",
    "#             break\n",
    "#     if prefix is None:\n",
    "#         raise KeyError(f\"Unrecognized prefix in key '{key}'\")\n",
    "\n",
    "#     # Subfolder name is everything after \"<prefix>_\"\n",
    "#     subfolder = key[len(prefix) + 1 :]\n",
    "\n",
    "#     # Map prefix to configured folder name\n",
    "#     folder_name = feature_folders[prefix]\n",
    "\n",
    "#     # Target directory and file path\n",
    "#     output_dir = os.path.join(output_root, folder_name, subfolder)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     csv_path = os.path.join(output_dir, f\"{subfolder}.csv\")\n",
    "\n",
    "#     # Write CSV (no index)\n",
    "#     df.to_csv(csv_path, index=False)\n",
    "#     print(f\"Saved {key!r} to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498fb1b",
   "metadata": {},
   "source": [
    "## Create overview of binning for report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218a496",
   "metadata": {},
   "source": [
    "#### Summarized overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3910e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BINNED_BASE_DIR   = \"3.2_binned_features\"\n",
    "ORIGINAL_BASE_DIR = \"3.1_selected_features\"  # pre-binning counterpart\n",
    "summary_dir  = os.path.join(BINNED_BASE_DIR, \"short_summary\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "summary_path = os.path.join(summary_dir, \"short_summary.csv\")\n",
    "\n",
    "# Name of the new column with binned unique counts (adjust per run)\n",
    "UNIQUE_COL_NAME = \"Unique values – strategy 2\"\n",
    "\n",
    "# Encodings to skip when summing uniques (case-insensitive folder names)\n",
    "EXCLUDED_ENCODINGS = {\"impressed\", \"mr\", \"mra\", \"tr\", \"tra\"}\n",
    "\n",
    "DROP_COLS = [\"Case_ID\", \"Label\"]\n",
    "KEYS = [\"Event Log\", \"Labeling\"]\n",
    "\n",
    "def _normalize_labeling(val: str):\n",
    "    s = str(val).lower()\n",
    "    if \"mr_tr\" in s:\n",
    "        return \"sequential\"\n",
    "    if \"decl\" in s:\n",
    "        return \"declare\"\n",
    "    if \"payload\" in s:\n",
    "        return \"payload\"\n",
    "    return val\n",
    "\n",
    "def _sum_unique_from_folder(folder_path: str) -> int:\n",
    "    \"\"\"Sum per-column unique counts across all CSVs in a folder (excluding Case_ID/Label).\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return 0\n",
    "    total_unique = 0\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(folder_path, fname))\n",
    "        df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        total_unique += df.nunique().sum()\n",
    "    return total_unique\n",
    "\n",
    "# Load prior summary (if present), normalize labels, and aggregate to (Event Log, Labeling)\n",
    "if os.path.exists(summary_path):\n",
    "    existing = pd.read_csv(summary_path)\n",
    "    if not existing.empty and \"Labeling\" in existing.columns:\n",
    "        existing[\"Labeling\"] = existing[\"Labeling\"].apply(_normalize_labeling)\n",
    "    if \"Total Features\" in existing.columns:  # drop legacy column\n",
    "        existing = existing.drop(columns=[\"Total Features\"])\n",
    "    if \"Encoding\" in existing.columns:        # roll up any older encoding-level files\n",
    "        numeric_cols = existing.select_dtypes(include=\"number\").columns.tolist()\n",
    "        existing = existing.groupby(KEYS, as_index=False)[numeric_cols].sum()\n",
    "else:\n",
    "    existing = pd.DataFrame()\n",
    "\n",
    "# Cache existing \"Original unique values\" to avoid recomputation\n",
    "existing_orig_lookup = {}\n",
    "if not existing.empty and \"Original unique values\" in existing.columns:\n",
    "    tmp = existing[KEYS + [\"Original unique values\"]].dropna(subset=[\"Original unique values\"])\n",
    "    if not tmp.empty:\n",
    "        tmp = tmp.groupby(KEYS, as_index=False)[\"Original unique values\"].sum()\n",
    "        existing_orig_lookup = {(r[\"Event Log\"], r[\"Labeling\"]): r[\"Original unique values\"] for _, r in tmp.iterrows()}\n",
    "\n",
    "# Build fresh binned unique counts per encoding (excluding EXCLUDED_ENCODINGS)\n",
    "records = []\n",
    "for event_log in sorted(os.listdir(BINNED_BASE_DIR)):\n",
    "    el_path = os.path.join(BINNED_BASE_DIR, event_log)\n",
    "    if not os.path.isdir(el_path) or event_log == \"short_summary\":\n",
    "        continue\n",
    "\n",
    "    for labeling in sorted(d for d in os.listdir(el_path) if os.path.isdir(os.path.join(el_path, d))):\n",
    "        strat_path_binned = os.path.join(el_path, labeling)\n",
    "        for encoding in sorted(os.listdir(strat_path_binned)):\n",
    "            if encoding.lower() in EXCLUDED_ENCODINGS:\n",
    "                continue\n",
    "            enc_path_binned = os.path.join(strat_path_binned, encoding)\n",
    "            if not os.path.isdir(enc_path_binned):\n",
    "                continue\n",
    "\n",
    "            binned_unique = _sum_unique_from_folder(enc_path_binned)\n",
    "            records.append({\n",
    "                \"Event Log\": event_log,\n",
    "                \"Labeling\": labeling,\n",
    "                \"Encoding\": encoding,\n",
    "                UNIQUE_COL_NAME: binned_unique,\n",
    "            })\n",
    "\n",
    "df_enc = (\n",
    "    pd.DataFrame(records).sort_values([\"Event Log\", \"Labeling\", \"Encoding\"]).reset_index(drop=True)\n",
    "    if records\n",
    "    else pd.DataFrame(columns=[\"Event Log\", \"Labeling\", \"Encoding\", UNIQUE_COL_NAME])\n",
    ")\n",
    "\n",
    "# Normalize labeling and aggregate to (Event Log, Labeling)\n",
    "df_enc[\"Labeling\"] = df_enc[\"Labeling\"].apply(_normalize_labeling)\n",
    "new_agg = (\n",
    "    df_enc.groupby(KEYS, as_index=False)[[UNIQUE_COL_NAME]].sum()\n",
    "    if not df_enc.empty\n",
    "    else pd.DataFrame(columns=KEYS + [UNIQUE_COL_NAME])\n",
    ")\n",
    "\n",
    "# Compute original (pre-binning) uniques only where not already known\n",
    "orig_values = []\n",
    "for _, row in new_agg.iterrows():\n",
    "    key = (row[\"Event Log\"], row[\"Labeling\"])\n",
    "    if key in existing_orig_lookup:\n",
    "        orig_values.append(existing_orig_lookup[key])\n",
    "    else:\n",
    "        strat_path_orig = os.path.join(ORIGINAL_BASE_DIR, row[\"Event Log\"], row[\"Labeling\"])\n",
    "        total_orig_unique = 0\n",
    "        if os.path.isdir(strat_path_orig):\n",
    "            for enc in sorted(os.listdir(strat_path_orig)):\n",
    "                if enc.lower() in EXCLUDED_ENCODINGS:\n",
    "                    continue\n",
    "                enc_path_orig = os.path.join(strat_path_orig, enc)\n",
    "                if os.path.isdir(enc_path_orig):\n",
    "                    total_orig_unique += _sum_unique_from_folder(enc_path_orig)\n",
    "        orig_values.append(total_orig_unique)\n",
    "\n",
    "new_agg[\"Original unique values\"] = orig_values\n",
    "\n",
    "# Merge with existing summary (update strategy column, preserve prior originals)\n",
    "if existing.empty:\n",
    "    updated = new_agg.copy()\n",
    "else:\n",
    "    ex = existing.set_index(KEYS)\n",
    "    nw = new_agg.set_index(KEYS)\n",
    "\n",
    "    if \"Original unique values\" not in ex.columns:\n",
    "        ex[\"Original unique values\"] = pd.NA\n",
    "    if UNIQUE_COL_NAME not in ex.columns:\n",
    "        ex[UNIQUE_COL_NAME] = pd.NA\n",
    "\n",
    "    # Update shared rows\n",
    "    common_idx = nw.index.intersection(ex.index)\n",
    "    ex.loc[common_idx, UNIQUE_COL_NAME] = nw.loc[common_idx, UNIQUE_COL_NAME]\n",
    "\n",
    "    need_orig_mask = ex.loc[common_idx, \"Original unique values\"].isna()\n",
    "    if need_orig_mask.any():\n",
    "        idx_to_fill = need_orig_mask[need_orig_mask].index\n",
    "        ex.loc[idx_to_fill, \"Original unique values\"] = nw.loc[idx_to_fill, \"Original unique values\"]\n",
    "\n",
    "    # Append new rows\n",
    "    missing_idx = nw.index.difference(ex.index)\n",
    "    ex = pd.concat([ex, nw.loc[missing_idx]], axis=0)\n",
    "\n",
    "    updated = ex.reset_index()\n",
    "\n",
    "# Finalize columns/order and write\n",
    "front = [\"Event Log\", \"Labeling\", \"Original unique values\"]\n",
    "rest = [c for c in updated.columns if c not in front]\n",
    "updated = updated[front + rest].sort_values(KEYS).reset_index(drop=True)\n",
    "\n",
    "for legacy in (\"Total Features\", \"Encoding\"):\n",
    "    if legacy in updated.columns:\n",
    "        updated = updated.drop(columns=[legacy])\n",
    "\n",
    "updated.to_csv(summary_path, index=False)\n",
    "print(f\"Summary updated: {summary_path}\")\n",
    "print(f\"Added/updated column: {UNIQUE_COL_NAME}\")\n",
    "updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2d65f",
   "metadata": {},
   "source": [
    "#### Long overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and paths\n",
    "BINNED_BASE_DIR   = \"3.2_binned_features\"\n",
    "ORIGINAL_BASE_DIR = \"3.1_selected_features\"\n",
    "long_dir  = os.path.join(BINNED_BASE_DIR, \"long_overview\")\n",
    "os.makedirs(long_dir, exist_ok=True)\n",
    "long_path = os.path.join(long_dir, \"long_overview.csv\")\n",
    "\n",
    "# Encodings to skip (case-insensitive folder names)\n",
    "EXCLUDED_ENCODINGS = {\"impressed\", \"mr\", \"mra\", \"tr\", \"tra\"}\n",
    "\n",
    "DROP_COLS = [\"Case_ID\", \"Label\"]\n",
    "KEYS = [\"Event Log\", \"Labeling\", \"Encoding\"]\n",
    "\n",
    "# Reuse UNIQUE_COL_NAME if set; otherwise infer from short_summary\n",
    "try:\n",
    "    UNIQUE_COL_NAME\n",
    "except NameError:\n",
    "    ss_path = os.path.join(BINNED_BASE_DIR, \"short_summary\", \"short_summary.csv\")\n",
    "    if os.path.exists(ss_path):\n",
    "        _ss = pd.read_csv(ss_path)\n",
    "        cand = [c for c in _ss.columns if isinstance(c, str) and c.lower().startswith(\"unique values\")]\n",
    "        UNIQUE_COL_NAME = cand[-1] if cand else \"Unique values – strategy\"\n",
    "    else:\n",
    "        UNIQUE_COL_NAME = \"Unique values – strategy\"\n",
    "\n",
    "def _normalize_labeling(val: str):\n",
    "    s = str(val).lower()\n",
    "    if \"mr_tr\" in s:\n",
    "        return \"sequential\"\n",
    "    if \"decl\" in s:\n",
    "        return \"declare\"\n",
    "    if \"payload\" in s:\n",
    "        return \"payload\"\n",
    "    return val\n",
    "\n",
    "def _sum_unique_from_folder(folder_path: str) -> int:\n",
    "    \"\"\"Sum per-column unique counts across all CSVs in a folder (excluding Case_ID/Label).\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        return 0\n",
    "    total_unique = 0\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        df = pd.read_csv(os.path.join(folder_path, fname))\n",
    "        df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "        total_unique += df.nunique().sum()\n",
    "    return total_unique\n",
    "\n",
    "# Load existing long_overview (if present) and normalize labels\n",
    "if os.path.exists(long_path):\n",
    "    existing = pd.read_csv(long_path)\n",
    "    if not existing.empty and \"Labeling\" in existing.columns:\n",
    "        existing[\"Labeling\"] = existing[\"Labeling\"].apply(_normalize_labeling)\n",
    "    for col in (\"Total Features\",):\n",
    "        if col in existing.columns:\n",
    "            existing = existing.drop(columns=[col])\n",
    "else:\n",
    "    existing = pd.DataFrame()\n",
    "\n",
    "# Cache original unique counts to avoid recomputation\n",
    "existing_orig_lookup = {}\n",
    "if not existing.empty and \"Original unique values\" in existing.columns:\n",
    "    tmp = existing[KEYS + [\"Original unique values\"]].dropna(subset=[\"Original unique values\"])\n",
    "    if not tmp.empty:\n",
    "        tmp = tmp.drop_duplicates(subset=KEYS, keep=\"last\")\n",
    "        existing_orig_lookup = {\n",
    "            (r[\"Event Log\"], r[\"Labeling\"], r[\"Encoding\"]): r[\"Original unique values\"]\n",
    "            for _, r in tmp.iterrows()\n",
    "        }\n",
    "\n",
    "# Build fresh records per encoding\n",
    "records = []\n",
    "for event_log in sorted(os.listdir(BINNED_BASE_DIR)):\n",
    "    if event_log in (\"short_summary\", \"long_overview\"):\n",
    "        continue\n",
    "    el_path = os.path.join(BINNED_BASE_DIR, event_log)\n",
    "    if not os.path.isdir(el_path):\n",
    "        continue\n",
    "\n",
    "    label_strats = sorted(d for d in os.listdir(el_path) if os.path.isdir(os.path.join(el_path, d)))\n",
    "    if not label_strats:\n",
    "        continue\n",
    "\n",
    "    for labeling in label_strats:\n",
    "        labeling_path_binned = os.path.join(el_path, labeling)\n",
    "        labeling_path_orig   = os.path.join(ORIGINAL_BASE_DIR, event_log, labeling)\n",
    "        labeling_norm = _normalize_labeling(labeling)\n",
    "\n",
    "        for encoding in sorted(os.listdir(labeling_path_binned)):\n",
    "            if encoding.lower() in EXCLUDED_ENCODINGS:\n",
    "                continue\n",
    "            enc_path_binned = os.path.join(labeling_path_binned, encoding)\n",
    "            if not os.path.isdir(enc_path_binned):\n",
    "                continue\n",
    "\n",
    "            binned_unique = _sum_unique_from_folder(enc_path_binned)\n",
    "\n",
    "            key = (event_log, labeling_norm, encoding)\n",
    "            if key in existing_orig_lookup:\n",
    "                orig_unique = existing_orig_lookup[key]\n",
    "            else:\n",
    "                enc_path_orig = os.path.join(labeling_path_orig, encoding)\n",
    "                orig_unique = _sum_unique_from_folder(enc_path_orig)\n",
    "\n",
    "            records.append({\n",
    "                \"Event Log\": event_log,\n",
    "                \"Labeling\": labeling_norm,\n",
    "                \"Encoding\": encoding,\n",
    "                \"Original unique values\": orig_unique,\n",
    "                UNIQUE_COL_NAME: binned_unique,\n",
    "            })\n",
    "\n",
    "new_df = (\n",
    "    pd.DataFrame(records).sort_values(KEYS).reset_index(drop=True)\n",
    "    if records else\n",
    "    pd.DataFrame(columns=KEYS + [\"Original unique values\", UNIQUE_COL_NAME])\n",
    ")\n",
    "\n",
    "# Merge with existing (update strategy column; preserve existing originals)\n",
    "if existing.empty:\n",
    "    updated = new_df.copy()\n",
    "else:\n",
    "    ex = existing.set_index(KEYS)\n",
    "    nw = new_df.set_index(KEYS)\n",
    "\n",
    "    if \"Original unique values\" not in ex.columns:\n",
    "        ex[\"Original unique values\"] = pd.NA\n",
    "    if UNIQUE_COL_NAME not in ex.columns:\n",
    "        ex[UNIQUE_COL_NAME] = pd.NA\n",
    "\n",
    "    common_idx = nw.index.intersection(ex.index)\n",
    "    ex.loc[common_idx, UNIQUE_COL_NAME] = nw.loc[common_idx, UNIQUE_COL_NAME]\n",
    "\n",
    "    need_orig_mask = ex.loc[common_idx, \"Original unique values\"].isna()\n",
    "    if need_orig_mask.any():\n",
    "        idx_to_fill = need_orig_mask[need_orig_mask].index\n",
    "        ex.loc[idx_to_fill, \"Original unique values\"] = nw.loc[idx_to_fill, \"Original unique values\"]\n",
    "\n",
    "    missing_idx = nw.index.difference(ex.index)\n",
    "    ex = pd.concat([ex, nw.loc[missing_idx]], axis=0)\n",
    "\n",
    "    updated = ex.reset_index()\n",
    "\n",
    "# Order and save\n",
    "front = [\"Event Log\", \"Labeling\", \"Encoding\", \"Original unique values\"]\n",
    "rest  = [c for c in updated.columns if c not in front]\n",
    "updated = updated[front + rest].sort_values(KEYS).reset_index(drop=True)\n",
    "\n",
    "updated.to_csv(long_path, index=False)\n",
    "print(f\"Long overview updated: {long_path}\")\n",
    "print(f\"Added/updated column: {UNIQUE_COL_NAME}\")\n",
    "updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdef24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for LaTeX export\n",
    "def fmt_rule(x):\n",
    "    return r\"\\detokenize{\" + str(x) + \"}\"\n",
    "\n",
    "# Output targets\n",
    "fp_short_tex = os.path.join(\"3.2_binned_features\", \"short_summary\", \"short_summary.tex\")\n",
    "fp_long_tex  = os.path.join(\"3.2_binned_features\", \"long_overview\",  \"long_summary.tex\")\n",
    "\n",
    "# Source CSVs\n",
    "fp_short_csv = os.path.join(\"3.2_binned_features\", \"short_summary\", \"short_summary.csv\")\n",
    "fp_long_csv  = os.path.join(\"3.2_binned_features\", \"long_overview\",  \"long_overview.csv\")\n",
    "\n",
    "def _colalign(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Right-align numeric columns, left-align others.\"\"\"\n",
    "    return \"\".join(\"r\" if pd.api.types.is_numeric_dtype(df[c]) else \"l\" for c in df.columns)\n",
    "\n",
    "def _to_integers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cast numeric columns to nullable integers (rounded).\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in out.select_dtypes(include=\"number\").columns:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(0).astype(\"Int64\")\n",
    "    return out\n",
    "\n",
    "def _reorder_unique_value_strategy_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reorder columns named 'Unique values – strategy N' by N while preserving\n",
    "    the position where the first such column originally appears.\n",
    "    Accepts '-' or '–' as the separator; case-insensitive.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    pat = re.compile(r\"^\\s*Unique\\s+values\\s*[–-]\\s*strategy\\s*(\\d+)\\s*$\", re.IGNORECASE)\n",
    "    matches = [(int(pat.match(str(c)).group(1)), c) for c in cols if pat.match(str(c))]\n",
    "    if not matches:\n",
    "        return df\n",
    "\n",
    "    matches_sorted = sorted(matches, key=lambda t: t[0])\n",
    "    uv_names_sorted = [c for _, c in matches_sorted]\n",
    "    first_pos = min(cols.index(c) for _, c in matches)\n",
    "    base = [c for c in cols if c not in [name for _, name in matches]]\n",
    "    base[first_pos:first_pos] = uv_names_sorted\n",
    "    return df[base]\n",
    "\n",
    "def _df_to_longtable_tex(df: pd.DataFrame, formatters: dict | None) -> str:\n",
    "    \"\"\"Render a DataFrame to a LaTeX longtable string.\"\"\"\n",
    "    return df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        longtable=True,\n",
    "        multicolumn=False,\n",
    "        column_format=_colalign(df),\n",
    "        na_rep=\"\",\n",
    "        formatters=formatters,\n",
    "    )\n",
    "\n",
    "def _save_tex(df: pd.DataFrame, out_path: str, formatters: dict | None):\n",
    "    \"\"\"Write a LaTeX longtable to disk.\"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(_df_to_longtable_tex(df, formatters))\n",
    "\n",
    "# Short summary → TeX\n",
    "if os.path.exists(fp_short_csv):\n",
    "    short_df = pd.read_csv(fp_short_csv)\n",
    "    short_df = _reorder_unique_value_strategy_cols(short_df)\n",
    "    short_df = _to_integers(short_df)\n",
    "    all_fmt = {col: fmt_rule for col in short_df.columns}\n",
    "    _save_tex(short_df, fp_short_tex, all_fmt)\n",
    "    print(f\"Wrote short_summary TeX → {fp_short_tex}\")\n",
    "else:\n",
    "    print(f\"Missing: {fp_short_csv}\")\n",
    "\n",
    "# Long overview → TeX\n",
    "if os.path.exists(fp_long_csv):\n",
    "    long_df = pd.read_csv(fp_long_csv)\n",
    "    long_df = _reorder_unique_value_strategy_cols(long_df)\n",
    "    long_df = _to_integers(long_df)\n",
    "    enc_fmt = {\"Encoding\": fmt_rule} if \"Encoding\" in long_df.columns else None\n",
    "    _save_tex(long_df, fp_long_tex, enc_fmt)\n",
    "    print(f\"Wrote long_overview TeX → {fp_long_tex}\")\n",
    "else:\n",
    "    print(f\"Missing: {fp_long_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752ba8e",
   "metadata": {},
   "source": [
    "### Checking unique values before binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize per-encoding unique-value counts (excluding specified encodings and ID/label columns)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path(\"3.1_selected_features\")\n",
    "exclude_encodings = {\"impressed\", \"mr\", \"mra\", \"tr\", \"tra\", \"dhl\"}  # case-insensitive\n",
    "exclude_cols = {\"case_id\", \"label\"}  # case-insensitive\n",
    "\n",
    "rows = []\n",
    "missing = []\n",
    "\n",
    "for log_dir in sorted([p for p in base_dir.iterdir() if p.is_dir()]):\n",
    "    log = log_dir.name\n",
    "    for lab_dir in sorted([p for p in log_dir.iterdir() if p.is_dir()]):\n",
    "        labeling = lab_dir.name\n",
    "        for enc_dir in sorted([p for p in lab_dir.iterdir() if p.is_dir()]):\n",
    "            encoding = enc_dir.name\n",
    "            if encoding.lower() in exclude_encodings:\n",
    "                continue\n",
    "\n",
    "            csv_files = sorted(enc_dir.glob(\"*.csv\"))\n",
    "            if not csv_files:\n",
    "                missing.append((log, labeling, encoding, \"no CSV found\"))\n",
    "                continue\n",
    "\n",
    "            for csv_fp in csv_files:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_fp, low_memory=False)\n",
    "                except Exception as e:\n",
    "                    missing.append((log, labeling, encoding, f\"failed to read: {csv_fp.name} ({e})\"))\n",
    "                    continue\n",
    "\n",
    "                per_col_counts = []\n",
    "                for col in df.columns:\n",
    "                    if col.strip().lower() in exclude_cols:\n",
    "                        continue\n",
    "                    try:\n",
    "                        per_col_counts.append(df[col].nunique(dropna=False))\n",
    "                    except Exception:\n",
    "                        per_col_counts.append(df[col].astype(str).nunique(dropna=False))\n",
    "\n",
    "                total_unique = int(sum(int(x) for x in per_col_counts)) if per_col_counts else 0\n",
    "                rows.append({\n",
    "                    \"Log\": log,\n",
    "                    \"Labeling\": labeling,\n",
    "                    \"Encoding\": encoding,\n",
    "                    \"Columns counted\": int(len(per_col_counts)),\n",
    "                    \"Unique values (total across columns)\": total_unique,\n",
    "                    \"Source file\": csv_fp.name,\n",
    "                })\n",
    "\n",
    "# Aggregate to one row per Log/Labeling/Encoding\n",
    "agg_per_encoding = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"Log\", \"Labeling\", \"Encoding\", \"Columns counted\", \"Unique values (total across columns)\", \"Source file\"]\n",
    ")\n",
    "\n",
    "if not agg_per_encoding.empty:\n",
    "    agg_per_encoding = (\n",
    "        agg_per_encoding\n",
    "        .groupby([\"Log\", \"Labeling\", \"Encoding\"], as_index=False)\n",
    "        .agg({\n",
    "            \"Columns counted\": \"sum\",\n",
    "            \"Unique values (total across columns)\": \"sum\",\n",
    "        })\n",
    "        .sort_values([\"Log\", \"Labeling\", \"Encoding\"], kind=\"stable\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "print(f\"Encodings summarized: {len(agg_per_encoding):,}\")\n",
    "if missing:\n",
    "    print(\"Notes/warnings:\")\n",
    "    for item in missing:\n",
    "        print(\"  -\", \" / \".join(map(str, item)))\n",
    "\n",
    "# Optional: save to disk\n",
    "# agg_per_encoding.to_csv(\"3.1_selected_features_unique_counts_per_encoding.csv\", index=False)\n",
    "\n",
    "agg_per_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec29cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate uniqueness across encodings within each (Log, Labeling)\n",
    "\n",
    "if \"agg_per_encoding\" not in globals() or agg_per_encoding.empty:\n",
    "    raise RuntimeError(\"agg_per_encoding not found or empty. Run the previous cell first.\")\n",
    "\n",
    "agg_per_labeling = (\n",
    "    agg_per_encoding\n",
    "    .groupby([\"Log\", \"Labeling\"], as_index=False)\n",
    "    .agg(\n",
    "        Encodings_count=(\"Encoding\", \"nunique\"),\n",
    "        Columns_counted_total=(\"Columns counted\", \"sum\"),\n",
    "        Unique_values_total=(\"Unique values (total across columns)\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Derived metrics\n",
    "agg_per_labeling[\"Avg columns per encoding\"] = (\n",
    "    agg_per_labeling[\"Columns_counted_total\"] / agg_per_labeling[\"Encodings_count\"]\n",
    ").round(2)\n",
    "\n",
    "agg_per_labeling[\"Avg unique-values per encoding\"] = (\n",
    "    agg_per_labeling[\"Unique_values_total\"] / agg_per_labeling[\"Encodings_count\"]\n",
    ").round(2)\n",
    "\n",
    "# Ordering\n",
    "agg_per_labeling = agg_per_labeling.sort_values([\"Log\", \"Labeling\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Labeling groups summarized: {len(agg_per_labeling):,}\")\n",
    "\n",
    "# Optional: save\n",
    "# agg_per_labeling.to_csv(\"3.1_selected_features_unique_counts_per_labeling.csv\", index=False)\n",
    "\n",
    "agg_per_labeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
