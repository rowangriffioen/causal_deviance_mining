{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1310e1",
   "metadata": {},
   "source": [
    "### 0. Initizalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3715e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# Add repository root to PYTHONPATH for local imports\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Load configuration\n",
    "with open(os.path.join(PROJECT_ROOT, \"config\", \"config_feature_selection.yaml\"), \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Configured dataset identifiers\n",
    "EVENT_LOG = cfg[\"event_log\"]\n",
    "EXP_NAME  = cfg[\"experiment_name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687528c",
   "metadata": {},
   "source": [
    "### Preprocessing for IMPressed\n",
    "1. Renames training_encoded_log.csv → IMPresseD.csv\n",
    "2. Renames column Outcome → Label inside IMPresseD.csv\n",
    "3. Copies all other files from each IMPresseD encoding folder into PROJECT_ROOT/IMPresseD_features/<dataset>/<labeling>/ (excluding IMPresseD.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d68ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_root = os.path.join(PROJECT_ROOT, \"3_extracted_features\")\n",
    "target_root    = os.path.join(PROJECT_ROOT, \"IMPresseD_patterns\")\n",
    "os.makedirs(target_root, exist_ok=True)\n",
    "\n",
    "# Walk: <root>/3_extracted_features/<dataset>/<labeling>/IMPresseD/\n",
    "for dataset in sorted(os.listdir(processed_root)):\n",
    "    dataset_dir = os.path.join(processed_root, dataset)\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        continue\n",
    "\n",
    "    for labeling in sorted(os.listdir(dataset_dir)):\n",
    "        labeling_dir = os.path.join(dataset_dir, labeling)\n",
    "        if not os.path.isdir(labeling_dir):\n",
    "            continue\n",
    "\n",
    "        imp_dir = os.path.join(labeling_dir, \"IMPresseD\")\n",
    "        if not os.path.isdir(imp_dir):\n",
    "            continue\n",
    "\n",
    "        old_csv = os.path.join(imp_dir, \"training_encoded_log.csv\")\n",
    "        new_csv = os.path.join(imp_dir, \"IMPresseD.csv\")\n",
    "\n",
    "        # 1) Ensure canonical CSV filename\n",
    "        try:\n",
    "            if os.path.exists(old_csv) and not os.path.exists(new_csv):\n",
    "                os.rename(old_csv, new_csv)\n",
    "                print(f\"[RENAME] {old_csv} -> {new_csv}\")\n",
    "            elif os.path.exists(new_csv):\n",
    "                print(f\"[OK] Canonical file present: {new_csv}\")\n",
    "            else:\n",
    "                print(f\"[WARN] Missing training file in {imp_dir} (skip rename)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Rename in {imp_dir}: {e}\")\n",
    "\n",
    "        # 2) Normalize column name: Outcome -> Label\n",
    "        if os.path.exists(new_csv):\n",
    "            try:\n",
    "                df = pd.read_csv(new_csv)\n",
    "                if \"Outcome\" in df.columns:\n",
    "                    df = df.rename(columns={\"Outcome\": \"Label\"})\n",
    "                    df.to_csv(new_csv, index=False)\n",
    "                    print(f\"[COLUMNS] Renamed 'Outcome' → 'Label' in {new_csv}\")\n",
    "                else:\n",
    "                    print(f\"[COLUMNS] No 'Outcome' column in {new_csv}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Read/write {new_csv}: {e}\")\n",
    "\n",
    "        # 3) Move auxiliary files to <root>/IMPresseD_patterns/<dataset>/<labeling>/\n",
    "        out_dir = os.path.join(target_root, dataset, labeling)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            for fname in os.listdir(imp_dir):\n",
    "                if fname == \"IMPresseD.csv\":\n",
    "                    continue  # keep canonical CSV in place\n",
    "\n",
    "                fpath = os.path.join(imp_dir, fname)\n",
    "                if not os.path.isfile(fpath):\n",
    "                    continue\n",
    "\n",
    "                dest = os.path.join(out_dir, fname)\n",
    "\n",
    "                # Overwrite destination if necessary\n",
    "                if os.path.exists(dest):\n",
    "                    try:\n",
    "                        os.remove(dest)\n",
    "                    except IsADirectoryError:\n",
    "                        shutil.rmtree(dest)\n",
    "\n",
    "                shutil.move(fpath, dest)\n",
    "                print(f\"[MOVE] {dataset}/{labeling}: {fname} -> {dest}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Moving extras from {imp_dir}: {e}\")\n",
    "\n",
    "        # 4) Remove IMPresseD directory if no files remain\n",
    "        try:\n",
    "            remaining = [\n",
    "                f for f in os.listdir(imp_dir)\n",
    "                if os.path.isfile(os.path.join(imp_dir, f))\n",
    "            ]\n",
    "            if not remaining:\n",
    "                os.rmdir(imp_dir)\n",
    "                print(f\"[CLEAN] Removed empty directory {imp_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Cleanup {imp_dir} skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad4b9a",
   "metadata": {},
   "source": [
    "### 1. Define function for Fisher Scoring (coverage and topK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher scoring for binary classification (standalone)\n",
    "def _fisher_scores_binary(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the Fisher score per feature for binary labels (0/1).\n",
    "    Score = between-class variance / within-class (pooled) variance.\n",
    "    \"\"\"\n",
    "    y = np.asarray(y).ravel()\n",
    "    if not set(np.unique(y)).issubset({0, 1}):\n",
    "        raise ValueError(\"Labels must be binary (0/1).\")\n",
    "\n",
    "    pos = (y == 1)\n",
    "    neg = (y == 0)\n",
    "    n_pos, n_neg = pos.sum(), neg.sum()\n",
    "\n",
    "    mu      = X.mean(axis=0)\n",
    "    mu_pos  = X[pos].mean(axis=0) if n_pos else np.zeros(X.shape[1])\n",
    "    mu_neg  = X[neg].mean(axis=0) if n_neg else np.zeros(X.shape[1])\n",
    "\n",
    "    var_pos = X[pos].var(axis=0, ddof=0) if n_pos else np.zeros(X.shape[1])\n",
    "    var_neg = X[neg].var(axis=0, ddof=0) if n_neg else np.zeros(X.shape[1])\n",
    "\n",
    "    between = n_pos * (mu_pos - mu) ** 2 + n_neg * (mu_neg - mu) ** 2\n",
    "    within  = n_pos * var_pos + n_neg * var_neg\n",
    "\n",
    "    # Features with zero within-class variance get score 0\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        scores = np.where(within > 0, between / within, 0.0).astype(float)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher helpers (prefer scikit-feature; fallback to local implementation)\n",
    "def _rank_by_fisher(X: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Return (rank_idx_desc, scores_or_None).\n",
    "    Uses skfeature.fisher_score if available; otherwise falls back to _fisher_scores_binary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from skfeature.function.similarity_based import fisher_score\n",
    "        try:\n",
    "            rank_idx = fisher_score.fisher_score(X, y, mode=\"rank\")\n",
    "            scores = None\n",
    "        except TypeError:\n",
    "            scores = fisher_score.fisher_score(X, y)\n",
    "            rank_idx = np.argsort(-scores)\n",
    "    except Exception:\n",
    "        scores = _fisher_scores_binary(X, y)\n",
    "        rank_idx = np.argsort(-scores)\n",
    "    return rank_idx, scores\n",
    "\n",
    "\n",
    "# Top-k Fisher selector (single DataFrame)\n",
    "def fisher_topk_select(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str = \"Label\",\n",
    "    case_id_col: str = \"Case_ID\",\n",
    "    k: int = 100,\n",
    "    drop_constant: bool = True,\n",
    "    drop_duplicate_columns: bool = True,\n",
    "    fillna_value: float = 0.0,\n",
    "    return_features_only: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rank numeric features by Fisher score and keep the top k after removing\n",
    "    duplicate and constant columns. Always preserves Label and Case_ID if present.\n",
    "    \"\"\"\n",
    "    if label_col not in df.columns:\n",
    "        raise KeyError(f\"Label column '{label_col}' not found.\")\n",
    "\n",
    "    # Labels → binary {0,1}\n",
    "    y = df[label_col]\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    uniq = set(pd.unique(y))\n",
    "    if not uniq.issubset({0, 1}):\n",
    "        y = pd.Series(pd.Categorical(y).codes, index=y.index)\n",
    "        if not set(pd.unique(y)).issubset({0, 1}):\n",
    "            raise ValueError(\"Label column must be binary (0/1 or bool).\")\n",
    "\n",
    "    # Columns to retain in the output\n",
    "    id_cols = [c for c in [case_id_col, label_col] if c in df.columns]\n",
    "\n",
    "    # Numeric feature pool (exclude ID columns)\n",
    "    feats_df = df.drop(columns=id_cols, errors=\"ignore\").select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    # Remove duplicate and constant columns\n",
    "    if drop_duplicate_columns and feats_df.shape[1] > 1:\n",
    "        feats_df = feats_df.T.drop_duplicates().T\n",
    "    if drop_constant and not feats_df.empty:\n",
    "        non_constant = (feats_df != feats_df.iloc[0]).any()\n",
    "        feats_df = feats_df.loc[:, non_constant]\n",
    "\n",
    "    feats_df = feats_df.fillna(fillna_value)\n",
    "    X = feats_df.to_numpy(dtype=float)\n",
    "    y_arr = y.to_numpy(dtype=int)\n",
    "\n",
    "    if X.shape[1] == 0:\n",
    "        out_df = df[id_cols].copy() if not return_features_only else df.iloc[:, 0:0].copy()\n",
    "        return out_df, [], {\n",
    "            \"method\": \"fisher_topk\",\n",
    "            \"reason\": \"no_features_after_filtering\",\n",
    "            \"selected_count\": 0,\n",
    "            \"postfilter_features\": 0,\n",
    "            \"k\": k,\n",
    "        }\n",
    "\n",
    "    rank_idx, scores = _rank_by_fisher(X, y_arr)\n",
    "    k_eff = min(k, X.shape[1])\n",
    "    keep_idx = rank_idx[:k_eff]\n",
    "    selected_cols = feats_df.columns[keep_idx].tolist()\n",
    "\n",
    "    # Assemble output\n",
    "    if return_features_only:\n",
    "        selected_df = feats_df[selected_cols].copy()\n",
    "    else:\n",
    "        selected_df = pd.concat([df[id_cols].copy(), feats_df[selected_cols]], axis=1)\n",
    "\n",
    "    if scores is None:\n",
    "        scores = _fisher_scores_binary(X, y_arr)\n",
    "\n",
    "    info = {\n",
    "        \"method\": \"fisher_topk\",\n",
    "        \"k\": k,\n",
    "        \"selected_count\": len(selected_cols),\n",
    "        \"postfilter_features\": feats_df.shape[1],\n",
    "        \"ranked_features\": feats_df.columns[rank_idx].tolist(),\n",
    "        \"ranked_scores\": scores[rank_idx].tolist(),\n",
    "    }\n",
    "    return selected_df, selected_cols, info\n",
    "\n",
    "\n",
    "# Greedy coverage selector (single DataFrame)\n",
    "def fisher_coverage_select(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str = \"Label\",\n",
    "    case_id_col: str = \"Case_ID\",\n",
    "    coverage_threshold: int = 20,\n",
    "    positive_predicate: Callable[[pd.Series], pd.Series] = None,\n",
    "    drop_constant: bool = True,\n",
    "    drop_duplicate_columns: bool = True,\n",
    "    fillna_value: float = 0.0,\n",
    "    return_features_only: bool = False,\n",
    ") -> Tuple[pd.DataFrame, List[str], Dict]:\n",
    "    \"\"\"\n",
    "    Fisher ranking combined with greedy coverage selection.\n",
    "    Select features until each row reaches the desired number of positive predicates.\n",
    "    Preserves Label and Case_ID if present.\n",
    "    \"\"\"\n",
    "    if label_col not in df.columns:\n",
    "        raise KeyError(f\"Label column '{label_col}' not found.\")\n",
    "\n",
    "    # Labels → binary {0,1}\n",
    "    y = df[label_col]\n",
    "    if y.dtype == bool:\n",
    "        y = y.astype(int)\n",
    "    uniq = set(pd.unique(y))\n",
    "    if not uniq.issubset({0, 1}):\n",
    "        y = pd.Series(pd.Categorical(y).codes, index=y.index)\n",
    "        if not set(pd.unique(y)).issubset({0, 1}):\n",
    "            raise ValueError(\"Label column must be binary (0/1 or bool).\")\n",
    "\n",
    "    # Columns to retain in the output\n",
    "    id_cols = [c for c in [case_id_col, label_col] if c in df.columns]\n",
    "\n",
    "    # Numeric feature pool (exclude ID columns)\n",
    "    feats_df = df.drop(columns=id_cols, errors=\"ignore\").select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    # Remove duplicate and constant columns\n",
    "    if drop_duplicate_columns and feats_df.shape[1] > 1:\n",
    "        feats_df = feats_df.T.drop_duplicates().T\n",
    "    if drop_constant and not feats_df.empty:\n",
    "        non_constant = (feats_df != feats_df.iloc[0]).any()\n",
    "        feats_df = feats_df.loc[:, non_constant]\n",
    "\n",
    "    feats_df = feats_df.fillna(fillna_value)\n",
    "    X = feats_df.to_numpy(dtype=float)\n",
    "    y_arr = y.to_numpy(dtype=int)\n",
    "\n",
    "    if X.shape[1] == 0:\n",
    "        out_df = df[id_cols].copy() if not return_features_only else df.iloc[:, 0:0].copy()\n",
    "        return out_df, [], {\n",
    "            \"method\": \"coverage\",\n",
    "            \"reason\": \"no_features_after_filtering\",\n",
    "            \"coverage_threshold\": coverage_threshold,\n",
    "            \"coverage_per_row\": np.zeros(len(df), dtype=int).tolist(),\n",
    "            \"unattainable_rows\": list(range(len(df))),\n",
    "            \"selected_count\": 0,\n",
    "        }\n",
    "\n",
    "    rank_idx, scores = _rank_by_fisher(X, y_arr)\n",
    "\n",
    "    if positive_predicate is None:\n",
    "        def positive_predicate(col: pd.Series) -> pd.Series:\n",
    "            return col > 0\n",
    "\n",
    "    needed = np.full(X.shape[0], coverage_threshold, dtype=int)\n",
    "    selected_indices: List[int] = []\n",
    "\n",
    "    positives_matrix = (X > 0)\n",
    "    max_attainable_per_row = positives_matrix.sum(axis=1)\n",
    "    unattainable_rows = np.where(max_attainable_per_row < coverage_threshold)[0].tolist()\n",
    "\n",
    "    for j in rank_idx:\n",
    "        if selected_indices and (needed <= 0).all():\n",
    "            break\n",
    "        col = feats_df.iloc[:, j]\n",
    "        pos_mask = positive_predicate(col).to_numpy()\n",
    "\n",
    "        helps_mask = pos_mask & (needed > 0)\n",
    "        if not np.any(helps_mask):\n",
    "            continue\n",
    "\n",
    "        selected_indices.append(j)\n",
    "        needed = np.where(helps_mask, needed - 1, needed)\n",
    "\n",
    "    selected_cols = feats_df.columns[selected_indices].tolist()\n",
    "\n",
    "    # Assemble output\n",
    "    if return_features_only:\n",
    "        selected_df = feats_df[selected_cols].copy()\n",
    "    else:\n",
    "        selected_df = pd.concat([df[id_cols].copy(), feats_df[selected_cols]], axis=1)\n",
    "\n",
    "    if scores is None:\n",
    "        scores = _fisher_scores_binary(X, y_arr)\n",
    "\n",
    "    info = {\n",
    "        \"method\": \"coverage\",\n",
    "        \"coverage_threshold\": coverage_threshold,\n",
    "        \"coverage_per_row\": (max_attainable_per_row - needed.clip(min=0)).tolist(),\n",
    "        \"unattainable_rows\": unattainable_rows,\n",
    "        \"attained_min\": int((max_attainable_per_row - needed.clip(min=0)).min()) if len(df) else 0,\n",
    "        \"selected_count\": len(selected_cols),\n",
    "        \"ranked_features\": feats_df.columns[rank_idx].tolist(),\n",
    "        \"ranked_scores\": scores[rank_idx].tolist(),\n",
    "    }\n",
    "    return selected_df, selected_cols, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75ee3d",
   "metadata": {},
   "source": [
    "### 1.1 Process all logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch feature selection runner\n",
    "\n",
    "# Selection method: \"coverage\" or \"fisher_topk\"\n",
    "SELECTION_METHOD = \"coverage\"\n",
    "\n",
    "# I/O roots\n",
    "IN_ROOT  = \"3_extracted_features\"\n",
    "OUT_ROOT = \"3.1_selected_features\"\n",
    "\n",
    "# Resolve a labeling folder name from an experiment name\n",
    "def _resolve_labeling_folder(experiment_name):\n",
    "    if experiment_name is None:\n",
    "        return None\n",
    "    name = str(experiment_name).strip()\n",
    "    if name.lower() in {\"\", \"all\", \"*\"}:\n",
    "        return None\n",
    "    return name if name.endswith(\"_features\") else f\"{name}_features\"\n",
    "\n",
    "# Optional filters supplied via config (if defined earlier)\n",
    "try:\n",
    "    DATASET_FILTER = None if (EVENT_LOG is None or str(EVENT_LOG).strip().lower() in {\"\", \"all\", \"*\"}) else str(EVENT_LOG).strip()\n",
    "except NameError:\n",
    "    DATASET_FILTER = None\n",
    "\n",
    "try:\n",
    "    LABELING_FILTER = _resolve_labeling_folder(EXP_NAME)\n",
    "except NameError:\n",
    "    LABELING_FILTER = None\n",
    "\n",
    "# Method parameters (defaults + optional overrides)\n",
    "THRESHOLD_DEFAULT = 10\n",
    "THRESHOLD_OVERRIDE = {\n",
    "    # \"declare\": 11,\n",
    "    # (\"traffic\", \"traffic_decl3_features\", \"declare\"): 11,\n",
    "}\n",
    "K_DEFAULT = 100\n",
    "K_OVERRIDE = {\n",
    "    # \"declare\": 150,\n",
    "    # (\"sepsis\", \"sepsis_mr_tr_features\", \"declare\"): 200,\n",
    "}\n",
    "\n",
    "def _get_threshold(dataset: str, labeling: str, encoding: str) -> int:\n",
    "    if (dataset, labeling, encoding) in THRESHOLD_OVERRIDE:\n",
    "        return THRESHOLD_OVERRIDE[(dataset, labeling, encoding)]\n",
    "    if encoding in THRESHOLD_OVERRIDE:\n",
    "        return THRESHOLD_OVERRIDE[encoding]\n",
    "    return THRESHOLD_DEFAULT\n",
    "\n",
    "def _get_k(dataset: str, labeling: str, encoding: str) -> int:\n",
    "    if (dataset, labeling, encoding) in K_OVERRIDE:\n",
    "        return K_OVERRIDE[(dataset, labeling, encoding)]\n",
    "    if encoding in K_OVERRIDE:\n",
    "        return K_OVERRIDE[encoding]\n",
    "    return K_DEFAULT\n",
    "\n",
    "def _find_csv_path(encoding_dir: str, encoding_name: str) -> str:\n",
    "    expected = os.path.join(encoding_dir, f\"{encoding_name}.csv\")\n",
    "    if os.path.isfile(expected):\n",
    "        return expected\n",
    "    csvs = [f for f in os.listdir(encoding_dir) if f.lower().endswith(\".csv\")]\n",
    "    if len(csvs) == 1:\n",
    "        return os.path.join(encoding_dir, csvs[0])\n",
    "    raise FileNotFoundError(f\"No CSV found for encoding '{encoding_name}' in {encoding_dir}\")\n",
    "\n",
    "def _ensure_out_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _count_original_features(df: pd.DataFrame, label_col: str = \"Label\", case_id_col: str = \"Case_ID\") -> int:\n",
    "    feats = df.drop(columns=[label_col, case_id_col], errors=\"ignore\").select_dtypes(include=[\"number\"])\n",
    "    return feats.shape[1]\n",
    "\n",
    "def _count_postfilter_features(df: pd.DataFrame, label_col: str = \"Label\", case_id_col: str = \"Case_ID\") -> int:\n",
    "    feats = df.drop(columns=[label_col, case_id_col], errors=\"ignore\").select_dtypes(include=[\"number\"]).copy()\n",
    "    if feats.shape[1] > 1:\n",
    "        feats = feats.T.drop_duplicates().T\n",
    "    if not feats.empty:\n",
    "        non_constant = (feats != feats.iloc[0]).any()\n",
    "        feats = feats.loc[:, non_constant]\n",
    "    return feats.shape[1]\n",
    "\n",
    "# -------------------- Batch over folders --------------------\n",
    "if not os.path.isdir(IN_ROOT):\n",
    "    raise FileNotFoundError(f\"Input root not found: {IN_ROOT}\")\n",
    "\n",
    "datasets = sorted([d for d in os.listdir(IN_ROOT) if os.path.isdir(os.path.join(IN_ROOT, d))])\n",
    "if DATASET_FILTER:\n",
    "    if DATASET_FILTER not in datasets:\n",
    "        raise FileNotFoundError(f\"Dataset '{DATASET_FILTER}' not found under {IN_ROOT}. Available: {datasets}\")\n",
    "    datasets = [DATASET_FILTER]\n",
    "\n",
    "total_sets = 0\n",
    "ok_sets = 0\n",
    "skipped_sets = 0\n",
    "errors = []\n",
    "summary_rows = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    ds_dir = os.path.join(IN_ROOT, dataset)\n",
    "    labelings = sorted([l for l in os.listdir(ds_dir) if os.path.isdir(os.path.join(ds_dir, l))])\n",
    "    if LABELING_FILTER:\n",
    "        if LABELING_FILTER not in labelings:\n",
    "            raise FileNotFoundError(f\"Labeling '{LABELING_FILTER}' not found under {ds_dir}. Available: {labelings}\")\n",
    "        labelings = [LABELING_FILTER]\n",
    "\n",
    "    for labeling in labelings:\n",
    "        lab_dir = os.path.join(ds_dir, labeling)\n",
    "        encodings = sorted([e for e in os.listdir(lab_dir) if os.path.isdir(os.path.join(lab_dir, e))])\n",
    "\n",
    "        for encoding in encodings:\n",
    "            enc_dir = os.path.join(lab_dir, encoding)\n",
    "            total_sets += 1\n",
    "            try:\n",
    "                csv_in = _find_csv_path(enc_dir, encoding)\n",
    "                df = pd.read_csv(csv_in)\n",
    "                rows_total = len(df)\n",
    "\n",
    "                if \"Label\" not in df.columns:\n",
    "                    skipped_sets += 1\n",
    "                    print(f\"[SKIP] {dataset}/{labeling}/{encoding}: no 'Label' column found.\")\n",
    "                    summary_rows.append({\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Labeling\": labeling,\n",
    "                        \"Encoding\": encoding,\n",
    "                        \"method\": SELECTION_METHOD,\n",
    "                        \"threshold\": _get_threshold(dataset, labeling, encoding) if SELECTION_METHOD == \"coverage\" else None,\n",
    "                        \"k\": _get_k(dataset, labeling, encoding) if SELECTION_METHOD == \"fisher_topk\" else None,\n",
    "                        \"rows_total\": rows_total,\n",
    "                        \"original_features\": _count_original_features(df),\n",
    "                        \"postfilter_features\": _count_postfilter_features(df),\n",
    "                        \"selected_features\": 0,\n",
    "                        \"unattainable_rows\": None,\n",
    "                        \"min_attained\": None,\n",
    "                        \"status\": \"SKIP_no_label\",\n",
    "                        \"output_csv\": None,\n",
    "                    })\n",
    "                    continue\n",
    "\n",
    "                if SELECTION_METHOD == \"coverage\":\n",
    "                    thr = _get_threshold(dataset, labeling, encoding)\n",
    "                    selected_df, selected_cols, info = fisher_coverage_select(\n",
    "                        df,\n",
    "                        label_col=\"Label\",\n",
    "                        case_id_col=\"Case_ID\",\n",
    "                        coverage_threshold=thr,\n",
    "                        positive_predicate=None,  # coverage defined as value > 0\n",
    "                        drop_duplicate_columns=True,\n",
    "                        drop_constant=True,\n",
    "                        return_features_only=False,\n",
    "                    )\n",
    "                elif SELECTION_METHOD == \"fisher_topk\":\n",
    "                    k_use = _get_k(dataset, labeling, encoding)\n",
    "                    selected_df, selected_cols, info = fisher_topk_select(\n",
    "                        df,\n",
    "                        label_col=\"Label\",\n",
    "                        case_id_col=\"Case_ID\",\n",
    "                        k=k_use,\n",
    "                        drop_duplicate_columns=True,\n",
    "                        drop_constant=True,\n",
    "                        return_features_only=False,\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown SELECTION_METHOD: {SELECTION_METHOD}\")\n",
    "\n",
    "                out_dir = os.path.join(OUT_ROOT, dataset, labeling, encoding)\n",
    "                _ensure_out_dir(out_dir)\n",
    "                csv_out = os.path.join(out_dir, os.path.basename(csv_in))\n",
    "                selected_df.to_csv(csv_out, index=False)\n",
    "\n",
    "                ok_sets += 1\n",
    "                if SELECTION_METHOD == \"coverage\":\n",
    "                    print(\n",
    "                        f\"[OK] {dataset}/{labeling}/{encoding}: \"\n",
    "                        f\"{SELECTION_METHOD}, selected {len(selected_cols)} features; \"\n",
    "                        f\"min_attained={info.get('attained_min','NA')}/{info.get('coverage_threshold','NA')}; \"\n",
    "                        f\"unattainable_rows={len(info.get('unattainable_rows',[]))} -> {csv_out}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"[OK] {dataset}/{labeling}/{encoding}: \"\n",
    "                        f\"{SELECTION_METHOD}, selected {len(selected_cols)}/{info.get('k')} -> {csv_out}\"\n",
    "                    )\n",
    "\n",
    "                summary_rows.append({\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"Labeling\": labeling,\n",
    "                    \"Encoding\": encoding,\n",
    "                    \"method\": info.get(\"method\"),\n",
    "                    \"threshold\": info.get(\"coverage_threshold\") if SELECTION_METHOD == \"coverage\" else None,\n",
    "                    \"k\": info.get(\"k\") if SELECTION_METHOD == \"fisher_topk\" else None,\n",
    "                    \"rows_total\": rows_total,\n",
    "                    \"original_features\": _count_original_features(df),\n",
    "                    \"postfilter_features\": info.get(\"postfilter_features\", _count_postfilter_features(df)),\n",
    "                    \"selected_features\": len(selected_cols),\n",
    "                    \"unattainable_rows\": len(info.get(\"unattainable_rows\", [])) if SELECTION_METHOD == \"coverage\" else None,\n",
    "                    \"min_attained\": info.get(\"attained_min\") if SELECTION_METHOD == \"coverage\" else None,\n",
    "                    \"status\": \"OK\",\n",
    "                    \"output_csv\": csv_out,\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                errors.append((dataset, labeling, encoding, str(e)))\n",
    "                print(f\"[ERR] {dataset}/{labeling}/{encoding}: {e}\")\n",
    "                summary_rows.append({\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"Labeling\": labeling,\n",
    "                    \"Encoding\": encoding,\n",
    "                    \"method\": SELECTION_METHOD,\n",
    "                    \"threshold\": _get_threshold(dataset, labeling, encoding) if SELECTION_METHOD == \"coverage\" else None,\n",
    "                    \"k\": _get_k(dataset, labeling, encoding) if SELECTION_METHOD == \"fisher_topk\" else None,\n",
    "                    \"rows_total\": None,\n",
    "                    \"original_features\": None,\n",
    "                    \"postfilter_features\": None,\n",
    "                    \"selected_features\": None,\n",
    "                    \"unattainable_rows\": None,\n",
    "                    \"min_attained\": None,\n",
    "                    \"status\": f\"ERR: {e}\",\n",
    "                    \"output_csv\": None,\n",
    "                })\n",
    "\n",
    "# Build summary\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "if not summary_df.empty:\n",
    "    summary_df = summary_df.sort_values([\"Dataset\", \"Labeling\", \"Encoding\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n=== Summary ({SELECTION_METHOD}) ===\")\n",
    "print(f\"Total sets found:   {total_sets}\")\n",
    "print(f\"Processed (OK):     {ok_sets}\")\n",
    "print(f\"Skipped (no Label): {skipped_sets}\")\n",
    "print(f\"Errors:             {len(errors)}\")\n",
    "\n",
    "display_cols = [\n",
    "    \"Dataset\", \"Labeling\", \"Encoding\",\n",
    "    \"method\", \"threshold\", \"k\",\n",
    "    \"rows_total\", \"original_features\", \"postfilter_features\", \"selected_features\",\n",
    "    \"unattainable_rows\", \"min_attained\",\n",
    "]\n",
    "if not summary_df.empty:\n",
    "    print(\"\\nFeature selection summary (key fields):\")\n",
    "    print(summary_df[display_cols].to_string(index=False))\n",
    "\n",
    "summary_out = os.path.join(OUT_ROOT, f\"feature_selection_summary_{SELECTION_METHOD}.csv\")\n",
    "summary_df.to_csv(summary_out, index=False)\n",
    "print(f\"\\nSaved full summary to: {summary_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7464ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export selected columns to LaTeX (clean names and formatting)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "OUT_ROOT = \"3.1_selected_features\"\n",
    "summary_out = os.path.join(OUT_ROOT, f\"feature_selection_summary_{SELECTION_METHOD}.csv\")\n",
    "\n",
    "# Reuse in-memory summary_df if available; otherwise read from disk\n",
    "try:\n",
    "    _ = summary_df\n",
    "except NameError:\n",
    "    summary_df = pd.read_csv(summary_out)\n",
    "\n",
    "# Columns to include\n",
    "keep_cols = [\n",
    "    \"Dataset\", \"Labeling\", \"Encoding\",\n",
    "    \"original_features\", \"postfilter_features\",\n",
    "    \"selected_features\", \"unattainable_rows\",\n",
    "]\n",
    "df = summary_df.loc[:, keep_cols].copy()\n",
    "\n",
    "# Strip trailing \"_features\" from labeling folder names\n",
    "df[\"Labeling\"] = df[\"Labeling\"].astype(str).str.replace(r\"_features$\", \"\", regex=True)\n",
    "\n",
    "# Convert tokens to readable text: underscores/hyphens → space; Title Case if all-lowercase\n",
    "def detokenize_text(x: str) -> str:\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    s = str(x)\n",
    "    s = re.sub(r\"[_\\-]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    if s.islower():  # keep acronyms as-is\n",
    "        s = s.title()\n",
    "    return s\n",
    "\n",
    "df[\"Labeling\"] = df[\"Labeling\"].apply(detokenize_text)\n",
    "df[\"Encoding\"] = df[\"Encoding\"].apply(detokenize_text)\n",
    "\n",
    "# Rename columns for presentation\n",
    "rename_map = {\n",
    "    \"original_features\": \"Original features\",\n",
    "    \"postfilter_features\": \"Postfilter features\",\n",
    "    \"selected_features\": \"Selected features\",\n",
    "    \"unattainable_rows\": \"Unattainable rows\",\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Cast counts to nullable integers (preserve NA as <NA>)\n",
    "int_cols = [\"Original features\", \"Postfilter features\", \"Selected features\", \"Unattainable rows\"]\n",
    "for c in int_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Stable ordering\n",
    "df = df.sort_values([\"Dataset\", \"Labeling\", \"Encoding\"]).reset_index(drop=True)\n",
    "\n",
    "# Output path (share stem with the CSV)\n",
    "try:\n",
    "    latex_out = Path(summary_out).with_suffix(\".tex\")\n",
    "except NameError:\n",
    "    latex_out = Path(\"feature_selection_summary_coverage.tex\")\n",
    "\n",
    "# Write LaTeX longtable\n",
    "df.to_latex(\n",
    "    latex_out,\n",
    "    index=False,\n",
    "    escape=True,\n",
    "    longtable=True,\n",
    "    column_format=\"lllrrrr\",\n",
    "    caption=\"Feature selection summary (coverage).\",\n",
    "    label=\"tab:feature_selection_summary_coverage\",\n",
    "    na_rep=\"--\",\n",
    ")\n",
    "\n",
    "print(f\"Wrote LaTeX table to: {latex_out.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e229",
   "metadata": {},
   "source": [
    "### 2.2 Process specific log + encoding (LEGACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Batch Fisher+Coverage selection + summary (config-driven: EVENT_LOG + EXP_NAME) ---\n",
    "\n",
    "# # Roots\n",
    "# IN_ROOT  = \"3_extracted_features\"\n",
    "# OUT_ROOT = \"3.1_selected_features\"\n",
    "\n",
    "# # Coverage threshold: default + optional overrides\n",
    "# THRESHOLD_DEFAULT = 10\n",
    "# THRESHOLD_OVERRIDE = {\n",
    "#     # Examples:\n",
    "#     # \"declare\": 11,\n",
    "#     # (\"sepsis\", \"sepsis_mr_tr_features\", \"declare\"): 11,\n",
    "# }\n",
    "\n",
    "# def _get_threshold(dataset: str, labeling: str, encoding: str) -> int:\n",
    "#     if (dataset, labeling, encoding) in THRESHOLD_OVERRIDE:\n",
    "#         return THRESHOLD_OVERRIDE[(dataset, labeling, encoding)]\n",
    "#     if encoding in THRESHOLD_OVERRIDE:\n",
    "#         return THRESHOLD_OVERRIDE[encoding]\n",
    "#     return THRESHOLD_DEFAULT\n",
    "\n",
    "# def _find_csv_path(encoding_dir: str, encoding_name: str) -> str:\n",
    "#     expected = os.path.join(encoding_dir, f\"{encoding_name}.csv\")\n",
    "#     if os.path.isfile(expected):\n",
    "#         return expected\n",
    "#     csvs = [f for f in os.listdir(encoding_dir) if f.lower().endswith(\".csv\")]\n",
    "#     if len(csvs) == 1:\n",
    "#         return os.path.join(encoding_dir, csvs[0])\n",
    "#     raise FileNotFoundError(f\"No CSV found for encoding '{encoding_name}' in {encoding_dir}\")\n",
    "\n",
    "# def _ensure_out_dir(path: str):\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# def _count_original_features(df: pd.DataFrame, label_col: str = \"Label\") -> int:\n",
    "#     feats = df.drop(columns=[label_col], errors=\"ignore\").select_dtypes(include=[\"number\"])\n",
    "#     return feats.shape[1]\n",
    "\n",
    "# def _count_postfilter_features(df: pd.DataFrame, label_col: str = \"Label\") -> int:\n",
    "#     feats = df.drop(columns=[label_col], errors=\"ignore\").select_dtypes(include=[\"number\"]).copy()\n",
    "#     if feats.shape[1] > 1:\n",
    "#         feats = feats.T.drop_duplicates().T\n",
    "#     if not feats.empty:\n",
    "#         non_constant = (feats != feats.iloc[0]).any()\n",
    "#         feats = feats.loc[:, non_constant]\n",
    "#     return feats.shape[1]\n",
    "\n",
    "# # --- Resolve config filters ---\n",
    "# def _resolve_labeling_folder(experiment_name: str) -> str:\n",
    "#     if experiment_name is None or str(experiment_name).strip().lower() in {\"\", \"all\", \"*\"}:\n",
    "#         return None  # means: process all labelings under the dataset\n",
    "#     name = str(experiment_name).strip()\n",
    "#     return name if name.endswith(\"_features\") else f\"{name}_features\"\n",
    "\n",
    "# DATASET_FILTER = None if (EVENT_LOG is None or str(EVENT_LOG).strip().lower() in {\"\", \"all\", \"*\"}) else str(EVENT_LOG).strip()\n",
    "# LABELING_FILTER = _resolve_labeling_folder(EXP_NAME)\n",
    "\n",
    "# if not os.path.isdir(IN_ROOT):\n",
    "#     raise FileNotFoundError(f\"Input root not found: {IN_ROOT}\")\n",
    "\n",
    "# # Build dataset list respecting config\n",
    "# datasets = sorted(os.listdir(IN_ROOT))\n",
    "# datasets = [d for d in datasets if os.path.isdir(os.path.join(IN_ROOT, d))]\n",
    "# if DATASET_FILTER:\n",
    "#     if DATASET_FILTER not in datasets:\n",
    "#         raise FileNotFoundError(f\"Dataset '{DATASET_FILTER}' not found under {IN_ROOT}. Available: {datasets}\")\n",
    "#     datasets = [DATASET_FILTER]\n",
    "\n",
    "# total_sets = 0\n",
    "# ok_sets = 0\n",
    "# skipped_sets = 0\n",
    "# errors = []\n",
    "# summary_rows = []\n",
    "\n",
    "# for dataset in datasets:\n",
    "#     ds_dir = os.path.join(IN_ROOT, dataset)\n",
    "#     labelings = sorted([l for l in os.listdir(ds_dir) if os.path.isdir(os.path.join(ds_dir, l))])\n",
    "\n",
    "#     if LABELING_FILTER:\n",
    "#         if LABELING_FILTER not in labelings:\n",
    "#             raise FileNotFoundError(\n",
    "#                 f\"Labeling '{LABELING_FILTER}' not found under {ds_dir}. \"\n",
    "#                 f\"Available: {labelings}\"\n",
    "#             )\n",
    "#         labelings = [LABELING_FILTER]\n",
    "\n",
    "#     for labeling in labelings:\n",
    "#         lab_dir = os.path.join(ds_dir, labeling)\n",
    "#         encodings = sorted([e for e in os.listdir(lab_dir) if os.path.isdir(os.path.join(lab_dir, e))])\n",
    "\n",
    "#         for encoding in encodings:\n",
    "#             enc_dir = os.path.join(lab_dir, encoding)\n",
    "#             total_sets += 1\n",
    "#             try:\n",
    "#                 csv_in = _find_csv_path(enc_dir, encoding)\n",
    "#                 df = pd.read_csv(csv_in)\n",
    "#                 rows_total = len(df)\n",
    "\n",
    "#                 if \"Label\" not in df.columns:\n",
    "#                     skipped_sets += 1\n",
    "#                     print(f\"[SKIP] {dataset}/{labeling}/{encoding}: no 'Label' column found.\")\n",
    "#                     summary_rows.append({\n",
    "#                         \"Dataset\": dataset,\n",
    "#                         \"Labeling\": labeling,\n",
    "#                         \"Encoding\": encoding,\n",
    "#                         \"threshold\": _get_threshold(dataset, labeling, encoding),\n",
    "#                         \"rows_total\": rows_total,\n",
    "#                         \"original_features\": _count_original_features(df),\n",
    "#                         \"postfilter_features\": _count_postfilter_features(df),\n",
    "#                         \"selected_features\": 0,\n",
    "#                         \"unattainable_rows\": rows_total,\n",
    "#                         \"min_attained\": 0,\n",
    "#                         \"status\": \"SKIP_no_label\",\n",
    "#                         \"output_csv\": None,\n",
    "#                     })\n",
    "#                     continue\n",
    "\n",
    "#                 thr = _get_threshold(dataset, labeling, encoding)\n",
    "\n",
    "#                 # Apply Fisher + coverage (value > 0 = coverage; mirrors paper's code path)\n",
    "#                 selected_df, selected_cols, info = fisher_coverage_select(\n",
    "#                     df,\n",
    "#                     label_col=\"Label\",\n",
    "#                     coverage_threshold=thr,\n",
    "#                     positive_predicate=None,           # coverage = value > 0\n",
    "#                     drop_duplicate_columns=True,\n",
    "#                     drop_constant=True,\n",
    "#                     return_features_only=False\n",
    "#                 )\n",
    "\n",
    "#                 out_dir = os.path.join(OUT_ROOT, dataset, labeling, encoding)\n",
    "#                 _ensure_out_dir(out_dir)\n",
    "#                 csv_out = os.path.join(out_dir, os.path.basename(csv_in))\n",
    "#                 selected_df.to_csv(csv_out, index=False)\n",
    "\n",
    "#                 ok_sets += 1\n",
    "#                 print(\n",
    "#                     f\"[OK] {dataset}/{labeling}/{encoding}: \"\n",
    "#                     f\"selected {len(selected_cols)} features; \"\n",
    "#                     f\"min_attained={info.get('attained_min', 'NA')}/{info.get('coverage_threshold', thr)}; \"\n",
    "#                     f\"unattainable_rows={len(info.get('unattainable_rows', []))} -> {csv_out}\"\n",
    "#                 )\n",
    "\n",
    "#                 summary_rows.append({\n",
    "#                     \"Dataset\": dataset,\n",
    "#                     \"Labeling\": labeling,\n",
    "#                     \"Encoding\": encoding,\n",
    "#                     \"threshold\": info.get(\"coverage_threshold\", thr),\n",
    "#                     \"rows_total\": rows_total,\n",
    "#                     \"original_features\": _count_original_features(df),\n",
    "#                     \"postfilter_features\": _count_postfilter_features(df),\n",
    "#                     \"selected_features\": len(selected_cols),\n",
    "#                     \"unattainable_rows\": len(info.get(\"unattainable_rows\", [])),\n",
    "#                     \"min_attained\": info.get(\"attained_min\", None),\n",
    "#                     \"status\": \"OK\",\n",
    "#                     \"output_csv\": csv_out,\n",
    "#                 })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 errors.append((dataset, labeling, encoding, str(e)))\n",
    "#                 print(f\"[ERR] {dataset}/{labeling}/{encoding}: {e}\")\n",
    "#                 summary_rows.append({\n",
    "#                     \"Dataset\": dataset,\n",
    "#                     \"Labeling\": labeling,\n",
    "#                     \"Encoding\": encoding,\n",
    "#                     \"threshold\": _get_threshold(dataset, labeling, encoding),\n",
    "#                     \"rows_total\": None,\n",
    "#                     \"original_features\": None,\n",
    "#                     \"postfilter_features\": None,\n",
    "#                     \"selected_features\": None,\n",
    "#                     \"unattainable_rows\": None,\n",
    "#                     \"min_attained\": None,\n",
    "#                     \"status\": f\"ERR: {e}\",\n",
    "#                     \"output_csv\": None,\n",
    "#                 })\n",
    "\n",
    "# # Build and show summary DataFrame\n",
    "# summary_df = pd.DataFrame(summary_rows)\n",
    "# if not summary_df.empty:\n",
    "#     summary_df = summary_df.sort_values([\"Dataset\", \"Labeling\", \"Encoding\"]).reset_index(drop=True)\n",
    "\n",
    "# print(\"\\n=== Summary ===\")\n",
    "# print(f\"Total sets found:   {total_sets}\")\n",
    "# print(f\"Processed (OK):     {ok_sets}\")\n",
    "# print(f\"Skipped (no Label): {skipped_sets}\")\n",
    "# print(f\"Errors:             {len(errors)}\")\n",
    "\n",
    "# display_cols = [\n",
    "#     \"Dataset\", \"Labeling\", \"Encoding\",\n",
    "#     \"threshold\", \"rows_total\",\n",
    "#     \"original_features\", \"postfilter_features\", \"selected_features\",\n",
    "#     \"unattainable_rows\", \"min_attained\"\n",
    "# ]\n",
    "# if not summary_df.empty:\n",
    "#     print(\"\\nFeature selection summary (key fields):\")\n",
    "#     print(summary_df[display_cols].to_string(index=False))\n",
    "\n",
    "# summary_out = os.path.join(OUT_ROOT, \"feature_selection_summary.csv\")\n",
    "# summary_df.to_csv(summary_out, index=False)\n",
    "# print(f\"\\nSaved full summary to: {summary_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
